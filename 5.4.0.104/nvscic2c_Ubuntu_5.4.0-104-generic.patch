diff --git a/drivers/misc/nvscic2c-pcie/Makefile b/drivers/misc/nvscic2c-pcie/Makefile
new file mode 100644
index 000000000000..ee951aa10e9b
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/Makefile
@@ -0,0 +1,38 @@
+# SPDX-License-Identifier: GPL-2.0
+#
+BINARY         := nvscic2c-pcie-epc
+KVER           ?= $(shell uname -r)
+MODULES_DIR    := /lib/modules/$(KVER)
+MODULE_SUBDIR  ?= /kernel/drivers/misc/
+KMOD_DIR       := $(shell pwd)
+KDIR           := $(MODULES_DIR)/build
+MODULE_DESTDIR := $(MODULES_DIR)/$(MODULE_SUBDIR)
+
+ARCH        := x86
+C_FLAGS     := -Wall
+DEPMOD      := /sbin/depmod
+
+OBJECTS := comm-channel.o endpoint.o pci-client.o stream-extensions.o vmap.o vmap-pin.o iova-mngr.o config.o tegra-pcie-edma.o module.o
+
+ifneq ($(DISABLE_GPU), 1)
+NVIDIA_SRC_DIR ?= $(shell find /usr/src/nvidia-* -name "nv-p2p.h"|head -1|xargs dirname || echo "NVIDIA_DRIVER_MISSING")
+ccflags-y += -DNVSCIC2C_GPU
+ccflags-y += -I$(NVIDIA_SRC_DIR)/../nvidia
+KBUILD_EXTRA_SYMBOLS :=$(NVIDIA_SRC_DIR)/../Module.symvers
+endif
+obj-m     += $(BINARY).o
+
+$(BINARY)-y := $(OBJECTS)
+
+$(BINARY).ko:
+	make -C $(KDIR) M=$(KMOD_DIR) modules
+
+install: $(BINARY).ko
+	[ -d $(DESTDIR)/$(MODULE_DESTDIR) ] || mkdir -p $(DESTDIR)/$(MODULE_DESTDIR)
+	cp nvscic2c-pcie-epc.ko $(DESTDIR)/$(MODULE_DESTDIR)
+	if [ ! -n "$(DESTDIR)" ]; then $(DEPMOD) -ae $(KVER); fi
+
+clean:
+	make -C $(KDIR) M=$(KMOD_DIR) clean
+	rm -f *.o
+	rm -f $(BINARY).ko
diff --git a/drivers/misc/nvscic2c-pcie/README b/drivers/misc/nvscic2c-pcie/README
new file mode 100644
index 000000000000..f86acdbc69f6
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/README
@@ -0,0 +1,89 @@
+README for bringup of NvSciC2c driver on x86. It covers:
+ 1. Platform.
+ 2. Build of NvSciC2c driver and its dependencies.
+ 3. NvSciC2c driver execution on x86.
+ 4. Contact.
+
+
+ 1. Platform
+    NvSciC2c is Nvidia proprietary Chip-to-Chip SW communication protocol which
+    allows exchange of data over PCIe Root-Port <-> Endpoint in the folliowing manner
+        a. Low latency small packets via CPU transfers.
+        b. Bulk data packets via DMA transfers.
+
+    Supported HW platforms:
+        a. 3rd generation scalable Intel Xeon with NVIDIA DRIVE-A100 Automotive GPU (optional)
+
+    Supported PCIe reference topology:
+        a. PCIe re-timers card P3722 is inserted into a X16 PCIe slot on Intel Xeon board
+        b. Interconnection Board P3713 is present in DRIVE AGX Devkit
+        c. miniSAS Port-B of P3713 is connected to miniSAS Port D of P3722 with a PCIe miniSAS cable
+
+    Supported SW Release and Configurations(s):
+        a. Ubuntu 20.04 LTS installation with kernel=5.4.0-104-generic
+        b. NVIDIA GPU Driver with version >= 510.73
+        c. The IOMMU should be enabled by having intel_iommu=on in kernel command
+
+ 2. Build NvSciC2c driver and its dependences.
+
+    2.1 Build and Install kernel 5.4.0-104-generic with IOMMU_DMA enabled:
+        NvSciC2c driver uses iommu APIs  to allocate iova, which is used for programming of DMA.
+        The IOMMU_DMA config that supports IOMMU agnostic DMA-mapping layer is currently diabled by default in
+        K5.4 and it needs to be enabled.
+
+        Steps to build Kernel 5.4: BuildingYourOwnKernel(https://wiki.ubuntu.com/Kernel/BuildYourOwnKernel)
+            a. Obtain the source for an Ubuntu release:
+               git clone git://kernel.ubuntu.com/ubuntu/ubuntu-focal.git
+            b. checkout the Ubuntu 5.4.0-104 tag
+               cd ubuntu-focal
+               git checkout tags/Ubuntu-5.4.0-104.118 -b your_branch
+            c. enable IOMMU_DMA by directly editing drivers/iommu/Kconfig and marking it "default y"
+               for the IOMMU_DMA section
+            d. Building the kernel
+               LANG=C fakeroot debian/rules clean
+               LANG=C fakeroot debian/rules binary-headers binary-generic
+            e. once built, install kernel deb files
+               sudo dpkg -i ../linux*5.4.0*.deb
+            g. set newly build image as default in grub
+               sudo vi /etc/default/grub
+               change GRUB_DEFAULT = "Advanced options for Ubuntu>Ubuntu, with Linux 5.4.0-104-generic"
+               set GRUB_CMDLINE_LINUX_DEFAULT="intel_iommu=on"
+               sudo update-grub
+               sudo reboot
+               On reboot, uname -r should show reflect the kernel version as:
+               5.4.0-104-generic
+
+    2.2  Steps to build NvSciC2c driver on top of K5.4 cloned in step 2.1:
+        a. Switch to the same branch created in step 2.1 or create a new branch
+           cd ubuntu-focal/drivers/misc/
+           git checkout your_branch
+        b. Apply the patches for NvSciC2c driver
+           git apply nvscic2c_Ubuntu_5.4.0-104-generic
+        c. Verify Module.symvers is present in /usr/src/nvidia-510.73, otherwise generate it manualy
+           cd /usr/src/nvidia-510.73
+           sudo make
+        d. Build NvSciC2c driver as an out-of-tree module
+           cd drivers/misc/nvscic2c-pcie
+           sudo make
+           sudo make install
+        e. To use the NvSciC2c driver without dGPU, build it using
+           sudo make -j DISABLE_GPU=1
+           sudo make install
+
+ 3.  NvSciC2c driver execution:
+
+   3.1 Detect AGX Orin as PCIe Endpoint
+
+      For NvSciC2c driver to be used on X86, the DRIVE AGX Orin should be detected as valid PCIe endpoint.
+      The details for Orin setup are captured in the DRIVE OS release documentation
+
+   3.2 Inserting NvSciC2c driver
+       Once the NvSciC2c driver is compiled by following the steps 2.2[a-d], it can be inserted into the kernel.
+       sudo modprobe nvscic2c-pcie-epc
+
+ 4. Contact:
+    Evan Shi <eshi@nvidia.com>
+    Chirantan <chirantan@nvidia.com>
+    Arihant Jejani <ajejani@nvidia.com>
+    Deepak Kumar Badgaiyan <dbadgaiyan@nvidia.com>
+    Bob Johnston <BJohnston@nvidia.com>
diff --git a/drivers/misc/nvscic2c-pcie/comm-channel.c b/drivers/misc/nvscic2c-pcie/comm-channel.c
new file mode 100644
index 000000000000..4e79e88b6a2d
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/comm-channel.c
@@ -0,0 +1,721 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvscic2c-pcie: comm-channel: " fmt
+
+#include <linux/atomic.h>
+#include <linux/dma-iommu.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/kthread.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/wait.h>
+#include <linux/pci.h>
+
+#include "comm-channel.h"
+#include "common.h"
+#include "module.h"
+#include "pci-client.h"
+#include "config.h"
+
+#define CACHE_ALIGN		(64)
+
+/* Fifo size */
+/*
+ * This is wrong, but to have private communication channel functional at the
+ * earliest, we allocate large set of frames assuming all the available
+ * endpoints can share all possible export descriptors without having to block
+ * and wait for channel to become writeable.
+ *
+ * Despite this huge fifo size, if msg cannot be send, it either means remote
+ * is busy processing them quite slow (unlikely) or ill. In such a case, we
+ * shall return -EAGAIN for application to retry and application can bail-out
+ * after few retries.
+ */
+#define COMM_CHANNEL_NFRAMES	(1024)
+#define COMM_CHANNEL_FRAME_SZ	(64)
+
+/* fifo header.*/
+struct header {
+	u32 wr_count;
+	u32 rd_count;
+	u8 pad[CACHE_ALIGN - sizeof(u32) - sizeof(u32)];
+} __packed;
+
+/* kthread. */
+struct task_t {
+	struct task_struct *task;
+	wait_queue_head_t waitq;
+	struct completion shutdown_compl;
+	bool shutdown;
+	bool created;
+};
+
+/* Notification handling. */
+struct syncpt_t {
+	/* PCIe aperture for writes to peer sync for same comm-channel. */
+	struct pci_aper_t peer_mem;
+
+	/* sync physical address for stitching to PCIe BAR backing.*/
+	size_t size;
+	phys_addr_t phy_addr;
+
+	/* iova mapping of client choice.*/
+	void *iova_block_h;
+	u64 iova;
+	bool mapped_iova;
+};
+
+struct fifo_t {
+	/* slot/frames for the comm-channel.*/
+	u32 nframes;
+	u32 frame_sz;
+
+	/* fifo operations.*/
+	struct header *send_hdr;
+	struct header *recv_hdr;
+	struct header *local_hdr;
+	u8 *send;
+	u8 *recv;
+	u32 wr_pos;
+	u32 rd_pos;
+	/* serialize send operations.*/
+	struct mutex send_lock;
+
+	/* fifo physical pages and stitched to iova of client choice(recv).*/
+	struct cpu_buff_t self_mem;
+	void *iova_block_h;
+	u64 iova;
+	bool mapped_iova;
+
+	/* PCIe aperture for writes to peer comm fifo. */
+	struct pci_aper_t peer_mem;
+};
+
+struct comm_channel_ctx_t {
+	/* data. */
+	struct fifo_t fifo;
+
+	/* Notification. */
+	struct syncpt_t syncpt;
+
+	/* receive message task.*/
+	struct task_t r_task;
+	atomic_t recv_count;
+
+	/* Callbacks registered for recv messages. */
+	struct mutex cb_ops_lock;
+	struct callback_ops cb_ops[COMM_MSG_TYPE_MAXIMUM];
+
+	/* pci client handle.*/
+	void *pci_client_h;
+};
+
+static inline bool
+can_send(struct fifo_t *fifo, int *ret)
+{
+	bool send = false;
+	u32 peer_toread =
+		(fifo->local_hdr->wr_count - fifo->recv_hdr->rd_count);
+
+	if (peer_toread < fifo->nframes) {
+		/* space available - can send.*/
+		send = true;
+		*ret = 0;
+	} else if (peer_toread == fifo->nframes) {
+		/* full. client can try again (at the moment.)*/
+		send = false;
+		*ret = -EAGAIN;	// -ENOMEM;
+	} else if (peer_toread > fifo->nframes) {
+		/* erroneous.*/
+		send = false;
+		*ret = -EOVERFLOW;
+	}
+
+	return send;
+}
+
+static inline bool
+can_recv(struct fifo_t *fifo, int *ret)
+{
+	bool recv = false;
+	u32 toread = (fifo->recv_hdr->wr_count - fifo->local_hdr->rd_count);
+
+	if (toread == 0) {
+		/* no frame available to read.*/
+		recv = false;
+		*ret = -ENODATA;
+	} else if (toread <= fifo->nframes) {
+		/* frames available - can read.*/
+		recv = true;
+		*ret = 0;
+	} else if (toread > fifo->nframes) {
+		/* erroneous.*/
+		recv = false;
+		*ret = -EOVERFLOW;
+	}
+
+	return recv;
+}
+
+static int
+send_msg(struct comm_channel_ctx_t *comm_ctx, struct comm_msg *msg)
+{
+	int ret = 0;
+	size_t size = 0;
+	void *from = NULL;
+	void __iomem *to = NULL;
+	struct fifo_t *fifo = NULL;
+	struct syncpt_t *syncpt = NULL;
+
+	fifo = &comm_ctx->fifo;
+	syncpt = &comm_ctx->syncpt;
+
+	mutex_lock(&fifo->send_lock);
+
+	/* if no space available, at the moment, client can try again. */
+	if (!can_send(fifo, &ret)) {
+		mutex_unlock(&fifo->send_lock);
+		return ret;
+	}
+
+	to = (void __iomem *)(fifo->send + (fifo->wr_pos * fifo->frame_sz));
+	from = (void *)(msg);
+	size = sizeof(*msg);
+	memcpy_toio(to, from, size);
+
+	fifo->local_hdr->wr_count++;
+	writel(fifo->local_hdr->wr_count,
+	       (void __iomem *)(&fifo->send_hdr->wr_count));
+
+	/* notify peer for each write.*/
+	writel(0x1, syncpt->peer_mem.pva);
+	fifo->wr_pos = fifo->wr_pos + 1;
+	if (fifo->wr_pos >= fifo->nframes)
+		fifo->wr_pos = 0;
+
+	mutex_unlock(&fifo->send_lock);
+
+	return ret;
+}
+
+int
+comm_channel_bootstrap_msg_send(void *comm_channel_h, struct comm_msg *msg)
+{
+	struct comm_channel_ctx_t *comm_ctx =
+				(struct comm_channel_ctx_t *)comm_channel_h;
+
+	if (WARN_ON(!comm_ctx || !msg))
+		return -EINVAL;
+
+	if (WARN_ON(msg->type != COMM_MSG_TYPE_BOOTSTRAP))
+		return -EINVAL;
+
+	/*
+	 * this is a special one-time message where the sender: @DRV_MODE_EPC
+	 * shares it's own iova with @DRV_MODE_EPF for @DRV_MODE_EPF CPU
+	 * access towards @DRV_MODE_EPC. We do not check for PCIe link here
+	 * and therefore must be send by @DRV_MODE_EPC only when @DRV_MODE_EPF
+	 * has initialized it's own comm-channel interface (during _bind() api).
+	 */
+	return send_msg(comm_ctx, msg);
+}
+
+int
+comm_channel_msg_send(void *comm_channel_h, struct comm_msg *msg)
+{
+	enum nvscic2c_pcie_link link = NVSCIC2C_PCIE_LINK_DOWN;
+	struct comm_channel_ctx_t *comm_ctx =
+				(struct comm_channel_ctx_t *)comm_channel_h;
+
+	if (WARN_ON(!comm_ctx || !msg))
+		return -EINVAL;
+
+	if (WARN_ON(msg->type <= COMM_MSG_TYPE_INVALID ||
+		    msg->type >= COMM_MSG_TYPE_MAXIMUM ||
+		    msg->type == COMM_MSG_TYPE_BOOTSTRAP))
+		return -EINVAL;
+
+	link = pci_client_query_link_status(comm_ctx->pci_client_h);
+	if (link != NVSCIC2C_PCIE_LINK_UP)
+		return -ENOLINK;
+
+	return send_msg(comm_ctx, msg);
+}
+
+static int
+recv_taskfn(void *arg)
+{
+	int ret = 0;
+	struct comm_channel_ctx_t *comm_ctx = NULL;
+	struct comm_msg *msg = NULL;
+	struct task_t *task = NULL;
+	struct fifo_t *fifo = NULL;
+	struct callback_ops *cb_ops = NULL;
+
+	comm_ctx = (struct comm_channel_ctx_t *)(arg);
+	task = &comm_ctx->r_task;
+	fifo = &comm_ctx->fifo;
+	while (!task->shutdown) {
+		/* wait for notification from peer or shutdown. */
+		wait_event_interruptible(task->waitq,
+					 (atomic_read(&comm_ctx->recv_count) ||
+					  task->shutdown));
+		/* task is exiting.*/
+		if (task->shutdown)
+			continue;
+
+		/* read all on single notify.*/
+		atomic_dec(&comm_ctx->recv_count);
+		while (can_recv(fifo, &ret)) {
+			msg = (struct comm_msg *)
+				(fifo->recv + (fifo->rd_pos * fifo->frame_sz));
+
+			if (msg->type > COMM_MSG_TYPE_INVALID &&
+			    msg->type < COMM_MSG_TYPE_MAXIMUM) {
+				mutex_lock(&comm_ctx->cb_ops_lock);
+				cb_ops = &comm_ctx->cb_ops[msg->type];
+				if (cb_ops->callback)
+					cb_ops->callback
+						((void *)msg, cb_ops->ctx);
+				mutex_unlock(&comm_ctx->cb_ops_lock);
+			}
+
+			fifo->local_hdr->rd_count++;
+			writel(fifo->local_hdr->rd_count,
+			       (void __iomem *)(&fifo->send_hdr->rd_count));
+
+			/* do not noifty peer for space availability. */
+
+			fifo->rd_pos = fifo->rd_pos + 1;
+			if (fifo->rd_pos >= fifo->nframes)
+				fifo->rd_pos = 0;
+		}
+
+		/* if nothing (left) to read, go back waiting. */
+		continue;
+	}
+
+	/* we do not use kthread_stop(), but wait on this.*/
+	complete(&task->shutdown_compl);
+	return 0;
+}
+
+/*
+ * X86  RP : MSI-X IRQ handling for the irq raised from Orin EP
+ * wake up recv
+ */
+static irqreturn_t
+syncpt_callback(int irq, void *data)
+{
+	struct comm_channel_ctx_t *comm_ctx = NULL;
+
+	if (WARN_ON(!data))
+		return IRQ_HANDLED;
+	comm_ctx = (struct comm_channel_ctx_t *)(data);
+
+	/* kick r_task for processing this notification.*/
+	atomic_inc(&comm_ctx->recv_count);
+	wake_up_interruptible_all(&comm_ctx->r_task.waitq);
+	return IRQ_HANDLED;
+}
+
+static int
+start_msg_handling(struct comm_channel_ctx_t *comm_ctx)
+{
+	int ret = 0;
+	struct task_t *r_task = &comm_ctx->r_task;
+
+	/* start the recv msg processing task.*/
+	init_waitqueue_head(&r_task->waitq);
+	init_completion(&r_task->shutdown_compl);
+	r_task->shutdown = false;
+	r_task->task = kthread_run(recv_taskfn, comm_ctx,
+				   "comm-channel-recv-task");
+	if (IS_ERR(r_task->task)) {
+		pr_err("Failed to create comm channel recv task\n");
+		return PTR_ERR(r_task->task);
+	}
+	r_task->created = true;
+
+	return ret;
+}
+
+static int
+stop_msg_handling(struct comm_channel_ctx_t *comm_ctx)
+{
+	int ret = 0;
+	struct task_t *r_task = NULL;
+
+	if (!comm_ctx)
+		return ret;
+
+	r_task = &comm_ctx->r_task;
+
+	if (r_task->created) {
+		/*
+		 * initiate stop.
+		 * we do not use kthread_stop(), but wait on this.
+		 */
+		r_task->shutdown = true;
+		wake_up_interruptible(&r_task->waitq);
+		ret = wait_for_completion_interruptible(&r_task->shutdown_compl);
+		if (ret)
+			pr_err("Failed to wait for completion\n");
+		r_task->created = false;
+	}
+
+	return ret;
+}
+
+static void
+free_sync_primitive(struct comm_channel_ctx_t *comm_ctx)
+{
+	struct syncpt_t *syncpt = NULL;
+	struct pci_dev *pdev = pci_client_get_pci_dev(comm_ctx->pci_client_h);
+
+	syncpt = &comm_ctx->syncpt;
+
+	if (syncpt->peer_mem.pva) {
+		iounmap(syncpt->peer_mem.pva);
+		syncpt->peer_mem.pva = NULL;
+	}
+
+	if (syncpt->mapped_iova) {
+		pci_client_unmap_addr(comm_ctx->pci_client_h,
+				      syncpt->iova, syncpt->size);
+		syncpt->mapped_iova = false;
+	}
+
+	if (syncpt->iova_block_h) {
+		pci_client_free_iova(comm_ctx->pci_client_h,
+				     &syncpt->iova_block_h);
+		syncpt->iova_block_h = NULL;
+	}
+	if ((pdev) && (comm_ctx))
+		free_irq(pci_irq_vector(pdev, 0), comm_ctx);
+}
+
+static int
+init_notif_var(struct comm_channel_ctx_t *comm_ctx)
+{
+	int ret = 0;
+	size_t offsetof = 0x0;
+	struct syncpt_t *syncpt = NULL;
+	struct pci_dev *pdev = NULL;
+
+	if (WARN_ON(!comm_ctx))
+		return -EINVAL;
+
+	pdev = pci_client_get_pci_dev(comm_ctx->pci_client_h);
+	if (WARN_ON(!pdev))
+		return -EINVAL;
+
+	syncpt = &comm_ctx->syncpt;
+
+	syncpt->size = SP_SIZE;
+
+	/* reserve iova with the iova manager.*/
+	ret = pci_client_alloc_iova(comm_ctx->pci_client_h, syncpt->size,
+				    &syncpt->iova, &offsetof,
+				    &syncpt->iova_block_h);
+	if (ret) {
+		pr_err("Err reserving comm syncpt iova region of size: 0x%lx\n",
+		       syncpt->size);
+		goto err;
+	}
+	syncpt->mapped_iova = true;
+
+	pr_debug("mapped phy:0x%pa[p]+0x%lx to iova:0x%llx\n",
+		 &syncpt->phy_addr, syncpt->size, syncpt->iova);
+
+	/*
+	 * get peer's aperture offset. Map tx (pcie aper for notif tx.)
+	 * for peer's access of comm-syncpt, it is assumed offsets are
+	 * same on both SoC.
+	 */
+	syncpt->peer_mem.size = syncpt->size;
+	ret = pci_client_get_peer_aper(comm_ctx->pci_client_h, offsetof,
+				       syncpt->peer_mem.size,
+				       &syncpt->peer_mem.aper);
+	if (ret) {
+		pr_err("Failed to get comm peer's syncpt aperture\n");
+		goto err;
+	}
+	syncpt->peer_mem.pva = ioremap(syncpt->peer_mem.aper,
+				       syncpt->peer_mem.size);
+
+	if (!syncpt->peer_mem.pva) {
+		ret = -ENOMEM;
+		pr_err("Failed to ioremap comm peer's syncpt pcie aperture\n");
+		goto err;
+	}
+	/* msi-x irq  from Orin EP */
+	ret = request_irq(pci_irq_vector(pdev, COMM_CHANNEL_IRQ),
+			  syncpt_callback, IRQF_SHARED, "c2c-comm-channel", comm_ctx);
+	return ret;
+err:
+	free_sync_primitive(comm_ctx);
+	return ret;
+}
+
+static void
+free_fifo_memory(struct comm_channel_ctx_t *comm_ctx)
+{
+	struct fifo_t *fifo = NULL;
+
+	if (!comm_ctx)
+		return;
+
+	fifo = &comm_ctx->fifo;
+
+	if (fifo->local_hdr) {
+		kfree((void *)fifo->local_hdr);
+		fifo->local_hdr = NULL;
+	}
+
+	if (fifo->peer_mem.pva) {
+		iounmap(fifo->peer_mem.pva);
+		fifo->peer_mem.pva = NULL;
+	}
+
+	if (fifo->mapped_iova) {
+		pci_client_unmap_addr(comm_ctx->pci_client_h,
+				      fifo->iova, fifo->self_mem.size);
+		fifo->mapped_iova = false;
+	}
+
+	if (fifo->iova_block_h) {
+		pci_client_free_iova(comm_ctx->pci_client_h,
+				     &fifo->iova_block_h);
+		fifo->iova_block_h = NULL;
+	}
+
+	if (fifo->self_mem.pva) {
+		free_pages_exact(fifo->self_mem.pva,
+				 fifo->self_mem.size);
+		fifo->self_mem.pva = NULL;
+	}
+
+	mutex_destroy(&fifo->send_lock);
+}
+
+static int
+allocate_fifo_memory(struct comm_channel_ctx_t *comm_ctx)
+{
+	int ret = 0;
+	int prot = 0;
+	size_t offsetof = 0x0;
+	struct fifo_t *fifo = &comm_ctx->fifo;
+
+	mutex_init(&fifo->send_lock);
+
+	/* memory size includes frames and header.*/
+	fifo->nframes = COMM_CHANNEL_NFRAMES;
+	fifo->frame_sz = COMM_CHANNEL_FRAME_SZ;
+	fifo->self_mem.size = (fifo->nframes * fifo->frame_sz);
+	fifo->self_mem.size += sizeof(struct header);
+	fifo->self_mem.size = ALIGN(fifo->self_mem.size, PAGE_SIZE);
+	fifo->self_mem.pva = alloc_pages_exact(fifo->self_mem.size,
+					       (GFP_KERNEL | __GFP_ZERO));
+	if (!fifo->self_mem.pva) {
+		pr_err("Error allocating fifo contiguous pages: (%lu)\n",
+		       (fifo->self_mem.size >> PAGE_SHIFT));
+		return -ENOMEM;
+	}
+	fifo->self_mem.phys_addr =
+				page_to_phys(virt_to_page(fifo->self_mem.pva));
+
+	/* reserve iova for stitching comm channel memory for peer access.*/
+	ret = pci_client_alloc_iova(comm_ctx->pci_client_h, fifo->self_mem.size,
+				    &fifo->iova, &offsetof,
+				    &fifo->iova_block_h);
+	if (ret) {
+		pr_err("Failed reserving fifo iova region of size: 0x%lx\n",
+		       fifo->self_mem.size);
+		goto err;
+	}
+
+	/* map the pages to the reserved iova.*/
+	prot = (IOMMU_CACHE | IOMMU_READ | IOMMU_WRITE);
+	ret = pci_client_map_addr(comm_ctx->pci_client_h, fifo->iova,
+				  fifo->self_mem.phys_addr, fifo->self_mem.size,
+				  prot);
+	if (ret) {
+		pr_err("Failed to map comm fifo pages to reserved iova\n");
+		goto err;
+	}
+	fifo->mapped_iova = true;
+
+	pr_debug("comm fifo mapped page:0x%pa[p]+0x%lx to iova:0x%llx\n",
+		 &fifo->self_mem.phys_addr, fifo->self_mem.size, fifo->iova);
+
+	/*
+	 * for peer's access of comm-fifo, it is assumed offsets are
+	 * same on both SoC.
+	 */
+	fifo->peer_mem.size = fifo->self_mem.size;
+	ret = pci_client_get_peer_aper(comm_ctx->pci_client_h, offsetof,
+				       fifo->peer_mem.size,
+				       &fifo->peer_mem.aper);
+	if (ret) {
+		pr_err("Failed to get comm peer's fifo aperture\n");
+		goto err;
+	}
+	fifo->peer_mem.pva = ioremap(fifo->peer_mem.aper, fifo->peer_mem.size);
+	if (!fifo->peer_mem.pva) {
+		ret = -ENOMEM;
+		pr_err("Failed to ioremap peer's comm fifo aperture\n");
+		goto err;
+	}
+
+	/* allocate local header.*/
+	fifo->local_hdr = kzalloc(sizeof(*fifo->local_hdr), GFP_KERNEL);
+	if (WARN_ON(!fifo->local_hdr)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	fifo->recv_hdr = (struct header *)(fifo->self_mem.pva);
+	fifo->send_hdr = (__force struct header *)(fifo->peer_mem.pva);
+	fifo->recv = ((u8 *)fifo->recv_hdr + sizeof(struct header));
+	fifo->send = ((u8 *)fifo->send_hdr + sizeof(struct header));
+
+	return ret;
+
+err:
+	free_fifo_memory(comm_ctx);
+	return ret;
+}
+
+int
+comm_channel_init(struct driver_ctx_t *drv_ctx, void **comm_channel_h)
+{
+	int ret = 0;
+	struct comm_channel_ctx_t *comm_ctx = NULL;
+
+	if (WARN_ON(sizeof(struct comm_msg) > COMM_CHANNEL_FRAME_SZ))
+		return -EINVAL;
+
+	/* should not be an already instantiated. */
+	if (WARN_ON(!drv_ctx || !comm_channel_h || *comm_channel_h))
+		return -EINVAL;
+
+	/* start by allocating the comm ctx.*/
+	comm_ctx = kzalloc(sizeof(*comm_ctx), GFP_KERNEL);
+	if (WARN_ON(!comm_ctx))
+		return -ENOMEM;
+	mutex_init(&comm_ctx->cb_ops_lock);
+	atomic_set(&comm_ctx->recv_count, 0);
+
+	comm_ctx->pci_client_h = drv_ctx->pci_client_h;
+
+	/*
+	 * allocate fifo area, make it visible to peer. Assume same aperture
+	 * for peer access too.
+	 */
+	ret = allocate_fifo_memory(comm_ctx);
+	if (ret)
+		goto err;
+
+	/*
+	 * allocate notification for comm-channel, Assume same aperture
+	 * for peer access too.
+	 */
+	ret = init_notif_var(comm_ctx);
+	if (ret)
+		goto err;
+	/* we can now wait for notifications/messages to be received.*/
+	ret = start_msg_handling(comm_ctx);
+	if (ret)
+		goto err;
+
+	*comm_channel_h = comm_ctx;
+	return ret;
+err:
+	comm_channel_deinit((void **)&comm_ctx);
+	return ret;
+}
+
+void
+comm_channel_deinit(void **comm_channel_h)
+{
+	struct comm_channel_ctx_t *comm_ctx =
+				(struct comm_channel_ctx_t *)(*comm_channel_h);
+	if (!comm_ctx)
+		return;
+	stop_msg_handling(comm_ctx);
+	free_sync_primitive(comm_ctx);
+	free_fifo_memory(comm_ctx);
+	mutex_destroy(&comm_ctx->cb_ops_lock);
+	kfree(comm_ctx);
+
+	*comm_channel_h = NULL;
+}
+
+int
+comm_channel_register_msg_cb(void *comm_channel_h, enum comm_msg_type type,
+			     struct callback_ops *ops)
+{
+	int ret = 0;
+	struct callback_ops *cb_ops = NULL;
+	struct comm_channel_ctx_t *comm_ctx =
+				(struct comm_channel_ctx_t *)comm_channel_h;
+
+	if (WARN_ON(!comm_ctx || !ops || !ops->callback))
+		return -EINVAL;
+
+	if (WARN_ON(type <= COMM_MSG_TYPE_INVALID ||
+		    type >= COMM_MSG_TYPE_MAXIMUM))
+		return -EINVAL;
+
+	mutex_lock(&comm_ctx->cb_ops_lock);
+
+	cb_ops = &comm_ctx->cb_ops[type];
+	if (cb_ops->callback) {
+		pr_err("Callback for msg type: (%u) is already taken\n", type);
+		ret = -EBUSY;
+	} else {
+		cb_ops->callback = ops->callback;
+		cb_ops->ctx = ops->ctx;
+	}
+
+	mutex_unlock(&comm_ctx->cb_ops_lock);
+	return ret;
+}
+
+int
+comm_channel_unregister_msg_cb(void *comm_channel_h, enum comm_msg_type type)
+{
+	int ret = 0;
+	struct callback_ops *cb_ops = NULL;
+	struct comm_channel_ctx_t *comm_ctx =
+				(struct comm_channel_ctx_t *)comm_channel_h;
+
+	if (WARN_ON(!comm_ctx))
+		return -EINVAL;
+
+	if (WARN_ON(type <= COMM_MSG_TYPE_INVALID ||
+		    type >= COMM_MSG_TYPE_MAXIMUM))
+		return -EINVAL;
+
+	mutex_lock(&comm_ctx->cb_ops_lock);
+	cb_ops = &comm_ctx->cb_ops[type];
+	cb_ops->callback = NULL;
+	cb_ops->ctx = NULL;
+	mutex_unlock(&comm_ctx->cb_ops_lock);
+
+	return ret;
+}
diff --git a/drivers/misc/nvscic2c-pcie/comm-channel.h b/drivers/misc/nvscic2c-pcie/comm-channel.h
new file mode 100644
index 000000000000..3ba3c0237109
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/comm-channel.h
@@ -0,0 +1,126 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __COMM_CHANNEL_H__
+#define __COMM_CHANNEL_H__
+
+#include "nvscic2c-pcie-ioctl.h"
+#include "common.h"
+
+/* forward declaration. */
+struct driver_ctx_t;
+
+enum comm_msg_type {
+	/* invalid.*/
+	COMM_MSG_TYPE_INVALID = 0,
+	/*
+	 * One time message from peer @DRV_MODE_EPC (PCIe RP) towards
+	 * @DRV_MODE_EPF(PCIe EP) for boot-strap mechanism.
+	 */
+	COMM_MSG_TYPE_BOOTSTRAP,
+
+	/* Link status shared between @DRV_MODE_EPC and @DRV_MODE_EPF.*/
+	COMM_MSG_TYPE_LINK,
+
+	/* Share/Register export object with peer.*/
+	COMM_MSG_TYPE_REGISTER,
+
+	/* Unregister exported object back with peer.*/
+	COMM_MSG_TYPE_UNREGISTER,
+
+	/* return iova of  edma channel from peer */
+	COMM_MSG_TYPE_EDMA_RX_DESC_IOVA_RETURN,
+
+	/* Maximum. */
+	COMM_MSG_TYPE_MAXIMUM,
+};
+
+/*
+ * For @DRV_MODE_EPF(PCIe EP), it doesn't know the send area towards
+ * @DRV_MODE_EPC (PCIe RP - there is not BAR with PCIe RP). This is the first
+ * message and only once sent by @DRV_MODE_EPC towards @DRV_MODE_EPF for latter
+ * to configure it's outbound translation.
+ */
+struct comm_msg_bootstrap {
+	u64 iova;
+	enum peer_cpu_t peer_cpu;
+};
+
+/* Link status shared between @DRV_MODE_EPC and @DRV_MODE_EPF.
+ * Possible use: @DRV_MODE_EPC sends bootstrap message
+ * to @DRV_MODE_EPF without setting it's own PCIe link = UP, therefore,
+ * on compeleting initialization, @DRV_MODE_EPF(once bootstrap msg
+ * is received) shall send link = up message to @DRV_MODE_EPC.
+ */
+struct comm_msg_link {
+	enum nvscic2c_pcie_link status;
+};
+
+/* to simply,only one channel c2c remote edma case   */
+struct comm_msg_edma_rx_desc_iova {
+	dma_addr_t iova;
+};
+
+/*
+ * Private channel communication message sent by NvSciC2cPcie consumer
+ * towards producer to register the exported object at the NvSciC2cPcie
+ * producer SoC.
+ */
+struct comm_msg_register {
+	u64 export_desc;
+	u64 iova;
+	size_t size;
+	size_t offsetof;
+};
+
+/*
+ * Private channel communication message sent by NvSciC2cPcie producer
+ * towards onsumer to unregister it's exported object.
+ */
+struct comm_msg_unregister {
+	u64 export_desc;
+	u64 iova;
+	size_t size;
+	size_t offsetof;
+};
+
+struct comm_msg {
+	enum comm_msg_type type;
+	union data {
+		struct comm_msg_bootstrap bootstrap;
+		struct comm_msg_link link;
+		struct comm_msg_register reg;
+		struct comm_msg_unregister unreg;
+		struct comm_msg_edma_rx_desc_iova edma_rx_desc_iova;
+	} u;
+} __aligned(8);
+
+int
+comm_channel_init(struct driver_ctx_t *drv_ctx, void **comm_channel_h);
+
+void
+comm_channel_deinit(void **comm_channel_h);
+
+int
+comm_channel_msg_send(void *comm_channel_h, struct comm_msg *msg);
+
+int
+comm_channel_bootstrap_msg_send(void *comm_channel_h, struct comm_msg *msg);
+
+int
+comm_channel_register_msg_cb(void *comm_channel_h, enum comm_msg_type type,
+			     struct callback_ops *ops);
+int
+comm_channel_unregister_msg_cb(void *comm_channel_h, enum comm_msg_type type);
+#endif //__COMM_CHANNEL_H__
diff --git a/drivers/misc/nvscic2c-pcie/common.h b/drivers/misc/nvscic2c-pcie/common.h
new file mode 100644
index 000000000000..7410da1f69d9
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/common.h
@@ -0,0 +1,223 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __COMMON_H__
+#define __COMMON_H__
+
+#include <linux/types.h>
+#include <linux/bitops.h>
+#include <linux/limits.h>
+
+#define MODULE_NAME		"nvscic2c-pcie"
+#define DRIVER_NAME_EPF		"nvscic2c-pcie-epf"
+#define DRIVER_NAME_EPC		"nvscic2c-pcie-epc"
+
+/* STREAM_OBJ_TYPE. */
+#define STREAM_OBJ_TYPE_MEM	(0)
+#define STREAM_OBJ_TYPE_SYNC	(1)
+
+/*
+ * This capped number shall be used to derive export descriptor, therefore any
+ * change should be evaluated thoroughly.
+ */
+#define MAX_STREAM_MEMOBJS	(1024)
+
+/*
+ * This capped number shall be used to derive export descriptor, therefore any
+ * change should be evaluated thoroughly.
+ */
+#define MAX_STREAM_SYNCOBJS	(1024)
+
+/*
+ * This capped number shall be used to allocate export descriptors for using DMA
+ * engine of Tegra.
+ */
+#define MAX_EDMA_DESC		(1024)
+
+/*
+ * In a topology of interconnected Boards + SoCs.
+ *
+ * This capped number shall be used to derive export descriptor, therefore any
+ * change should be evaluated thoroughly.
+ */
+#define MAX_BOARDS		(16)
+#define MAX_SOCS		(16)
+#define MAX_PCIE_CNTRLRS	(16)
+/*
+ * Maximum NvSciIpc INTER_CHHIP(NvSciC2cPcie) endpoints that can be supported
+ * for single pair of PCIe RP<>EP connection (referred just as 'connection'
+ * henceforth). We have specific customer need for a set of Eleven NvSciC2cPcie
+ * endpoints for single connection.
+ *
+ * This capped number shall be used to derive export descriptor, therefore any
+ * change should be evaluated thoroughly.
+ */
+#define MAX_ENDPOINTS		(16)
+
+/*
+ * Each NvSciIpc INTER_CHIP(NvSciC2cPcie) endpoint shall require atleast one
+ * distinct notification Id (MSI/MSI-X, GIC SPI or Tegra-Sync primitive).
+ * Also, these notification mechanisms: MSI/MSI-X, GIC SPI, Tegra-Sync primitive are
+ * limited on SoC or per connection (configurable via device-tree).
+ *
+ * Also, there is a private communication channel between the two ends of a
+ * single connection that need notification Ids for message passing. Assuming
+ * this private communication channel to be a Queue-Pair (Cmd, Resp), need
+ * atleast 2 distinct notification Ids for it on a single connection.
+ */
+#define MIN_NUM_NOTIFY		(MAX_ENDPOINTS + (2))
+
+/*
+ * Sync primitives have size: 4KB on Xavier and 64KB on Orin.
+ * However, for our use-case, even if it is virtually mapped only 4 bytes of
+ * Sync primitive aperture to PCIe device, any writes (SZ_4B) from
+ * remote is enough to update the primitive.
+ *
+ */
+#define SP_SIZE			(4096)
+
+/*
+ * For NvStreams extensions over NvSciC2cPcie, an endpoint is a producer on
+ * one SoC and a corresponding consumer on the remote SoC. The role
+ * classification cannot be deduced in KMD.
+ */
+
+/*
+ * PCIe BAR aperture for Tx to/Rx from peer.
+ */
+struct pci_aper_t {
+	/* physical Pcie aperture.*/
+	phys_addr_t aper;
+
+	/* process virtual address for CPU access.*/
+	void __iomem *pva;
+
+	/* size of the perture.*/
+	size_t size;
+};
+
+/*
+ * DMA'able memory registered/exported to peer -
+ * either allocated by dma_buf API or physical pages pinned to
+ * pcie address space(dma_handle).
+ */
+struct dma_buff_t {
+	/* process virtual address for CPU access. */
+	void *pva;
+
+	/* iova(iommu=ON) or bus address/physical address for device access. */
+	dma_addr_t dma_handle;
+
+	/* physical address.*/
+	u64 phys_addr;
+
+	/* size of the memory allocated. */
+	size_t size;
+};
+
+/*
+ * CPU-only accessible memory which is not PCIe aper or PCIe
+ * DMA'able memory. This shall contain information of memory
+ * allocated via kalloc()/likewise.
+ */
+struct cpu_buff_t {
+	/* process virtual address for CPU access. */
+	void *pva;
+
+	/* (va->pa) physical address. */
+	u64 phys_addr;
+
+	/* size of the memory allocated. */
+	size_t size;
+};
+
+/*
+ * Callback options for user to register with occurrence of an event.
+ */
+struct callback_ops {
+	/*
+	 * User callback to be invoked.
+	 * @data: Event-type or likewise data. read-only for user.
+	 * @ctx: user ctx returned as-is in the callback.
+	 */
+	void (*callback)(void *data, void *ctx);
+
+	/* user context that shall be passed with @callback.*/
+	void *ctx;
+};
+
+/*
+ * Node information. A combination of Board + SoC + PCIe controller
+ * should be unique within the PCIe controllers/SoCs/Boards interconnected
+ for NvSciC2cPcie.
+ */
+struct node_info_t {
+	u32 board_id;
+	u32 soc_id;
+	u32 cntrlr_id;
+};
+
+/*
+ * NvSciC2CPcie IPC end point prop
+ *
+ */
+struct endpoint_prop_t {
+	/* Endpoint ID as populated from device tree node. */
+	u8 id;
+
+	/*
+	 * Human readable name of the endpoint - char dev node shall be
+	 * instanced using this name.
+	 */
+	char name[NAME_MAX];
+
+	/* Frames and per frame size.*/
+	u8 nframes;
+	u32 frame_sz;
+};
+
+/*
+ * NvSciC2cPcie either works as EndpointClient module - client driver for
+ * remote PCIe EP (runs on the PCIe RP SoC) or as EndpointFunction module -
+ * PCIe EP function driver (runs on the PCIe EP SoC).
+ */
+enum drv_mode_t {
+	/* Invalid. */
+	DRV_MODE_INVALID = 0,
+
+	/* Driver module runs as EndpointClient driver.*/
+	DRV_MODE_EPC,
+
+	/* Drive module runs as EndpointFunction driver.*/
+	DRV_MODE_EPF,
+
+	/* Maximum.*/
+	DRV_MODE_MAXIMUM,
+};
+
+/*
+ * soc information on root complex
+ */
+enum peer_cpu_t {
+	/* Peer CPU as Orin with EndpointFunction driver*/
+	NVCPU_ORIN = 0,
+
+	/* Peer CPU as X86 with EndpointClient driver*/
+	NVCPU_X86_64,
+
+	/* Maximum.*/
+	NVCPU_MAXIMUM,
+};
+
+#endif //__COMMON_H__
diff --git a/drivers/misc/nvscic2c-pcie/config.c b/drivers/misc/nvscic2c-pcie/config.c
new file mode 100644
index 000000000000..328c8c0bd1fe
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/config.c
@@ -0,0 +1,90 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include "common.h"
+#include "module.h"
+#include "config.h"
+
+#include <linux/errno.h>
+#include <linux/printk.h>
+
+struct endpoint_prop_t endpoint_props[NUM_ENDPOINTS] = ENDPOINT_DB;
+/*
+ * Debug only.
+ */
+static void
+cfg_print(struct driver_param_t *drv_param)
+{
+	u32 i = 0;
+	struct node_info_t *local_node = &drv_param->local_node;
+	struct node_info_t *peer_node = &drv_param->peer_node;
+
+	pr_debug("cfg parsing leads to:\n");
+	pr_debug("\tdriver mode  = (%s)\n", ((drv_param->drv_mode == DRV_MODE_EPC) ?
+					("epc") : ("epf")));
+	pr_debug("\tpci dev id   = 0x%x\n", drv_param->pci_dev_id);
+	pr_debug("\tNode information\n");
+	pr_debug("\t\tlocal board id = %u\n", local_node->board_id);
+	pr_debug("\t\tpeer board id  = %u\n", peer_node->board_id);
+	pr_debug("\t\tlocal soc id   = %u\n", local_node->soc_id);
+	pr_debug("\t\tpeer soc id    = %u\n", peer_node->soc_id);
+	pr_debug("\t\tlocal pcie cntrlr id = %u\n", local_node->cntrlr_id);
+	pr_debug("\t\tpeer pcie cntrlr id  = %u\n", peer_node->cntrlr_id);
+	if (drv_param->drv_mode == DRV_MODE_EPF)
+		pr_debug("\tbar win size = 0x%x\n", drv_param->bar_win_size);
+	pr_debug("\ttotal endpoints	= (%u)\n", drv_param->nr_endpoint);
+	for (i = 0; i < drv_param->nr_endpoint; i++) {
+		struct endpoint_prop_t *prop = NULL;
+
+		prop = &drv_param->endpoint_props[i];
+		pr_debug("\t\t(%s)::\n", prop->name);
+		pr_debug("\t\t\tnframes   = (%02u) frame_size=(%08u)",
+		    prop->nframes, prop->frame_sz);
+	}
+	pr_debug("cfg parsing ends\n");
+}
+
+/*
+ * Look-up device tree node for the compatible string. Check for the
+ * pci-dev-id within the compatible node, if more than one such node found also
+ * return error.
+ */
+int
+cfg_parse(u32 pci_dev_id, enum drv_mode_t drv_mode,
+	 struct driver_param_t *drv_param)
+{
+	int ret = 0;
+	int i = 0;
+	struct node_info_t local_node = LOCAL_NODE;
+	struct node_info_t peer_node = PEER_NODE;
+
+	if (WARN_ON(!pci_dev_id))
+		return -EINVAL;
+
+	if (WARN_ON(!drv_param))
+		return -EINVAL;
+
+	memset(drv_param, 0x0, sizeof(*drv_param));
+	drv_param->drv_mode = drv_mode;
+	drv_param->pci_dev_id = pci_dev_id;
+	drv_param->local_node = local_node;
+	drv_param->peer_node  = peer_node;
+
+	for (i = 0; i < NUM_ENDPOINTS; i++)
+		drv_param->endpoint_props[i] = endpoint_props[i];
+	drv_param->nr_endpoint = NUM_ENDPOINTS;
+	/* all okay.*/
+	cfg_print(drv_param);
+	return ret;
+}
diff --git a/drivers/misc/nvscic2c-pcie/config.h b/drivers/misc/nvscic2c-pcie/config.h
new file mode 100644
index 000000000000..6ea640c9a577
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/config.h
@@ -0,0 +1,49 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef NVSCIC2C_X86_PCIE_DT_H
+#define NVSCIC2C_X86_PCIE_DT_H
+
+/* To match the DT node in Orin EPF */
+#define PCI_DEVICE_ID_NVIDIA_C2C_2	0x22CC
+
+#define MAX_NVSCIC2C_IRQ		(32)
+#define COMM_CHANNEL_IRQ		(0)
+#define ENDPOINT_IRQ_INDEX		(1)
+#define EDMA_REMOTE_IRQ			(MAX_NVSCIC2C_IRQ - 1)
+
+/* Endpoints configures to match the C6 as the default PCIE EP on Orin */
+/* The NUM_ENDPOINTS is configurable and can be changed based on the number of
+ * NvSciStream required for a usecase. The corresponding changes will also be
+ * needed in ENDPOINT_DB below.
+ */
+#define NUM_ENDPOINTS                   (11)
+#define LOCAL_NODE                      { .board_id = 0, .soc_id = 0, .cntrlr_id = 5 }
+#define PEER_NODE                       { .board_id = 0, .soc_id = 0, .cntrlr_id = 6 }
+#define ENDPOINT_DB { \
+{ .id = 0,  .name = "nvscic2c_pcie_s0_c5_1",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 1,  .name = "nvscic2c_pcie_s0_c5_2",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 2,  .name = "nvscic2c_pcie_s0_c5_3",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 3,  .name = "nvscic2c_pcie_s0_c5_4",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 4,  .name = "nvscic2c_pcie_s0_c5_5",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 5,  .name = "nvscic2c_pcie_s0_c5_6",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 6,  .name = "nvscic2c_pcie_s0_c5_7",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 7,  .name = "nvscic2c_pcie_s0_c5_8",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 8,  .name = "nvscic2c_pcie_s0_c5_9",  .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 9,  .name = "nvscic2c_pcie_s0_c5_10", .nframes = 16,  .frame_sz = 32768 }, \
+{ .id = 10, .name = "nvscic2c_pcie_s0_c5_11", .nframes = 16,  .frame_sz = 32768 } \
+}
+
+#endif /* !NVSCIC2C_X86_PCIE_DT_H */
+
diff --git a/drivers/misc/nvscic2c-pcie/descriptor.h b/drivers/misc/nvscic2c-pcie/descriptor.h
new file mode 100644
index 000000000000..2ee56ee4ac4a
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/descriptor.h
@@ -0,0 +1,172 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __DESCRIPTOR_H__
+#define __DESCRIPTOR_H__
+
+#include "common.h"
+#include <linux/errno.h>
+
+/* Magic code for descriptor.*/
+#define DESC_MAGIC_CODE_32BIT	(0x69152734)
+
+/*
+ * Format of Export Descriptor (at the moment)
+ * 0xXXXXXXXXRRRREIII
+ * 32bit(XXXXXXXX00000000): Reserved.
+ * 04bit(00000000B0000000): Peer Board Id.
+ * 04bit(000000000S000000): Peer SoC Id.
+ * 04bit(0000000000C00000): Peer PCIe Controller Id.
+ * 04bit(00000000000E0000): Endpoint Id.
+ * 04bit(000000000000X000): Reserved.
+ * 12bit(0000000000000III): Obj type(1bit) + Obj Id(11bits).
+ *                          (Bit 11 : ObjType - Mem/Sync)
+ *                          (Bit 0-10 : ObjId - Mem or Sync Obj Id)
+ *
+ * Board Id and SoC Id together can be a Node Id to allow for cases, where SoC
+ * on a single board: [0-63] and number of boards: [0-3]. Essentially uniquely
+ * identifying each SoC inter-connected within or across the boards.
+ */
+
+/*
+ * Topology can have a:
+ * A or a Set of boards
+ *   - (Assumed [0, 15]).
+ * Each Board can have a or a set of SoC(s)
+ *   - ID : [0, 15].
+ * Each SoC can have a or a set of PCIe controllers either in RP or EP mode.
+ *   - ID:  [0, 15].
+ * Each Controller can have a or a set of NvSciIpc INTER_CHIP endpoints.
+ *   - ID:  [0, 15].
+ * Each NvSciIpc INTER_CHIP can export either a Mem object or Sync object
+ *   - STREAM_OBJ_TYPE_MEM or STREAM_OBJ_TYPE_SYNC
+ *   - Type: [0, 1].
+ * Each NvSciIpc INTER_CHIP can export a set of either Mem or Sync objects.
+ *   - ID:   [0, 2047].
+ */
+struct descriptor_bit_t {
+	u64 reserved1     : 32;
+	u64 board_id      : 4;
+	u64 soc_id        : 4;
+	u64 cntrlr_id     : 4;
+	u64 endpoint_id   : 4;
+	u64 reserved2     : 4;
+	u64 handle_type   : 1;
+	u64 handle_id     : 11;
+};
+
+/* bit-field manipulation. */
+union descriptor_t {
+	u64 value;
+	struct descriptor_bit_t bit;
+};
+
+/* Generate a descriptor (auth token) */
+static inline u64
+gen_desc(u32 peer_board_id, u32 peer_soc_id, u32 peer_cntrlr_id, u32 ep_id,
+	 u32 handle_type, u32 handle_id)
+{
+	union descriptor_t desc;
+
+	desc.bit.reserved1 = DESC_MAGIC_CODE_32BIT;
+	desc.bit.board_id = peer_board_id;
+	desc.bit.soc_id = peer_soc_id;
+	desc.bit.cntrlr_id = peer_cntrlr_id;
+	desc.bit.endpoint_id = ep_id;
+	desc.bit.handle_type = handle_type;
+	desc.bit.handle_id = handle_id;
+
+	return desc.value;
+}
+
+/* Validate a descriptor (auth token) */
+static inline int
+validate_desc(u64 in_desc, u32 local_board_id, u32 local_soc_id,
+	      u32 local_cntrlr_id, u32 ep_id)
+{
+	int ret = 0;
+	union descriptor_t desc;
+
+	desc.value = in_desc;
+	if (desc.bit.reserved1 != DESC_MAGIC_CODE_32BIT ||
+	    desc.bit.board_id != local_board_id ||
+	    desc.bit.soc_id != local_soc_id ||
+	    desc.bit.cntrlr_id != local_cntrlr_id ||
+	    desc.bit.endpoint_id != ep_id) {
+		ret = -EINVAL;
+	}
+
+	return ret;
+}
+
+/* return handle type embedded in the descriptor (auth token) */
+static inline u32
+get_handle_type_from_desc(u64 in_desc)
+{
+	union descriptor_t desc;
+
+	desc.value = in_desc;
+	return (u32)desc.bit.handle_type;
+}
+
+/*
+ * Board Id, SoC Id, PCIe Controller Id should not be beyond 16 [0-15] - We have
+ * reserved 4b each for boardId to generate export descriptor.
+ */
+#if MAX_BOARDS > (0xF + 1)
+	#error MAX_BOARDS assumed to be less-than or equal to (16)
+#endif
+#if MAX_SOCS > (0xF + 1)
+	#error MAX_SOCS assumed to be less-than or equal to (16)
+#endif
+#if MAX_PCIE_CNTRLRS > (0xF + 1)
+	#error MAX_PCIE_CNTRLRS assumed to be less-than or equal to (16)
+#endif
+
+/*
+ * Endpoints should not be beyond 16 [0-15] - We have reserved 4b for
+ * endpoint Id to generate export descriptor (although we could use
+ * up the reserved2 if needed).
+ */
+#if MAX_ENDPOINTS > (0xF + 1)
+	#error MAX_ENDPOINTS to be less than or equal to (16)
+#endif
+
+/*
+ * Memory or Sync object indicator in descriptor should not be beyond 1 [0-1].
+ * The value must be less than 1 as the descriptor accounts just 1b (1bit).
+ */
+#if STREAM_OBJ_TYPE_MEM > (0x1)
+	#error STREAM_OBJ_TYPE_MEM to be less-than or equal-to (1)
+#endif
+#if STREAM_OBJ_TYPE_SYNC > (0x1)
+	#error STREAM_OBJ_TYPE_SYNC to be less-than or equal-to (1)
+#endif
+
+/*
+ * Mem objects should not be beyond 2048 [0-2047] - We have reserved 11b for
+ * Obj Id to generate export descriptor.
+ */
+#if MAX_STREAM_MEMOBJS > (0x7FF + 1)
+	#error MAX_STREAM_MEMOBJS to be less than or equal to (2048)
+#endif
+
+/*
+ * Sync objects should not be beyond 2048 [0-2047] - We have reserved 11b for
+ * Obj Id to generate export descriptor.
+ */
+#if MAX_STREAM_SYNCOBJS > (0x7FF + 1)
+	#error MAX_STREAM_SYNCOBJS to be less than or equal to (2048)
+#endif
+#endif //__DESCRIPTOR_H__
diff --git a/drivers/misc/nvscic2c-pcie/endpoint.c b/drivers/misc/nvscic2c-pcie/endpoint.c
new file mode 100644
index 000000000000..1a21edfa7c1d
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/endpoint.c
@@ -0,0 +1,1015 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvscic2c-pcie: endpoint: " fmt
+
+#include <linux/atomic.h>
+#include <linux/cdev.h>
+#include <linux/dma-iommu.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/poll.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/wait.h>
+
+#include "common.h"
+#include "endpoint.h"
+#include "module.h"
+#include "pci-client.h"
+#include "stream-extensions.h"
+#include "config.h"
+
+#define PCIE_STATUS_CHANGE_ACK_TIMEOUT (2000)
+
+/*
+ * Masked offsets to return to user, allowing them to mmap
+ * different memory segments of endpoints in user-space.
+ */
+enum mem_mmap_type {
+	/* Invalid.*/
+	MEM_MMAP_INVALID = 0,
+	/* Map Peer PCIe aperture: For Tx across PCIe.*/
+	PEER_MEM_MMAP,
+	/* Map Self PCIe shared memory: For Rx across PCIe.*/
+	SELF_MEM_MMAP,
+	/* Map Link memory segment to query link status with Peer.*/
+	LINK_MEM_MMAP,
+	/* Maximum. */
+	MEM_MAX_MMAP,
+};
+
+/* sync primitive handling. */
+struct syncpt_t {
+	/* PCIe aperture for writes to peer sync primitive for same the endpoint. */
+	struct pci_aper_t peer_mem;
+
+	/* sync primitive physical address for stritching to PCIe BAR backing.*/
+	size_t size;
+	phys_addr_t phy_addr;
+
+	/* for mapping above physical pages to iova of client choice.*/
+	void *iova_block_h;
+	u64 iova;
+	bool mapped_iova;
+};
+
+/* private data structure for each endpoint. */
+struct endpoint_t {
+	/* properties / attributes of this endpoint.*/
+	char name[NAME_MAX];
+
+	/* char device management.*/
+	int minor;
+	dev_t dev;
+	struct cdev cdev;
+	struct device *device;
+
+	/* slot/frames this endpoint is divided into honoring alignment.*/
+	u32 nframes;
+	u32 frame_sz;
+
+	/* allocated physical memory info for mmap.*/
+	struct cpu_buff_t self_mem;
+
+	/* mapping physical pages to iova of client choice.*/
+	void *iova_block_h;
+	u64 iova;
+	bool mapped_iova;
+
+	/* PCIe aperture for writes to peer over pcie. */
+	struct pci_aper_t peer_mem;
+
+	/* poll/notifications.*/
+	wait_queue_head_t waitq;
+
+	/* sync primitive for notifications (rx). */
+	struct syncpt_t syncpt;
+
+	/* x86 RP(rx) msi-x irq <- orin EP (tx)*/
+	u32 msi_x_irq;
+
+	/* book-keeping of peer notifications.*/
+	atomic_t dataevent_count;
+
+	/* book-keeping of PCIe link event.*/
+	atomic_t linkevent_count;
+	u32 linkevent_id;
+
+	/* propagate events when endpoint was initialized.*/
+	atomic_t event_handling;
+
+	/* serialise access to fops.*/
+	struct mutex fops_lock;
+	atomic_t in_use;
+
+	bool link_status_ack_frm_usr;
+	wait_queue_head_t ack_waitq;
+	wait_queue_head_t close_waitq;
+
+	/* pci client handle.*/
+	void *pci_client_h;
+
+	/* stream extensions.*/
+	struct stream_ext_params stream_ext_params;
+	void *stream_ext_h;
+};
+
+/* Overall context for the endpoint sub-module of  nvscic2c-pcie driver.*/
+struct endpoint_drv_ctx_t {
+	/* entire char device region allocated for all endpoints.*/
+	dev_t char_dev;
+
+	/* every endpoint char device will be registered to this class.*/
+	struct class *class;
+
+	/* array of nvscic2c-pcie endpoint logical devices.*/
+	u8 nr_endpoint;
+	struct endpoint_t *endpoints;
+};
+
+/*
+ * pci-client would raise this callback only when there is change
+ * in PCIe link status(up->down OR down->up).
+ */
+static void
+link_event_callback(void *event_type, void *ctx);
+
+/* prototype. */
+static int
+enable_event_handling(struct endpoint_t *endpoint);
+
+/* prototype. */
+static int
+disable_event_handling(struct endpoint_t *endpoint);
+
+/* prototype. */
+static int
+ioctl_notify_remote_impl(struct endpoint_t *endpoint);
+
+/* prototype. */
+static int
+link_change_ack(struct endpoint_t *endpoint,
+		struct nvscic2c_link_change_ack *ack);
+
+/* prototype. */
+static int
+ioctl_get_info_impl(struct endpoint_t *endpoint,
+		    struct nvscic2c_pcie_endpoint_info *get_info);
+
+/*
+ * open() syscall backing for nvscic2c-pcie endpoint devices.
+ *
+ * Populate the endpoint_device internal data-structure into fops private data
+ * for subsequent calls to other fops handlers.
+ */
+static int
+endpoint_fops_open(struct inode *inode, struct file *filp)
+{
+	int ret = 0;
+	struct endpoint_t *endpoint =
+		container_of(inode->i_cdev, struct endpoint_t, cdev);
+
+	mutex_lock(&endpoint->fops_lock);
+	if (atomic_read(&endpoint->in_use)) {
+		/* already in use.*/
+		mutex_unlock(&endpoint->fops_lock);
+		return -EBUSY;
+	}
+	/* create stream extension handle.*/
+	ret = stream_extension_init(&endpoint->stream_ext_params,
+				    &endpoint->stream_ext_h);
+	if (ret) {
+		pr_err("Failed setting up stream extension handle: (%s)\n",
+		       endpoint->name);
+		goto err;
+	}
+
+	/* start link, data event handling.*/
+	ret = enable_event_handling(endpoint);
+	if (ret) {
+		pr_err("(%s): Failed to enable link, syncpt event handling\n",
+		       endpoint->name);
+		stream_extension_deinit(&endpoint->stream_ext_h);
+		goto err;
+	}
+
+	filp->private_data = endpoint;
+	endpoint->link_status_ack_frm_usr = true;
+	atomic_set(&endpoint->in_use, 1);
+err:
+	mutex_unlock(&endpoint->fops_lock);
+	return ret;
+}
+
+/* close() syscall backing for nvscic2c-pcie endpoint devices.*/
+static int
+endpoint_fops_release(struct inode *inode, struct file *filp)
+{
+	int ret = 0;
+	struct nvscic2c_link_change_ack ack = {0};
+	struct endpoint_t *endpoint = filp->private_data;
+
+	if (!endpoint)
+		return ret;
+
+	mutex_lock(&endpoint->fops_lock);
+	if (atomic_read(&endpoint->in_use)) {
+		disable_event_handling(endpoint);
+		stream_extension_deinit(&endpoint->stream_ext_h);
+		ack.done = false;
+		link_change_ack(endpoint, &ack);
+		atomic_set(&endpoint->in_use, 0);
+		wake_up_interruptible_all(&endpoint->close_waitq);
+	}
+	stream_extension_deinit(&endpoint->stream_ext_h);
+	filp->private_data = NULL;
+	mutex_unlock(&endpoint->fops_lock);
+
+	return ret;
+}
+
+/*
+ * mmap() syscall backing for nvscic2c-pcie endpoint device.
+ *
+ * We support mapping following distinct regions of memory:
+ * - Peer's memory for same endpoint(used for Tx),
+ * - Self's memory (used for Rx),
+ * - pci-client link status memory.
+ *
+ * We map just one segment of memory in each call based on the information
+ * (which memory segment) provided by user-space code.
+ */
+static int
+endpoint_fops_mmap(struct file *filp, struct vm_area_struct *vma)
+{
+	struct endpoint_t *endpoint = filp->private_data;
+	u64 mmap_type = vma->vm_pgoff;
+	u64 memaddr = 0x0;
+	u64 memsize = 0x0;
+	int ret = 0;
+
+	if (WARN_ON(!endpoint))
+		return -EFAULT;
+
+	if (WARN_ON(!(vma)))
+		return -EFAULT;
+
+	mutex_lock(&endpoint->fops_lock);
+
+	switch (mmap_type) {
+	case PEER_MEM_MMAP:
+		vma->vm_page_prot = pgprot_device(vma->vm_page_prot);
+		memaddr = endpoint->peer_mem.aper;
+		memsize = endpoint->peer_mem.size;
+		break;
+	case SELF_MEM_MMAP:
+		memaddr = endpoint->self_mem.phys_addr;
+		memsize = endpoint->self_mem.size;
+		break;
+	case LINK_MEM_MMAP:
+		if (vma->vm_flags & VM_WRITE) {
+			ret = -EPERM;
+			pr_err("(%s): LINK_MEM_MMAP called with WRITE prot\n",
+			       endpoint->name);
+			goto exit;
+		}
+		ret = pci_client_mmap_link_mem(endpoint->pci_client_h, vma);
+		goto exit;
+	default:
+		pr_err("(%s): unrecognised mmap type: (%llu)\n",
+		       endpoint->name, mmap_type);
+		goto exit;
+	}
+
+	if ((vma->vm_end - vma->vm_start) != memsize) {
+		pr_err("(%s): mmap type: (%llu), memsize mismatch\n",
+		       endpoint->name, mmap_type);
+		goto exit;
+	}
+
+	vma->vm_pgoff  = 0;
+	vma->vm_flags |= (VM_DONTCOPY); // fork() not supported.
+	ret = remap_pfn_range(vma, vma->vm_start,
+			      PFN_DOWN(memaddr),
+			      memsize, vma->vm_page_prot);
+	if (ret) {
+		pr_err("(%s): mmap() failed, mmap type:(%llu)\n",
+		       endpoint->name, mmap_type);
+	}
+exit:
+	mutex_unlock(&endpoint->fops_lock);
+	return ret;
+}
+
+/*
+ * poll() syscall backing for nvscic2c-pcie endpoint devices.
+ *
+ * user-space code shall call poll with FD on read, write and probably exception
+ * for endpoint state changes.
+ *
+ * If we are able to read(), write() or there is a pending state change event
+ * to be serviced, we return letting application call get_event(), otherwise
+ * kernel f/w will wait for waitq activity to occur.
+ */
+static unsigned int
+endpoint_fops_poll(struct file *filp, poll_table *wait)
+{
+	unsigned int mask = 0;
+	struct endpoint_t *endpoint = filp->private_data;
+
+	if (WARN_ON(!endpoint))
+		return -EFAULT;
+
+	mutex_lock(&endpoint->fops_lock);
+
+	/* add all waitq if they are different for read, write & link+state.*/
+	poll_wait(filp, &endpoint->waitq, wait);
+
+	/*
+	 * wake up read, write (& exception - those who want to use) fd on
+	 * getting Link + peer notifications.
+	 */
+	if (atomic_read(&endpoint->linkevent_count)) {
+		atomic_dec(&endpoint->linkevent_count);
+		mask = (POLLPRI | POLLIN | POLLOUT);
+	} else if (atomic_read(&endpoint->dataevent_count)) {
+		atomic_dec(&endpoint->dataevent_count);
+		mask = (POLLPRI | POLLIN | POLLOUT);
+	}
+
+	mutex_unlock(&endpoint->fops_lock);
+
+	return mask;
+}
+
+/* ioctl() syscall backing for nvscic2c-pcie endpoint device. */
+#define MAX_IOCTL_ARG_SIZE (sizeof(union nvscic2c_pcie_ioctl_arg_max_size))
+static long
+endpoint_fops_ioctl(struct file *filp, unsigned int cmd, unsigned long arg)
+{
+	int ret = 0;
+	u8 buf[MAX_IOCTL_ARG_SIZE] __aligned(sizeof(u64)) = {0};
+	struct endpoint_t *endpoint = filp->private_data;
+
+	if (WARN_ON(!endpoint))
+		return -EFAULT;
+
+	if (WARN_ON(_IOC_TYPE(cmd) != NVSCIC2C_PCIE_IOCTL_MAGIC ||
+		    _IOC_NR(cmd) == 0 ||
+		    _IOC_NR(cmd) > NVSCIC2C_PCIE_IOCTL_NUMBER_MAX) ||
+		    _IOC_SIZE(cmd) > MAX_IOCTL_ARG_SIZE)
+		return -ENOTTY;
+
+	/* copy the cmd if it was meant from user->kernel. */
+	(void)memset(buf, 0, sizeof(buf));
+	if (_IOC_DIR(cmd) & _IOC_WRITE) {
+		if (copy_from_user(buf, (void __user *)arg, _IOC_SIZE(cmd)))
+			return -EFAULT;
+	}
+
+	mutex_lock(&endpoint->fops_lock);
+	switch (cmd) {
+	case NVSCIC2C_PCIE_IOCTL_GET_INFO:
+		ret = ioctl_get_info_impl
+			(endpoint, (struct nvscic2c_pcie_endpoint_info *)buf);
+		break;
+	case NVSCIC2C_PCIE_IOCTL_NOTIFY_REMOTE:
+		ret = ioctl_notify_remote_impl(endpoint);
+		break;
+	case NVSCIC2C_PCIE_LINK_STATUS_CHANGE_ACK:
+		link_change_ack(endpoint,
+				(struct nvscic2c_link_change_ack *)buf);
+		break;
+	default:
+		ret = stream_extension_ioctl(endpoint->stream_ext_h, cmd, buf);
+		break;
+	}
+	mutex_unlock(&endpoint->fops_lock);
+
+	/* copy the cmd result back to user if it was kernel->user: get_info.*/
+	if (ret == 0 && (_IOC_DIR(cmd) & _IOC_READ))
+		ret = copy_to_user((void __user *)arg, buf, _IOC_SIZE(cmd));
+	return ret;
+}
+
+/*
+ * All important endpoint dev node properites required for user-space
+ * to map the channel memory and work without going to LKM for data
+ * xfer are exported in this ioctl implementation.
+ *
+ * Because we export different memory for a single nvscic2c-pcie endpoint,
+ * export the memory regions as masked offsets.
+ */
+static int
+ioctl_get_info_impl(struct endpoint_t *endpoint,
+		    struct nvscic2c_pcie_endpoint_info *get_info)
+{
+	get_info->nframes     = endpoint->nframes;
+	get_info->frame_size  = endpoint->frame_sz;
+	get_info->peer.offset = (PEER_MEM_MMAP << PAGE_SHIFT);
+	get_info->peer.size   = endpoint->peer_mem.size;
+	get_info->self.offset = (SELF_MEM_MMAP << PAGE_SHIFT);
+	get_info->self.size   = endpoint->self_mem.size;
+	get_info->link.offset = (LINK_MEM_MMAP << PAGE_SHIFT);
+	get_info->link.size   = PAGE_ALIGN(sizeof(enum nvscic2c_pcie_link));
+
+	return 0;
+}
+
+/*
+ * implement NVSCIC2C_PCIE_IOCTL_NOTIFY_REMOTE ioctl call.
+ */
+static int
+ioctl_notify_remote_impl(struct endpoint_t *endpoint)
+{
+	int ret = 0;
+	enum nvscic2c_pcie_link link = NVSCIC2C_PCIE_LINK_DOWN;
+	struct syncpt_t *syncpt = &endpoint->syncpt;
+
+	link = pci_client_query_link_status(endpoint->pci_client_h);
+
+	if (link != NVSCIC2C_PCIE_LINK_UP)
+		return -ENOLINK;
+
+	/*
+	 * increment peer's sync. Write of any 4-byte value
+	 * increments Tegra's sync primitive by 1.
+	 */
+	writel(0x1, syncpt->peer_mem.pva);
+	return ret;
+}
+
+static int
+link_change_ack(struct endpoint_t *endpoint,
+		struct nvscic2c_link_change_ack *ack)
+{
+	endpoint->link_status_ack_frm_usr = ack->done;
+	wake_up_interruptible_all(&endpoint->ack_waitq);
+
+	return 0;
+}
+
+static int
+enable_event_handling(struct endpoint_t *endpoint)
+{
+	int ret = 0;
+
+	/*
+	 * propagate link and state change events that occur after the device
+	 * is opened and not the stale ones.
+	 */
+	atomic_set(&endpoint->dataevent_count, 0);
+	atomic_set(&endpoint->linkevent_count, 0);
+	atomic_set(&endpoint->event_handling, 1);
+	return ret;
+}
+
+static int
+disable_event_handling(struct endpoint_t *endpoint)
+{
+	int ret = 0;
+
+	if (!endpoint)
+		return ret;
+
+	atomic_set(&endpoint->event_handling, 0);
+	atomic_set(&endpoint->linkevent_count, 0);
+	atomic_set(&endpoint->dataevent_count, 0);
+
+	return ret;
+}
+
+static void
+link_event_callback(void *data, void *ctx)
+{
+	struct endpoint_t *endpoint = NULL;
+
+	if (!ctx) {
+		pr_err("Spurious link event callback\n");
+		return;
+	}
+
+	endpoint = (struct endpoint_t *)(ctx);
+
+	/* notify only if the endpoint was openend.*/
+	if (atomic_read(&endpoint->event_handling)) {
+		atomic_inc(&endpoint->linkevent_count);
+		wake_up_interruptible_all(&endpoint->waitq);
+	}
+}
+
+/*
+ * X86 RP callback handing irq raized from Orin EP
+ *
+ */
+static irqreturn_t
+syncpt_callback(int irq, void *data)
+{
+	struct endpoint_t *endpoint = (struct endpoint_t *)(data);
+	/* notify only if the endpoint was openend - else drain.*/
+	if (atomic_read(&endpoint->event_handling)) {
+		atomic_inc(&endpoint->dataevent_count);
+		wake_up_interruptible_all(&endpoint->waitq);
+	}
+	return IRQ_HANDLED;
+}
+
+/*
+ * unpin/unmap and free the sync primitive allocated.
+ */
+static void
+free_sync_primitive(struct endpoint_drv_ctx_t *eps_ctx,
+		    struct endpoint_t *endpoint)
+{
+	struct syncpt_t *syncpt = NULL;
+	struct pci_dev *pdev = pci_client_get_pci_dev(endpoint->pci_client_h);
+
+	if (!eps_ctx || !endpoint)
+		return;
+
+	syncpt = &endpoint->syncpt;
+
+	if (syncpt->peer_mem.pva) {
+		iounmap(syncpt->peer_mem.pva);
+		syncpt->peer_mem.pva = NULL;
+	}
+
+	if (syncpt->mapped_iova) {
+		pci_client_unmap_addr(endpoint->pci_client_h,
+				      syncpt->iova, syncpt->size);
+		syncpt->mapped_iova = false;
+	}
+
+	if (syncpt->iova_block_h) {
+		pci_client_free_iova(endpoint->pci_client_h,
+				     &syncpt->iova_block_h);
+		syncpt->iova_block_h = NULL;
+	}
+	/* free irq assocated with endpoint */
+	if ((pdev) && (endpoint))
+		free_irq(pci_irq_vector(pdev, endpoint->msi_x_irq), endpoint);
+}
+
+/*
+ * Map/pin the peer sync primitive to PCIe BAR backing.
+ */
+static int
+init_notif_var(struct endpoint_drv_ctx_t *eps_ctx,
+	       struct endpoint_t *endpoint)
+{
+	int ret = 0;
+	size_t offsetof = 0x0;
+	struct syncpt_t *syncpt = NULL;
+	struct pci_dev *pdev = NULL;
+
+	if (WARN_ON(!endpoint))
+		return -EINVAL;
+
+	pdev = pci_client_get_pci_dev(endpoint->pci_client_h);
+	if (WARN_ON(!pdev))
+		return -EINVAL;
+
+	/* sync point irq to Orin EP */
+	syncpt = &endpoint->syncpt;
+	syncpt->size = SP_SIZE;
+	/* reserve iova with the iova manager.*/
+	ret = pci_client_alloc_iova(endpoint->pci_client_h, syncpt->size,
+				    &syncpt->iova, &offsetof,
+				    &syncpt->iova_block_h);
+	if (ret) {
+		pr_err("(%s): Err reserving iova region of size(SP): (%lu)\n",
+		       endpoint->name, syncpt->size);
+		goto err;
+	}
+
+	/* map the pages to the reserved iova.*/
+	syncpt->mapped_iova = true;
+
+	pr_debug("(%s): mapped phy:0x%pa[p]+0x%lx to iova:0x%llx\n",
+		 endpoint->name, &syncpt->phy_addr, syncpt->size, syncpt->iova);
+
+	/* get peer's aperture offset. Map tx (pcie aper for notif tx.)*/
+	syncpt->peer_mem.size = syncpt->size;
+	ret = pci_client_get_peer_aper(endpoint->pci_client_h, offsetof,
+				       syncpt->peer_mem.size,
+				       &syncpt->peer_mem.aper);
+	if (ret) {
+		pr_err("Failed to get comm peer's syncpt pcie aperture\n");
+		goto err;
+	}
+	syncpt->peer_mem.pva = ioremap(syncpt->peer_mem.aper,
+				       syncpt->peer_mem.size);
+	if (!syncpt->peer_mem.pva) {
+		ret = -ENOMEM;
+		pr_err("(%s): Failed to ioremap peer's syncpt pcie aperture\n",
+		       endpoint->name);
+		goto err;
+	}
+	/* msi-x irq  from Orin EP */
+	ret = request_irq(pci_irq_vector(pdev, endpoint->msi_x_irq),
+			  syncpt_callback, 0, endpoint->name, endpoint);
+
+	return ret;
+err:
+	free_sync_primitive(eps_ctx, endpoint);
+	return ret;
+}
+
+/* unmap the memory from PCIe BAR iova and free the allocated physical pages. */
+static void
+free_memory(struct endpoint_drv_ctx_t *eps_ctx, struct endpoint_t *endpoint)
+{
+	if (!eps_ctx || !endpoint)
+		return;
+
+	if (endpoint->mapped_iova) {
+		pci_client_unmap_addr(endpoint->pci_client_h,
+				      endpoint->iova, endpoint->self_mem.size);
+		endpoint->mapped_iova = false;
+	}
+
+	if (endpoint->iova_block_h) {
+		pci_client_free_iova(endpoint->pci_client_h,
+				     &endpoint->iova_block_h);
+		endpoint->iova_block_h = NULL;
+	}
+
+	if (endpoint->self_mem.pva) {
+		free_pages_exact(endpoint->self_mem.pva,
+				 endpoint->self_mem.size);
+		endpoint->self_mem.pva = NULL;
+	}
+}
+
+/*
+ * allocate coniguous physical memory for endpoint. This shall be mapped
+ * to PCIe BAR iova.
+ */
+static int
+allocate_memory(struct endpoint_drv_ctx_t *eps_ctx, struct endpoint_t *ep)
+{
+	int ret = 0;
+	int prot = 0;
+	size_t offsetof = 0x0;
+
+	/*
+	 * memory size includes space for frames(aligned to PAGE_SIZE) plus
+	 * one additional PAGE for frames header (managed/used by user-space).
+	 */
+	ep->self_mem.size = (ep->nframes * ep->frame_sz);
+	ep->self_mem.size = ALIGN(ep->self_mem.size, PAGE_SIZE);
+	ep->self_mem.size += PAGE_SIZE;
+	ep->self_mem.pva = alloc_pages_exact(ep->self_mem.size,
+					     (GFP_KERNEL | __GFP_ZERO));
+	if (!ep->self_mem.pva) {
+		ret = -ENOMEM;
+		pr_err("(%s): Error allocating: (%lu) contiguous pages\n",
+		       ep->name, (ep->self_mem.size >> PAGE_SHIFT));
+		goto err;
+	}
+	ep->self_mem.phys_addr = page_to_phys(virt_to_page(ep->self_mem.pva));
+
+	pr_debug("(%s): physical page allocated at:(0x%pa[p]+0x%lx)\n",
+		 ep->name, &ep->self_mem.phys_addr, ep->self_mem.size);
+
+	/* reserve iova with the iova manager.*/
+	ret = pci_client_alloc_iova(ep->pci_client_h, ep->self_mem.size,
+				    &ep->iova, &offsetof, &ep->iova_block_h);
+	if (ret) {
+		pr_err("(%s): Failed to reserve iova region of size: 0x%lx\n",
+		       ep->name, ep->self_mem.size);
+		goto err;
+	}
+
+	/* map the pages to the reserved iova.*/
+	prot = (IOMMU_CACHE | IOMMU_READ | IOMMU_WRITE);
+	ret = pci_client_map_addr(ep->pci_client_h, ep->iova,
+				  ep->self_mem.phys_addr, ep->self_mem.size,
+				  prot);
+	if (ret) {
+		pr_err("(%s): Failed to map physical page to reserved iova\n",
+		       ep->name);
+		goto err;
+	}
+	ep->mapped_iova = true;
+
+	pr_debug("(%s): mapped page:0x%pa[p]+0x%lx to iova:0x%llx\n", ep->name,
+		 &ep->self_mem.phys_addr, ep->self_mem.size, ep->iova);
+
+	/* get peer's aperture offset. Used in mmaping tx mem.*/
+	ep->peer_mem.size = ep->self_mem.size;
+	ret = pci_client_get_peer_aper(ep->pci_client_h, offsetof,
+				       ep->peer_mem.size, &ep->peer_mem.aper);
+	if (ret) {
+		pr_err("Failed to get peer's endpoint pcie aperture\n");
+		goto err;
+	}
+
+	return ret;
+err:
+	free_memory(eps_ctx, ep);
+	return ret;
+}
+
+/*
+ * Set of per-endpoint char device file operations. Do not support:
+ * read() and write() on nvscic2c-pcie endpoint descriptors.
+ */
+static const struct file_operations endpoint_fops = {
+	.owner          = THIS_MODULE,
+	.open           = endpoint_fops_open,
+	.release        = endpoint_fops_release,
+	.mmap           = endpoint_fops_mmap,
+	.unlocked_ioctl = endpoint_fops_ioctl,
+	.poll           = endpoint_fops_poll,
+	.llseek         = noop_llseek,
+};
+
+/* Clean up the endpoint devices. */
+static int
+remove_endpoint_device(struct endpoint_drv_ctx_t *eps_ctx,
+		       struct endpoint_t *endpoint)
+{
+	int ret = 0;
+
+	if (!eps_ctx || !endpoint)
+		return ret;
+
+	wait_event_interruptible(endpoint->close_waitq, !(atomic_read(&endpoint->in_use)));
+
+	pci_client_unregister_for_link_event(endpoint->pci_client_h,
+					     endpoint->linkevent_id);
+	free_sync_primitive(eps_ctx, endpoint);
+	free_memory(eps_ctx, endpoint);
+	atomic_set(&endpoint->in_use, 0);
+	mutex_destroy(&endpoint->fops_lock);
+
+	if (endpoint->device) {
+		cdev_del(&endpoint->cdev);
+		device_del(endpoint->device);
+		endpoint->device = NULL;
+	}
+
+	return ret;
+}
+
+/* Create the nvscic2c-pcie endpoint devices for the user-space to:
+ * - Map the endpoints Self and Peer area.
+ * - send notifications to remote/peer.
+ * - receive notifications from peer.
+ */
+static int
+create_endpoint_device(struct endpoint_drv_ctx_t *eps_ctx,
+		       struct endpoint_t *endpoint)
+{
+	int ret = 0;
+	struct callback_ops ops = {0};
+	/* create the nvscic2c endpoint char device.*/
+	endpoint->dev = MKDEV(MAJOR(eps_ctx->char_dev), endpoint->minor);
+	cdev_init(&endpoint->cdev, &endpoint_fops);
+	endpoint->cdev.owner = THIS_MODULE;
+	ret = cdev_add(&endpoint->cdev, endpoint->dev, 1);
+	if (ret != 0) {
+		pr_err("(%s): cdev_add() failed\n", endpoint->name);
+		goto err;
+	}
+	/* parent is this hvd dev */
+	endpoint->device = device_create(eps_ctx->class, NULL,
+					 endpoint->dev, endpoint,
+					 endpoint->name);
+	if (IS_ERR(endpoint->device)) {
+		ret = PTR_ERR(endpoint->device);
+		pr_err("(%s): device_create() failed\n", endpoint->name);
+		goto err;
+	}
+	dev_set_drvdata(endpoint->device, endpoint);
+
+	/* initialise the endpoint internals.*/
+	mutex_init(&endpoint->fops_lock);
+	atomic_set(&endpoint->in_use, 0);
+	init_waitqueue_head(&endpoint->waitq);
+	endpoint->link_status_ack_frm_usr = false;
+	init_waitqueue_head(&endpoint->ack_waitq);
+	init_waitqueue_head(&endpoint->close_waitq);
+
+	/* allocate physical pages for the endpoint PCIe BAR (rx) area.*/
+	ret = allocate_memory(eps_ctx, endpoint);
+	if (ret) {
+		pr_err("(%s): Failed to allocate physical pages\n",
+		       endpoint->name);
+		goto err;
+	}
+
+	/* allocate  for notification.*/
+	ret = init_notif_var(eps_ctx, endpoint);
+	if (ret) {
+		pr_err("(%s): Failed to allocate syncpt shim for notifications\n",
+		       endpoint->name);
+		goto err;
+	}
+
+	/* Register for link events.*/
+	ops.callback = &(link_event_callback);
+	ops.ctx = (void *)(endpoint);
+	ret = pci_client_register_for_link_event(endpoint->pci_client_h, &ops,
+						 &endpoint->linkevent_id);
+	if (ret) {
+		pr_err("(%s): Failed to register for PCIe link events\n",
+		       endpoint->name);
+	}
+
+	/* all okay.*/
+	return ret;
+err:
+	remove_endpoint_device(eps_ctx, endpoint);
+	return ret;
+}
+
+/*
+ * Entry point for the endpoint(s) char device sub-module/abstraction.
+ *
+ * On successful return (0), devices would have been created and ready to
+ * accept ioctls from user-space application.
+ */
+int
+endpoints_setup(struct driver_ctx_t *drv_ctx, void **endpoints_h)
+{
+	u32 i = 0;
+	int ret = 0;
+	struct endpoint_t *endpoint = NULL;
+	struct endpoint_prop_t *ep_prop = NULL;
+	struct endpoint_drv_ctx_t *eps_ctx = NULL;
+	struct stream_ext_params *stream_ext_params = NULL;
+
+	/* this cannot be initialized again.*/
+	if (WARN_ON(!drv_ctx || !endpoints_h || *endpoints_h))
+		return -EINVAL;
+
+	if (WARN_ON(drv_ctx->drv_param.nr_endpoint == 0 ||
+		    drv_ctx->drv_param.nr_endpoint > MAX_ENDPOINTS))
+		return -EINVAL;
+
+	/* start by allocating the endpoint driver (global for all eps) ctx.*/
+	eps_ctx = kzalloc(sizeof(*eps_ctx), GFP_KERNEL);
+	if (WARN_ON(!eps_ctx))
+		return -ENOMEM;
+
+	eps_ctx->nr_endpoint = drv_ctx->drv_param.nr_endpoint;
+
+	/* allocate the whole chardev range */
+	ret = alloc_chrdev_region(&eps_ctx->char_dev, 0,
+				  eps_ctx->nr_endpoint, drv_ctx->drv_name);
+	if (ret < 0)
+		goto err;
+
+	eps_ctx->class = class_create(THIS_MODULE, drv_ctx->drv_name);
+	if (IS_ERR_OR_NULL(eps_ctx->class)) {
+		ret = PTR_ERR(eps_ctx->class);
+		goto err;
+	}
+
+	/* allocate char devices context for supported endpoints.*/
+	eps_ctx->endpoints = kzalloc((eps_ctx->nr_endpoint *
+				      sizeof(*eps_ctx->endpoints)),
+				     GFP_KERNEL);
+	if (WARN_ON(!eps_ctx->endpoints)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+	/* create char devices for each endpoint.*/
+	for (i = 0; i < eps_ctx->nr_endpoint; i++) {
+		endpoint = &eps_ctx->endpoints[i];
+		ep_prop = &drv_ctx->drv_param.endpoint_props[i];
+		stream_ext_params = &endpoint->stream_ext_params;
+
+		/* copy the parameters from nvscic2c-pcie driver ctx.*/
+		strcpy(endpoint->name, ep_prop->name);
+		endpoint->minor = ep_prop->id;
+		endpoint->nframes = ep_prop->nframes;
+		endpoint->frame_sz = ep_prop->frame_sz;
+		endpoint->pci_client_h = drv_ctx->pci_client_h;
+		/*set endpoint  msi-x irq  start from 1, comm-channel take 0 */
+		endpoint->msi_x_irq =  ENDPOINT_IRQ_INDEX + i;
+
+		stream_ext_params->local_node = &drv_ctx->drv_param.local_node;
+		stream_ext_params->peer_node = &drv_ctx->drv_param.peer_node;
+		stream_ext_params->pci_client_h = drv_ctx->pci_client_h;
+		stream_ext_params->comm_channel_h = drv_ctx->comm_channel_h;
+		stream_ext_params->vmap_h = drv_ctx->vmap_h;
+		stream_ext_params->edma_h = drv_ctx->edma_h;
+		stream_ext_params->ep_id = ep_prop->id;
+		stream_ext_params->ep_name = endpoint->name;
+		stream_ext_params->drv_mode = drv_ctx->drv_mode;
+
+		/* create nvscic2c-pcie endpoint device.*/
+		ret = create_endpoint_device(eps_ctx, endpoint);
+		if (ret)
+			goto err;
+	}
+
+	*endpoints_h = eps_ctx;
+	return ret;
+err:
+	endpoints_release((void **)&eps_ctx);
+	return ret;
+}
+
+/*refresh edma_h retured from edma_mould_init cb*/
+int  endpoints_refresh_edma_h(struct driver_ctx_t *drv_ctx)
+{
+	u32 i = 0;
+	int ret = 0;
+	struct endpoint_drv_ctx_t *eps_ctx = NULL;
+
+	/* this cannot be initialized again.*/
+	if (WARN_ON(!drv_ctx || !drv_ctx->endpoints_h))
+		return -EINVAL;
+
+	eps_ctx = (struct endpoint_drv_ctx_t *)drv_ctx->endpoints_h;
+	for (i = 0; i < eps_ctx->nr_endpoint; i++)
+		eps_ctx->endpoints[i].stream_ext_params.edma_h = drv_ctx->edma_h;
+
+	return ret;
+}
+
+/* exit point for nvscic2c-pcie endpoints char device sub-module/abstraction.*/
+int
+endpoints_release(void **endpoints_h)
+{
+	u32 i = 0;
+	int ret = 0;
+	struct endpoint_drv_ctx_t *eps_ctx =
+				 (struct endpoint_drv_ctx_t *)(*endpoints_h);
+	if (!eps_ctx)
+		return -EINVAL;
+
+	/* remove all the endpoints char devices.*/
+	if (eps_ctx->endpoints) {
+		for (i = 0; i < eps_ctx->nr_endpoint; i++) {
+			struct endpoint_t *endpoint =
+						 &eps_ctx->endpoints[i];
+			remove_endpoint_device(eps_ctx, endpoint);
+		}
+		kfree(eps_ctx->endpoints);
+		eps_ctx->endpoints = NULL;
+	}
+
+	if (eps_ctx->class) {
+		class_destroy(eps_ctx->class);
+		eps_ctx->class = NULL;
+	}
+
+	if (eps_ctx->char_dev) {
+		unregister_chrdev_region(eps_ctx->char_dev,
+					 eps_ctx->nr_endpoint);
+		eps_ctx->char_dev = 0;
+	}
+
+	kfree(eps_ctx);
+	*endpoints_h = NULL;
+
+	return ret;
+}
+
+int
+endpoints_core_deinit(void *endpoints_h)
+{
+	u32 i = 0;
+	int ret = 0;
+	struct endpoint_drv_ctx_t *eps_ctx =
+				 (struct endpoint_drv_ctx_t *)endpoints_h;
+	if (!eps_ctx)
+		return ret;
+
+	if (eps_ctx->endpoints) {
+		for (i = 0; i < eps_ctx->nr_endpoint; i++) {
+			struct endpoint_t *endpoint =
+						 &eps_ctx->endpoints[i];
+
+			mutex_lock(&endpoint->fops_lock);
+			stream_extension_edma_deinit(endpoint->stream_ext_h);
+			mutex_unlock(&endpoint->fops_lock);
+			(void)wait_event_interruptible_timeout(endpoint->ack_waitq,
+							       !endpoint->link_status_ack_frm_usr,
+							       msecs_to_jiffies
+							       (PCIE_STATUS_CHANGE_ACK_TIMEOUT));
+
+			pci_client_unregister_for_link_event(endpoint->pci_client_h,
+							     endpoint->linkevent_id);
+		}
+	}
+
+	return ret;
+}
diff --git a/drivers/misc/nvscic2c-pcie/endpoint.h b/drivers/misc/nvscic2c-pcie/endpoint.h
new file mode 100644
index 000000000000..379e809a7550
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/endpoint.h
@@ -0,0 +1,46 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __ENDPOINT_H__
+#define __ENDPOINT_H__
+
+#include "common.h"
+
+/* forward declaration. */
+struct driver_ctx_t;
+
+/*
+ * Entry point for the endpoint(s) char device sub-module/abstraction.
+ *
+ * On successful return (0), devices would have been created and ready to
+ * accept ioctls from user-space application.
+ */
+int
+endpoints_setup(struct driver_ctx_t *drv_ctx, void **endpoints_h);
+
+/*refresh edma handle retured from edma_module_init cb*/
+int
+endpoints_refresh_edma_h(struct driver_ctx_t *drv_ctx);
+
+/* exit point for nvscic2c-pcie endpoints char device sub-module/abstraction.*/
+int
+endpoints_release(void **endpoints_h);
+
+/*
+ * Wait for ack from user space process for PCIe link status change.
+ * Deinit edma handle with stream-extension.
+ */
+int
+endpoints_core_deinit(void *endpoints_h);
+#endif //__ENDPOINT_H__
diff --git a/drivers/misc/nvscic2c-pcie/iova-mngr.c b/drivers/misc/nvscic2c-pcie/iova-mngr.c
new file mode 100644
index 000000000000..53b97641505a
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/iova-mngr.c
@@ -0,0 +1,393 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvscic2c-pcie: iova-mgr: " fmt
+
+#include <linux/errno.h>
+#include <linux/mutex.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+
+#include "common.h"
+#include "iova-mngr.h"
+
+/*
+ * INTERNAL DataStructure that define a single IOVA block/chunk
+ * in a pool of IOVA region managed by IOVA manager.
+ *
+ * IOVA manager chunks entire IOVA space into these blocks/chunks.
+ *
+ * The chunk/block is also a node for linking to previous and next
+ * nodes of a circular doubly linked list - free or reserved.
+ */
+struct block_t {
+	/* for management of this chunk in either avail or reserve lists.*/
+	struct list_head node;
+
+	/* block address.*/
+	u64 address;
+
+	/* block size.*/
+	size_t size;
+};
+
+/*
+ * INTERNAL datastructure for IOVA space manager.
+ *
+ * IOVA space manager would fragment and manage the IOVA region
+ * using two circular doubly linked lists - reserved list and
+ * free list. These lists contain blocks/chunks reserved
+ * or free for use by clients (callers) from the overall
+ * IOVA region the IOVA manager was configured with.
+ */
+struct mngr_ctx_t {
+	/*
+	 * For debug purpose only, all other usage prohibited.
+	 * In the event there are multiple iova_managers within
+	 * an LKM instance, name helps in identification.
+	 */
+	char name[NAME_MAX];
+
+	/*
+	 * Circular doubly linked list of blocks indicating
+	 * available/free IOVA space(s). When IOVA manager is
+	 * initialised all of the IOVA space is marked as available
+	 * to begin with.
+	 */
+	struct list_head *free_list;
+
+	/*
+	 * Book-keeping of the user IOVA blocks in a circular double
+	 * linked list.
+	 */
+	struct list_head *reserved_list;
+
+	/* Ensuring reserve, free and the list operations are serialized.*/
+	struct mutex lock;
+
+	/* base address memory manager is configured with. */
+	u64 base_address;
+};
+
+/*
+ * Reserves a block from the free IOVA regions. Once reserved, the block
+ * is marked reserved and appended in the reserved list (no ordering
+ * required and trying to do so shall increase the time)
+ */
+int
+iova_mngr_block_reserve(void *mngr_handle, size_t size,
+			u64 *address, size_t *offset,
+			void **block_handle)
+{
+	struct mngr_ctx_t *ctx = (struct mngr_ctx_t *)(mngr_handle);
+	struct block_t *reserve = NULL, *curr = NULL, *best = NULL;
+	int ret = 0;
+
+	if (WARN_ON(!block_handle))
+		return -EINVAL;
+	if (WARN_ON(!ctx || *block_handle || !size))
+		return -EINVAL;
+
+	mutex_lock(&ctx->lock);
+	/* if there are no free blocks to reserve. */
+	if (list_empty(ctx->free_list)) {
+		ret = -ENOMEM;
+		pr_err("(%s): No memory available to reserve block of size:(%lu)\n",
+		       ctx->name, size);
+		goto err;
+	}
+
+	/* find the best of all free bocks to reserve.*/
+	list_for_each_entry(curr, ctx->free_list, node) {
+		if (curr->size >= size) {
+			if (!best)
+				best = curr;
+			else if (curr->size < best->size)
+				best = curr;
+		}
+	}
+
+	/* if there isn't any free block of requested size. */
+	if (!best) {
+		ret = -ENOMEM;
+		pr_err("(%s): No enough mem available to reserve block sz:(%lu)\n",
+		       ctx->name, size);
+		goto err;
+	} else {
+		struct block_t *found = NULL;
+
+		/* perfect fit.*/
+		if (best->size == size) {
+			list_del(&best->node);
+			list_add_tail(&best->node, ctx->reserved_list);
+			found = best;
+		} else {
+			/* chunk out a new block, adjust the free block.*/
+			reserve = kzalloc(sizeof(*reserve), GFP_KERNEL);
+			if (WARN_ON(!reserve)) {
+				ret = -ENOMEM;
+				goto err;
+			}
+			reserve->address = best->address;
+			reserve->size = size;
+			best->address += size;
+			best->size -= size;
+			list_add_tail(&reserve->node, ctx->reserved_list);
+			found = reserve;
+		}
+		*block_handle = (void *)(found);
+
+		if (address)
+			*address = found->address;
+		if (offset)
+			*offset = (found->address - ctx->base_address);
+	}
+err:
+	mutex_unlock(&ctx->lock);
+	return ret;
+}
+
+/*
+ * Release an already reserved IOVA block/chunk by the caller back to
+ * free list.
+ */
+int
+iova_mngr_block_release(void *mngr_handle, void **block_handle)
+{
+	struct mngr_ctx_t *ctx = (struct mngr_ctx_t *)(mngr_handle);
+	struct block_t *release = (struct block_t *)(*block_handle);
+	struct block_t *curr = NULL, *prev = NULL;
+	bool done = false;
+	int ret = 0;
+
+	if (!ctx || !release)
+		return -EINVAL;
+
+	mutex_lock(&ctx->lock);
+
+	list_for_each_entry(curr, ctx->free_list, node) {
+		if (release->address < curr->address) {
+			/* if the immediate next node is available for merge.*/
+			if (curr->address == release->address + release->size) {
+				curr->address = release->address;
+				curr->size += release->size;
+				list_del(&release->node);
+				kfree(release);
+				/*
+				 * if the immediate previous node is also
+				 * available for merge.
+				 */
+				if ((prev) &&
+				    ((prev->address + prev->size)
+				      == curr->address)) {
+					prev->size += curr->size;
+					list_del(&curr->node);
+					kfree(curr);
+				}
+			} else if ((prev) &&
+				   ((prev->address + prev->size)
+				    == release->address)) {
+				/*
+				 * if only the immediate prev node is available
+				 */
+				prev->size += release->size;
+				list_del(&release->node);
+				kfree(release);
+			} else {
+				/*
+				 * cannot be merged with either the immediate
+				 * prev or the immediate next node. Add it in the
+				 * free list before the current node.
+				 */
+				list_del(&release->node);
+				list_add_tail(&release->node, &curr->node);
+			}
+			done = true;
+			break;
+		}
+		prev = curr;
+	}
+
+	/*
+	 * Even if after traversing each entry in list, we could not
+	 * add the block to be released back in free list, because:
+	 */
+	if (!done) {
+		if (!list_empty(ctx->free_list)) {
+			/*
+			 * The block to be freed has the highest order
+			 * address of all the existing blocks in free list.
+			 */
+			struct block_t *last =
+			list_last_entry(ctx->free_list, struct block_t, node);
+			if ((last->address + last->size) == release->address) {
+				/* can be merged with last node of list.*/
+				last->size += release->size;
+				list_del(&release->node);
+				kfree(release);
+			} else {
+				/* cannot be merged, add as the last node.*/
+				list_del(&release->node);
+				list_add_tail(&release->node, ctx->free_list);
+			}
+		} else {
+			/* free list was empty.*/
+			list_del(&release->node);
+			list_add_tail(&release->node, ctx->free_list);
+		}
+	}
+	*block_handle = NULL;
+
+	mutex_unlock(&ctx->lock);
+	return ret;
+}
+
+/*
+ * iova_mngr_print
+ *
+ * DEBUG only.
+ *
+ * Helper function to print all the reserved and free blocks with
+ * their names, size and start address.
+ */
+void
+iova_mngr_print(void *mngr_handle)
+{
+	struct mngr_ctx_t *ctx = (struct mngr_ctx_t *)(mngr_handle);
+	struct block_t *block = NULL;
+
+	if (ctx) {
+		mutex_lock(&ctx->lock);
+		pr_debug("(%s): Reserved\n", ctx->name);
+		list_for_each_entry(block, ctx->reserved_list, node) {
+			pr_debug("\t\t (%s): address = 0x%pa[p], size = 0x%lx\n",
+				 ctx->name, &block->address, block->size);
+		}
+		pr_debug("(%s): Free\n", ctx->name);
+		list_for_each_entry(block, ctx->free_list, node) {
+			pr_debug("\t\t (%s): address = 0x%pa[p], size = 0x%lx\n",
+				 ctx->name, &block->address, block->size);
+		}
+		mutex_unlock(&ctx->lock);
+	}
+}
+
+/*
+ * Initialises the IOVA space manager with the base address + size
+ * provided. IOVA manager would use two lists for book-keeping reserved
+ * memory blocks and free memory blocks.
+ *
+ * When initialised all of the IOVA region: base_address + size is free.
+ */
+int
+iova_mngr_init(char *name, u64 base_address, size_t size, void **mngr_handle)
+{
+	int ret = 0;
+	struct block_t *block = NULL;
+	struct mngr_ctx_t *ctx = NULL;
+
+	if (WARN_ON(!base_address || !size ||
+		    !mngr_handle || *mngr_handle || !name))
+		return -EINVAL;
+
+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+	if (WARN_ON(!ctx)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	ctx->free_list = kzalloc(sizeof(*ctx->free_list), GFP_KERNEL);
+	if (WARN_ON(!ctx->free_list)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	ctx->reserved_list = kzalloc(sizeof(*ctx->reserved_list), GFP_KERNEL);
+	if (WARN_ON(!ctx->reserved_list)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	if (strlen(name) > (NAME_MAX - 1)) {
+		ret = -EINVAL;
+		pr_err("name: (%s) long, max char:(%u)\n", name, (NAME_MAX - 1));
+		goto err;
+	}
+	strcpy(ctx->name, name);
+	INIT_LIST_HEAD(ctx->reserved_list);
+	INIT_LIST_HEAD(ctx->free_list);
+	mutex_init(&ctx->lock);
+	ctx->base_address = base_address;
+
+	/* add the base_addrss+size as one whole free block.*/
+	block = kzalloc(sizeof(*block), GFP_KERNEL);
+	if (WARN_ON(!block)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+	block->address = base_address;
+	block->size = size;
+	list_add(&block->node, ctx->free_list);
+
+	*mngr_handle = ctx;
+	return ret;
+err:
+	iova_mngr_deinit((void **)(&ctx));
+	return ret;
+}
+
+/*
+ * iova_mngr_deinit
+ *
+ * deinitialize the IOVA space manager. Any blocks unreturned from the client
+ * (caller) shall become dangling.
+ */
+void
+iova_mngr_deinit(void **mngr_handle)
+{
+	struct block_t *block = NULL;
+	struct list_head *curr = NULL, *next = NULL;
+	struct mngr_ctx_t *ctx = NULL;
+
+	if (WARN_ON(!mngr_handle))
+		return;
+	ctx = (struct mngr_ctx_t *)(*mngr_handle);
+
+	if (ctx) {
+		/* debug only to ensure, lists do not have dangling data left.*/
+		iova_mngr_print(*mngr_handle);
+
+		/* ideally, all blocks should have returned before this.*/
+		if (!list_empty(ctx->reserved_list)) {
+			list_for_each_safe(curr, next, ctx->reserved_list) {
+				block = list_entry(curr, struct block_t, node);
+				iova_mngr_block_release(*mngr_handle,
+							(void **)(&block));
+			}
+		}
+
+		/* ideally, just one whole free block should remain as free.*/
+		list_for_each_safe(curr, next, ctx->free_list) {
+			block = list_entry(curr, struct block_t, node);
+			list_del(&block->node);
+			kfree(block);
+		}
+
+		mutex_destroy(&ctx->lock);
+		kfree(ctx->reserved_list);
+		kfree(ctx->free_list);
+		kfree(ctx);
+		*mngr_handle = NULL;
+	}
+}
diff --git a/drivers/misc/nvscic2c-pcie/iova-mngr.h b/drivers/misc/nvscic2c-pcie/iova-mngr.h
new file mode 100644
index 000000000000..a6fbd581afe4
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/iova-mngr.h
@@ -0,0 +1,72 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __IOVA_MNGR_H__
+#define __IOVA_MNGR_H__
+
+#include <linux/types.h>
+
+/*
+ * iova_mngr_block_reserve
+ *
+ * Reserves a block from the free IOVA regions. Once reserved, the block
+ * is marked reserved and appended in the reserved list. Use
+ * iova_mngr_block_get_address to fetch the address of the block reserved.
+ */
+int
+iova_mngr_block_reserve(void *mngr_handle, size_t size,
+			u64 *address, size_t *offset,
+			void **block_handle);
+
+/*
+ * iova_mngr_block_release
+ *
+ * Release an already reserved IOVA block/chunk by the caller back to
+ * free list.
+ */
+int
+iova_mngr_block_release(void *mngr_handle, void **block_handle);
+
+/*
+ * iova_mngr_print
+ *
+ * DEBUG only.
+ *
+ * Helper function to print all the reserved and free blocks with
+ * their names, size and start address.
+ */
+void iova_mngr_print(void *handle);
+
+/*
+ * iova_mngr_init
+ *
+ * Initialises the IOVA space manager with the base address + size
+ * provided. IOVA manager would use two lists for book-keeping reserved
+ * memory blocks and free memory blocks.
+ *
+ * When initialised all of the IOVA region: base_address + size is free.
+ */
+int
+iova_mngr_init(char *name, u64 base_address, size_t size, void **mngr_handle);
+
+/*
+ * iova_mngr_deinit
+ *
+ * deinitialize the IOVA space manager. Any blocks unreturned from the client
+ * (caller) shall become dangling.
+ */
+void
+iova_mngr_deinit(void **handle);
+
+#endif //__IOVA_MNGR_H__
diff --git a/drivers/misc/nvscic2c-pcie/module.c b/drivers/misc/nvscic2c-pcie/module.c
new file mode 100644
index 000000000000..87f076022f0f
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/module.c
@@ -0,0 +1,528 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvscic2c-pcie: epc: " fmt
+
+#include <linux/dma-iommu.h>
+#include <linux/init.h>
+#include <linux/kernel.h>
+#include <linux/mm.h>
+#include <linux/module.h>
+#include <linux/of_device.h>
+#include <linux/pci.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/iommu.h>
+#include <linux/iova.h>
+#include <linux/version.h>
+
+#include "tegra-pcie-edma.h"
+#include "comm-channel.h"
+#include "common.h"
+#include "endpoint.h"
+#include "module.h"
+#include "pci-client.h"
+#include "vmap.h"
+#include "config.h"
+
+static const struct pci_device_id nvscic2c_pcie_epc_tbl[] = {
+	{ PCI_DEVICE(0x10DE, PCI_DEVICE_ID_NVIDIA_C2C_2) },
+	{},
+};
+
+enum iommu_dma_cookie_type {
+	IOMMU_DMA_IOVA_COOKIE,
+	IOMMU_DMA_MSI_COOKIE,
+};
+
+struct iommu_dma_cookie {
+	enum iommu_dma_cookie_type	type;
+	union {
+		/* Full allocator for IOMMU_DMA_IOVA_COOKIE */
+		struct iova_domain	iovad;
+		/* Trivial linear page allocator for IOMMU_DMA_MSI_COOKIE */
+		dma_addr_t		msi_iova;
+	};
+	struct list_head		msi_page_list;
+
+	/* Domain for flush queue callback; NULL if flush queue not in use */
+	struct iommu_domain		*fq_domain;
+};
+
+static dma_addr_t iommu_dma_alloc_iova_memory(struct device *dev, size_t size,
+					      u64 dma_limit)
+{
+	struct iommu_domain *domain = NULL;
+	struct iommu_dma_cookie *cookie = NULL;
+	struct iova_domain *iovad = NULL;
+	int ret = 0;
+	unsigned long shift, iova_len, iova = 0;
+	unsigned long order;
+
+	domain = iommu_get_domain_for_dev(dev);
+	if (!domain) {
+		pr_err("Got empty iommu domain for pcie\n");
+		return iova;
+	}
+	if (!domain->iova_cookie) {
+		ret = iommu_get_dma_cookie(domain);
+		if (ret) {
+			pr_err("iommu failed allocating dma cookie ret = %d\n", ret);
+			return iova;
+		}
+	}
+	cookie = domain->iova_cookie;
+	iovad = &cookie->iovad;
+	ret = iova_cache_get();
+	if (ret) {
+		pr_err("failed to create iova cache\n");
+		return iova;
+	}
+	order = __ffs(domain->pgsize_bitmap);
+	init_iova_domain(iovad, 1UL << order, 0);
+	if (cookie->type == IOMMU_DMA_MSI_COOKIE) {
+		cookie->msi_iova += size;
+		return cookie->msi_iova - size;
+	}
+	shift = iova_shift(iovad);
+	iova_len = size >> shift;
+	/*
+	 * Freeing non-power-of-two-sized allocations back into the IOVA caches
+	 * will come back to bite us badly, so we have to waste a bit of space
+	 * rounding up anything cacheable to make sure that can't happen. The
+	 * order of the unadjusted size will still match upon freeing.
+	 */
+	if (iova_len < (1 << (IOVA_RANGE_CACHE_MAX_SIZE - 1)))
+		iova_len = roundup_pow_of_two(iova_len);
+
+	if (dev->bus_dma_mask)
+		dma_limit &= dev->bus_dma_mask;
+
+	if (domain->geometry.force_aperture)
+		dma_limit = min(dma_limit, domain->geometry.aperture_end);
+
+	if (!iovad) {
+		pr_err("empty iovad\n");
+		return (dma_addr_t)NULL;
+	}
+	/* Try to get PCI devices a SAC address */
+	if (dma_limit > DMA_BIT_MASK(32) && dev_is_pci(dev)) {
+		iova = alloc_iova_fast(iovad, iova_len,
+				       DMA_BIT_MASK(32) >> shift, false);
+	}
+	if (!iova)
+		iova = alloc_iova_fast(iovad, iova_len, dma_limit >> shift, true);
+
+	return (dma_addr_t)iova << shift;
+}
+
+static void iommu_dma_free_iova_memory(struct device *dev, dma_addr_t iova,
+				       size_t size)
+{
+	struct iommu_domain *domain = iommu_get_domain_for_dev(dev);
+	struct iommu_dma_cookie *cookie = domain->iova_cookie;
+	struct iova_domain *iovad = &cookie->iovad;
+
+	/* The MSI case is only ever cleaning up its most recent allocation */
+	if (cookie->type == IOMMU_DMA_MSI_COOKIE)
+		cookie->msi_iova -= size;
+	else if (cookie->fq_domain)	/* non-strict mode */
+		queue_iova(iovad, iova_pfn(iovad, iova),
+			   size >> iova_shift(iovad), 0);
+	else
+		free_iova_fast(iovad, iova_pfn(iovad, iova),
+			       size >> iova_shift(iovad));
+}
+
+/* wrapper over tegra-pcie-edma init api. */
+static int
+edma_module_init(struct driver_ctx_t *drv_ctx, dma_addr_t desc_iova)
+{
+	int ret = 0;
+	struct pci_dev *pdev;
+	struct tegra_pcie_edma_init_info info = {0};
+	void *edma_remote;
+	u32 val;
+	u16 val_16;
+	phys_addr_t desc_phy_base;
+
+	if (WARN_ON(!drv_ctx))
+		return -EINVAL;
+
+	pdev = drv_ctx->dev_ctx;
+	memset(&info, 0x0, sizeof(info));
+	desc_phy_base = drv_ctx->peer_mem.aper + SZ_4K;
+	edma_remote = devm_kzalloc(&pdev->dev, sizeof(*info.edma_remote), GFP_KERNEL);
+	if (!edma_remote)
+		return -ENOMEM;
+
+	/* x86 only use rx[0] channel  */
+	info.edma_remote = edma_remote;
+	info.rx[0].ch_type = EDMA_CHAN_XFER_ASYNC;
+	info.rx[0].num_descriptors = MAX_EDMA_DESC;
+	info.rx[0].desc_phy_base = desc_phy_base;
+	info.rx[0].desc_iova = desc_iova;
+	info.rx[1].ch_type = EDMA_CHAN_XFER_ASYNC;
+	/* skip rx [1] */
+	info.rx[1].num_descriptors = 0;
+
+	/* use  msi irq  */
+	ret = pci_read_config_word(pdev, pdev->msi_cap + PCI_MSI_FLAGS, &val_16);
+	if (ret) {
+		pr_err("pci_read_config_word failed\n");
+		return -ENODEV;
+	}
+	if (val_16 & PCI_MSI_FLAGS_64BIT) {
+		pci_read_config_dword(pdev, pdev->msi_cap + PCI_MSI_ADDRESS_HI, &val);
+		info.edma_remote->msi_addr = val;
+		pci_read_config_word(pdev, pdev->msi_cap + PCI_MSI_DATA_64, &val_16);
+		info.edma_remote->msi_data = val_16;
+	} else {
+		pci_read_config_word(pdev, pdev->msi_cap + PCI_MSI_DATA_32, &val_16);
+		info.edma_remote->msi_data = val_16;
+	}
+	info.edma_remote->msi_data = val_16 + EDMA_REMOTE_IRQ;
+	pci_read_config_dword(pdev, pdev->msi_cap + PCI_MSI_ADDRESS_LO, &val);
+	info.edma_remote->msi_addr = (info.edma_remote->msi_addr << 32) | val;
+	info.edma_remote->msi_irq = pci_irq_vector(pdev, EDMA_REMOTE_IRQ);
+	info.edma_remote->dma_phy_base = pci_resource_start(pdev, 4);
+	info.edma_remote->dma_size = pci_resource_len(pdev, 4);
+	info.edma_remote->dev = &pdev->dev;
+	drv_ctx->edma_h = tegra_pcie_edma_initialize(&info);
+	if (!drv_ctx->edma_h) {
+		ret = -ENODEV;
+		pr_err("tegra_pcie_edma_initialize failed\n");
+		return ret;
+	}
+	ret = endpoints_refresh_edma_h(drv_ctx);
+
+	return ret;
+}
+
+/* should not have any ongoing eDMA transfers.*/
+static void
+edma_module_deinit(struct driver_ctx_t *drv_ctx)
+{
+	if (!drv_ctx || !drv_ctx->edma_h)
+		return;
+	/* tegra  */
+	tegra_pcie_edma_deinit(drv_ctx->edma_h);
+	drv_ctx->edma_h = NULL;
+}
+
+static void
+free_inbound_area(struct pci_dev *pdev, struct dma_buff_t *self_mem)
+{
+	if (!pdev || !self_mem || !self_mem->dma_handle)
+		return;
+	iommu_dma_free_iova_memory(&pdev->dev, self_mem->dma_handle, self_mem->size);
+	self_mem->dma_handle = 0x0;
+}
+
+/*
+ * Allocate area visible to PCIe EP/DRV_MODE_EPF. To have symmetry between the
+ * two modules, even PCIe RP/DRV_MODE_EPC allocates an empty area for all writes
+ * from PCIe EP/DRV_MODE_EPF to land into. Also, all CPU access from PCIe EP/
+ * DRV_MODE_EPF need be for one continguous region.
+ */
+static int
+allocate_inbound_area(struct pci_dev *pdev, size_t win_size,
+		      struct dma_buff_t *self_mem)
+{
+	int ret = 0;
+
+	/* allocate same area size as that of exported by PCIe EP.*/
+	self_mem->size = win_size;
+	self_mem->dma_handle = iommu_dma_alloc_iova_memory(&pdev->dev, self_mem->size,
+							(&pdev->dev)->coherent_dma_mask);
+
+	if (!self_mem->dma_handle) {
+		ret = -ENOMEM;
+		pr_err("iommu_dma_alloc_iova() failed for size:(0x%lx)\n",
+		       self_mem->size);
+	}
+
+	return ret;
+}
+
+static void
+free_outbound_area(struct pci_dev *pdev, struct pci_aper_t *peer_mem)
+{
+	if (!pdev || !peer_mem)
+		return;
+
+	peer_mem->aper = 0x0;
+	peer_mem->size = 0;
+}
+
+/* Assign outbound pcie aperture for CPU/eDMA access towards PCIe EP. */
+static int
+assign_outbound_area(struct pci_dev *pdev, size_t win_size,
+		     struct pci_aper_t *peer_mem)
+{
+	int ret = 0;
+
+	peer_mem->size = win_size;
+	peer_mem->aper = pci_resource_start(pdev, 0);
+
+	return ret;
+}
+
+/* Handle link message from @DRV_MODE_EPC. */
+static void
+link_msg_cb(void *data, void *ctx)
+{
+	struct comm_msg *msg = (struct comm_msg *)data;
+	struct driver_ctx_t *drv_ctx = (struct driver_ctx_t *)ctx;
+
+	if (WARN_ON(!msg || !drv_ctx))
+		return;
+
+	/* inidicate link status to application.*/
+	pci_client_change_link_status(drv_ctx->pci_client_h,
+				      msg->u.link.status);
+}
+
+/* Handle edma rx desc iova return from @DRV_MODE_EPF. */
+static void
+edma_rx_desc_iova_cb(void *data, void *ctx)
+{
+	int ret;
+	struct comm_msg *msg = (struct comm_msg *)data;
+	struct driver_ctx_t *drv_ctx = (struct driver_ctx_t *)ctx;
+	dma_addr_t desc_iova = 0;
+
+	if (WARN_ON(!msg || !drv_ctx))
+		return;
+	/* edma init.*/
+	desc_iova = msg->u.edma_rx_desc_iova.iova;
+
+	ret = edma_module_init(drv_ctx, desc_iova);
+	if (ret)
+		pr_err("edma module init failed\n");
+}
+
+static void
+nvscic2c_pcie_epc_remove(struct pci_dev *pdev)
+{
+	struct driver_ctx_t *drv_ctx = NULL;
+
+	if (!pdev)
+		return;
+
+	drv_ctx = pci_get_drvdata(pdev);
+	if (!drv_ctx)
+		return;
+
+	pci_client_change_link_status(drv_ctx->pci_client_h,
+				      NVSCIC2C_PCIE_LINK_DOWN);
+	endpoints_core_deinit(drv_ctx->endpoints_h);
+	edma_module_deinit(drv_ctx);
+	endpoints_release(&drv_ctx->endpoints_h);
+	vmap_deinit(&drv_ctx->vmap_h);
+	comm_channel_deinit(&drv_ctx->comm_channel_h);
+	pci_client_deinit(&drv_ctx->pci_client_h);
+	free_outbound_area(pdev, &drv_ctx->peer_mem);
+	free_inbound_area(pdev, &drv_ctx->self_mem);
+	pci_release_region(pdev, 0);
+	pci_clear_master(pdev);
+	pci_free_irq_vectors(pdev);
+	pci_disable_device(pdev);
+
+	kfree_const(drv_ctx->drv_name);
+	kfree(drv_ctx);
+}
+
+static int
+nvscic2c_pcie_epc_probe(struct pci_dev *pdev,
+			const struct pci_device_id *id)
+{
+	int ret = 0;
+	char *name = NULL;
+	size_t win_size = 0;
+	struct comm_msg msg = {0};
+	struct callback_ops cb_ops = {0};
+	struct driver_ctx_t *drv_ctx = NULL;
+	struct pci_client_params params = {0};
+
+	/* allocate module context.*/
+	drv_ctx = kzalloc(sizeof(*drv_ctx), GFP_KERNEL);
+	if (WARN_ON(!drv_ctx))
+		return -ENOMEM;
+
+	name = kasprintf(GFP_KERNEL, "%s-%x", DRIVER_NAME_EPC, id->device);
+	if (WARN_ON(!name)) {
+		kfree(drv_ctx);
+		return -ENOMEM;
+	}
+
+	drv_ctx->drv_mode = DRV_MODE_EPC;
+	drv_ctx->drv_name = name;
+	drv_ctx->dev_ctx = (void *)pdev;
+	pci_set_drvdata(pdev, drv_ctx);
+
+	/* check for the device tree node against this Id, must be only one.*/
+	ret = cfg_parse(id->device, drv_ctx->drv_mode, &drv_ctx->drv_param);
+	if (ret)
+		goto err_cfg_parse;
+
+	ret = pcim_enable_device(pdev);
+	if (ret) {
+		pr_err("Failed to enable pci device\n");
+		goto err_enable_device;
+	}
+
+	pci_set_master(pdev);
+	ret = pci_request_region(pdev, 0, MODULE_NAME);
+	if (ret)
+		goto err_request_region;
+
+	ret = dma_set_mask_and_coherent(&pdev->dev, DMA_BIT_MASK(64));
+	if (ret)
+		goto err_request_region;
+
+	win_size = pci_resource_len(pdev, 0);
+	ret = pci_alloc_irq_vectors(pdev, MAX_NVSCIC2C_IRQ, MAX_NVSCIC2C_IRQ, PCI_IRQ_MSI);
+	if (ret < 0) {
+		pr_err("Failed to allocate msix irq vectors\n");
+		goto err_alloc_irq;
+	}
+
+	ret = allocate_inbound_area(pdev, win_size, &drv_ctx->self_mem);
+	if (ret) {
+		pr_err("Failed to allocate_inbound_area ...\n");
+		goto err_alloc_inbound;
+	}
+	ret = assign_outbound_area(pdev, win_size, &drv_ctx->peer_mem);
+	if (ret) {
+		pr_err("Failed to assign_outbound_area ...\n");
+		goto err_assign_outbound;
+	}
+
+	params.dev = &pdev->dev;
+	params.self_mem = &drv_ctx->self_mem;
+	params.peer_mem = &drv_ctx->peer_mem;
+	ret = pci_client_init(&params, &drv_ctx->pci_client_h);
+	if (ret) {
+		pr_err("pci_client_init() failed\n");
+		goto err_pci_client;
+	}
+	ret = comm_channel_init(drv_ctx, &drv_ctx->comm_channel_h);
+	if (ret) {
+		pr_err("Failed to initialize comm-channel\n");
+		goto err_comm_init;
+	}
+	ret = vmap_init(drv_ctx, &drv_ctx->vmap_h);
+	if (ret) {
+		pr_err("Failed to initialize vmap\n");
+		goto err_vmap_init;
+	}
+
+	ret = endpoints_setup(drv_ctx, &drv_ctx->endpoints_h);
+	if (ret) {
+		pr_err("Failed to initialize endpoints\n");
+		goto err_endpoints_init;
+	}
+
+	/* register for edma rx desc iova  message from @DRV_MODE_EPF (PCIe EP).*/
+
+	cb_ops.callback = edma_rx_desc_iova_cb;
+	cb_ops.ctx = (void *)drv_ctx;
+	ret = comm_channel_register_msg_cb(drv_ctx->comm_channel_h,
+					   COMM_MSG_TYPE_EDMA_RX_DESC_IOVA_RETURN, &cb_ops);
+	if (ret) {
+		pr_err("Failed to register for link message from EP\n");
+		goto err_register_msg;
+	}
+
+	/* register for link status message from @DRV_MODE_EPF (PCIe EP).*/
+
+	cb_ops.callback = link_msg_cb;
+	cb_ops.ctx = (void *)drv_ctx;
+	ret = comm_channel_register_msg_cb(drv_ctx->comm_channel_h,
+					   COMM_MSG_TYPE_LINK, &cb_ops);
+	if (ret) {
+		pr_err("Failed to register for link message from EP\n");
+		goto err_register_msg;
+	}
+	/*
+	 * share iova with @DRV_MODE_EPF for it's outbound translation.
+	 * This must be send only after comm-channel, endpoint memory backing
+	 * is created and mapped to self_mem. @DRV_MODE_EPF on seeing this
+	 * message shall send link-up message over comm-channel and possibly
+	 * applications can also start endpoint negotiation, therefore.
+	 */
+	msg.type = COMM_MSG_TYPE_BOOTSTRAP;
+	msg.u.bootstrap.iova = drv_ctx->self_mem.dma_handle;
+	msg.u.bootstrap.peer_cpu = NVCPU_X86_64;
+
+	pr_debug("Sending bootstap msg = %llu\n", msg.u.bootstrap.iova);
+	ret = comm_channel_bootstrap_msg_send(drv_ctx->comm_channel_h, &msg);
+	if (ret) {
+		pr_err("Failed to send comm bootstrap message\n");
+		goto err_msg_send;
+	}
+	return ret;
+
+err_msg_send:
+err_register_msg:
+	endpoints_release(&drv_ctx->endpoints_h);
+
+err_endpoints_init:
+	edma_module_deinit(drv_ctx);
+
+err_vmap_init:
+	comm_channel_deinit(&drv_ctx->comm_channel_h);
+
+err_comm_init:
+	pci_client_deinit(&drv_ctx->pci_client_h);
+
+err_pci_client:
+	free_outbound_area(pdev, &drv_ctx->peer_mem);
+
+err_assign_outbound:
+	free_inbound_area(pdev, &drv_ctx->self_mem);
+
+err_alloc_inbound:
+	pci_release_region(pdev, 0);
+
+err_request_region:
+err_alloc_irq:
+	pci_clear_master(pdev);
+	pci_disable_device(pdev);
+
+err_enable_device:
+err_cfg_parse:
+	pci_set_drvdata(pdev, NULL);
+	kfree_const(drv_ctx->drv_name);
+	kfree(drv_ctx);
+	return ret;
+}
+
+MODULE_DEVICE_TABLE(pci, nvscic2c_pcie_epc_tbl);
+static struct pci_driver nvscic2c_pcie_epc_driver = {
+	.name		= DRIVER_NAME_EPC,
+	.id_table	= nvscic2c_pcie_epc_tbl,
+	.probe		= nvscic2c_pcie_epc_probe,
+	.remove		= nvscic2c_pcie_epc_remove,
+};
+
+module_pci_driver(nvscic2c_pcie_epc_driver);
+
+#define DRIVER_LICENSE		"GPL"
+#define DRIVER_DESCRIPTION	"NVIDIA Chip-to-Chip transfer module for X86 PCIeRP"
+#define DRIVER_AUTHOR		"Nvidia Corporation"
+MODULE_DESCRIPTION(DRIVER_DESCRIPTION);
+MODULE_LICENSE(DRIVER_LICENSE);
+MODULE_AUTHOR(DRIVER_AUTHOR);
diff --git a/drivers/misc/nvscic2c-pcie/module.h b/drivers/misc/nvscic2c-pcie/module.h
new file mode 100644
index 000000000000..088f5c65942d
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/module.h
@@ -0,0 +1,123 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+/*
+ * Internal to nvscic2c-pcie module. This file is not supposed to be included
+ * by any other external modules.
+ */
+#ifndef __MODULE_H__
+#define __MODULE_H__
+
+#include <linux/device.h>
+#include <linux/types.h>
+#include <linux/pci-epc.h>
+#include <linux/pci-epf.h>
+#include <linux/iommu.h>
+
+#include "common.h"
+
+/* forward declaration.*/
+struct device_node;
+struct platform_device;
+
+/*
+ * Parameters for the nvscic2c-pcie module and it's endpoints.
+ *
+ * These are read-only for the rest of the nvscic2c-pcie module.
+ */
+struct driver_param_t {
+	/* This is (nvscic2c-pcie) device tree node as found in device tree.*/
+	struct device_node *of_node;
+
+	/* driver mode as parsed from compatible string in device-tree.*/
+	enum drv_mode_t drv_mode;
+
+	/* pci endpoint device id.*/
+	u32 pci_dev_id;
+
+	/* bar window size. - applicable only for epf.*/
+	u32 bar_win_size;
+
+	/* node information, Board+SoC Id.*/
+	struct node_info_t local_node;
+	struct node_info_t peer_node;
+
+	/*
+	 * Properties that each endpoint shall be configured with.
+	 * These properties are populated from device tree node.
+	 */
+	u8 nr_endpoint;
+	struct endpoint_prop_t  endpoint_props[MAX_ENDPOINTS];
+};
+
+/*
+ * nvscic2c-pcie module context.
+ * Contains all the information for all
+ *  - Configuration parameters per device-tree.
+ */
+struct driver_ctx_t {
+	/* driver mode as parsed from compatible string in device-tree.*/
+	enum drv_mode_t drv_mode;
+	char *drv_name;
+
+	/* the configuration for module and it's endpoints.*/
+	struct driver_param_t drv_param;
+
+	/*
+	 * Visible region to peer SoC for PCIe writes. In nvscic2c-pcie
+	 * use-cases, it is backed by physical memory allocated for
+	 * comm-channel, endpoints and stream-objs(mem and sync), etc.
+	 *
+	 * Peer's write lands here to be read by local/self.
+	 */
+	struct dma_buff_t self_mem;
+
+	/*
+	 * Point to peer's visible region for data-writes. This is a PCIe
+	 * aperture which allows local/self to write into peer's memory.
+	 */
+	struct pci_aper_t peer_mem;
+
+	/* pci-client abstraction handle.*/
+	void *pci_client_h;
+
+	/* comm-channel abstraction. */
+	void *comm_channel_h;
+
+	/* vmap abstraction, this can be moved within endpoints.*/
+	void *vmap_h;
+
+	/* tegra-pcie-edma module handle.*/
+	void *edma_h;
+
+	/* endpoint absraction handle.*/
+	void *endpoints_h;
+
+	/* context of EPC(pci_dev).*/
+	void *dev_ctx;
+
+	/* SOC arch types */
+	enum peer_cpu_t  peer_cpu;
+};
+
+/*
+ * Look-up device tree node for the compatible string. Check for the
+ * pci-dev-id within the compatible node, if more than one such node found also
+ * return error.
+ */
+int
+cfg_parse(u32 pci_dev_id, enum drv_mode_t drv_mode,
+	 struct driver_param_t *drv_param);
+
+#endif //__MODULE_H__
diff --git a/drivers/misc/nvscic2c-pcie/nvscic2c-pcie-ioctl.h b/drivers/misc/nvscic2c-pcie/nvscic2c-pcie-ioctl.h
new file mode 100644
index 000000000000..79bdf13e8cde
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/nvscic2c-pcie-ioctl.h
@@ -0,0 +1,277 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __UAPI_NVSCIC2C_PCIE_IOCTL_H__
+#define __UAPI_NVSCIC2C_PCIE_IOCTL_H__
+
+#include <linux/ioctl.h>
+#include <linux/types.h>
+
+#if !defined(__KERNEL__)
+#define __user
+#include <stdbool.h>
+#endif
+
+#define MAX_NAME_SZ		(32)
+
+/* Link status between the two peers - encapsulates PCIe link also.*/
+enum nvscic2c_pcie_link {
+	NVSCIC2C_PCIE_LINK_DOWN = 0,
+	NVSCIC2C_PCIE_LINK_UP,
+};
+
+/**
+ * stream extensions - object type.
+ */
+enum nvscic2c_pcie_obj_type {
+	NVSCIC2C_PCIE_OBJ_TYPE_INVALID = 0,
+
+	/* local NvRmHandle(x86) obj. */
+	NVSCIC2C_PCIE_OBJ_TYPE_SOURCE_MEM,
+
+	/* Exported NvRmHandle(x86) obj. */
+	NVSCIC2C_PCIE_OBJ_TYPE_TARGET_MEM,
+
+	/* local GPU Semaphore(x86) obj. */
+	NVSCIC2C_PCIE_OBJ_TYPE_LOCAL_SYNC,
+
+	/* Exported GPU Semaphore(x86) obj. */
+	NVSCIC2C_PCIE_OBJ_TYPE_REMOTE_SYNC,
+
+	/* (virtual) objects imported from remote SoC. */
+	NVSCIC2C_PCIE_OBJ_TYPE_IMPORT,
+
+	NVSCIC2C_PCIE_OBJ_TYPE_MAXIMUM,
+};
+
+/**
+ * PCIe aperture and PCIe shared memory
+ * are divided in different C2C endpoints.
+ * Data structure represents endpoint's
+ * physical address and size.
+ */
+struct nvscic2c_pcie_endpoint_mem_info {
+	/* would be one of the enum nvscic2c_mem_type.*/
+	__u32 offset;
+
+	/* size of this memory type device would like user-space to map.*/
+	__u32 size;
+};
+
+/**
+ * NvSciIpc endpoint information relayed to UMD. This information
+ * is per endpoint which shall allow UMD to mmap the endpoint's
+ * send, recv and pcie link area in user-space.
+ */
+struct nvscic2c_pcie_endpoint_info {
+	__u32 nframes;
+	__u32 frame_size;
+	struct nvscic2c_pcie_endpoint_mem_info peer;
+	struct nvscic2c_pcie_endpoint_mem_info self;
+	struct nvscic2c_pcie_endpoint_mem_info link;
+};
+
+/**
+ * stream extensions - Pin/Map.
+ */
+struct nvscic2c_pcie_map_in_arg {
+	/*
+	 * Mem and Sync obj - user-space virtual address of NvSciC2cPcieBufRmHandle
+	 */
+	__u64 vaddr;
+	__u64 size;
+	__u32 mem_type;
+};
+
+struct nvscic2c_pcie_map_out_arg {
+	__s32 handle;
+	__u32 pad;
+};
+
+struct nvscic2c_pcie_map_obj_args {
+	__s32 obj_type;
+	__u32 pad;
+	struct nvscic2c_pcie_map_in_arg in;
+	struct nvscic2c_pcie_map_out_arg out;
+};
+
+/**
+ * stream extensions - Export.
+ */
+struct nvscic2c_pcie_export_in_arg {
+	__s32 handle;
+	__u32 pad;
+};
+
+struct nvscic2c_pcie_export_out_arg {
+	__u64 desc;
+};
+
+struct nvscic2c_pcie_export_obj_args {
+	__s32 obj_type;
+	__u32 pad;
+	struct nvscic2c_pcie_export_in_arg in;
+	struct nvscic2c_pcie_export_out_arg out;
+};
+
+/**
+ * stream extensions - Import.
+ */
+struct nvscic2c_pcie_import_in_arg {
+	__u64 desc;
+};
+
+struct nvscic2c_pcie_import_out_arg {
+	__s32 handle;
+	__u32 pad;
+};
+
+struct nvscic2c_pcie_import_obj_args {
+	__s32 obj_type;
+	__u32 pad;
+	struct nvscic2c_pcie_import_in_arg in;
+	struct nvscic2c_pcie_import_out_arg out;
+};
+
+/**
+ * stream extensions - Free Pinned Or Imported objects.
+ */
+struct nvscic2c_pcie_free_obj_args {
+	__s32 obj_type;
+	__s32 handle;
+};
+
+/**
+ * stream extensions - one transfer/copy unit.
+ */
+struct nvscic2c_pcie_flush_range {
+	__s32 src_handle;
+	__s32 dst_handle;
+	__u64 offset;
+	__u64 size;
+};
+
+/*
+ * @local_post_fences: user memory atleast of size:
+ *  num_local_post_fences * sizeof(__s32) - local sync handles
+ *
+ * @remote_post_fences: user memory atleast of size:
+ *  num_remote_post_fences * sizeof(__s32) - import sync handles
+ *
+ * @copy_requests: user memory atleast of size:
+ *  num_flush_ranges * sizeof(struct nvscic2c_pcie_flush_range)
+ */
+struct nvscic2c_pcie_submit_copy_args {
+	__u64 num_local_post_fences;
+	__u64 local_post_fences;
+	__u64 num_remote_post_fences;
+	__u64 remote_post_fences;
+	__u64 num_flush_ranges;
+	__u64 flush_ranges;
+	__u64 local_post_fence_values;
+};
+
+/**
+ * stream extensions - Pass upper limit for the total possible outstanding
+ * submit copy requests.
+ * @max_copy_requests: Maximum outstanding @nvscic2c_pcie_submit_copy_args.
+ * @max_flush_ranges: Maximum @nvscic2c_pcie_flush_range possible for each
+ *  of the @max_copy_requests (@nvscic2c_pcie_submit_copy_args)
+ * @max_post_fences: Maximum post-fences possible for each of the
+ *  @max_copy_requests (@nvscic2c_pcie_submit_copy_args)
+ */
+struct nvscic2c_pcie_max_copy_args {
+	__u64 max_copy_requests;
+	__u64 max_flush_ranges;
+	__u64 max_post_fences;
+};
+
+struct nvscic2c_link_change_ack {
+	bool done;
+};
+
+/* Only to facilitate calculation of maximum size of ioctl arguments.*/
+union nvscic2c_pcie_ioctl_arg_max_size {
+	struct nvscic2c_pcie_max_copy_args mc;
+	struct nvscic2c_pcie_submit_copy_args cr;
+	struct nvscic2c_pcie_free_obj_args fo;
+	struct nvscic2c_pcie_import_obj_args io;
+	struct nvscic2c_pcie_export_obj_args eo;
+	struct nvscic2c_pcie_map_obj_args mp;
+	struct nvscic2c_pcie_endpoint_info ep;
+	struct nvscic2c_link_change_ack ack;
+};
+
+/* IOCTL magic number - seen available in ioctl-number.txt*/
+#define NVSCIC2C_PCIE_IOCTL_MAGIC    0xC2
+
+#define NVSCIC2C_PCIE_IOCTL_GET_INFO \
+	_IOWR(NVSCIC2C_PCIE_IOCTL_MAGIC, 1,\
+	      struct nvscic2c_pcie_endpoint_info)
+
+/**
+ * notify remote
+ */
+#define NVSCIC2C_PCIE_IOCTL_NOTIFY_REMOTE \
+	_IO(NVSCIC2C_PCIE_IOCTL_MAGIC, 2)
+
+/**
+ * Pin/Map Mem or Sync objects.
+ */
+#define NVSCIC2C_PCIE_IOCTL_MAP \
+	_IOWR(NVSCIC2C_PCIE_IOCTL_MAGIC, 3,\
+	      struct nvscic2c_pcie_map_obj_args)
+
+/**
+ * Get Export descriptor for Target/Remote Mem/Sync objects.
+ */
+#define NVSCIC2C_PCIE_IOCTL_GET_AUTH_TOKEN \
+	_IOWR(NVSCIC2C_PCIE_IOCTL_MAGIC, 4,\
+	      struct nvscic2c_pcie_export_obj_args)
+
+/**
+ * Get Handle from the imported export descriptor.
+ */
+#define NVSCIC2C_PCIE_IOCTL_GET_HANDLE \
+	_IOWR(NVSCIC2C_PCIE_IOCTL_MAGIC, 5,\
+	      struct nvscic2c_pcie_import_obj_args)
+
+/**
+ * Free the Mapped/Pinned Source, Target or Imported Mem or Sync object handle.
+ */
+#define NVSCIC2C_PCIE_IOCTL_FREE \
+	_IOW(NVSCIC2C_PCIE_IOCTL_MAGIC, 6,\
+	      struct nvscic2c_pcie_free_obj_args)
+
+/**
+ * Submit a Copy request for transfer.
+ */
+#define NVSCIC2C_PCIE_IOCTL_SUBMIT_COPY_REQUEST \
+	_IOW(NVSCIC2C_PCIE_IOCTL_MAGIC, 7,\
+	      struct nvscic2c_pcie_submit_copy_args)
+
+/**
+ * Set the maximum possible outstanding copy requests that can be submitted.
+ */
+#define NVSCIC2C_PCIE_IOCTL_MAX_COPY_REQUESTS \
+	_IOW(NVSCIC2C_PCIE_IOCTL_MAGIC, 8,\
+	      struct nvscic2c_pcie_max_copy_args)
+
+#define NVSCIC2C_PCIE_LINK_STATUS_CHANGE_ACK \
+	_IOW(NVSCIC2C_PCIE_IOCTL_MAGIC, 9,\
+	     struct nvscic2c_link_change_ack)
+
+#define NVSCIC2C_PCIE_IOCTL_NUMBER_MAX 9
+
+#endif /*__UAPI_NVSCIC2C_PCIE_IOCTL_H__*/
diff --git a/drivers/misc/nvscic2c-pcie/pci-client.c b/drivers/misc/nvscic2c-pcie/pci-client.c
new file mode 100644
index 000000000000..70e4de4c42c0
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/pci-client.c
@@ -0,0 +1,537 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+#define pr_fmt(fmt)	"nvscic2c-pcie: pci-client: " fmt
+
+#include <linux/dma-buf.h>
+#include <linux/dma-iommu.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+#include <linux/pci-epc.h>
+#include <linux/pci-epf.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/types.h>
+#include <linux/module.h>
+
+#include "common.h"
+#include "iova-mngr.h"
+#include "pci-client.h"
+
+/* Anticipate as many users as endpoints in worst-case. */
+#define MAX_LINK_EVENT_USERS	(MAX_ENDPOINTS)
+
+/*
+ * BAR0 base + Offset:64K, Size=64K is not usable. We cannot leave a hole so
+ * mark all first 128K as to be skipped for use.
+ */
+#define SKIP_BAR_AREA		(SZ_128K)
+
+/* Internal private data-structure as PCI client. */
+struct pci_client_t {
+	struct device *dev;
+	struct iommu_domain *domain;
+
+	/* Recv area. Peer's write reflect here. */
+	struct dma_buff_t *self_mem;
+
+	/* Send area. PCIe aperture area. Self's Write reach Peer via this.*/
+	struct pci_aper_t *peer_mem;
+
+	/* PCI link status memory. mmap() to user-space.*/
+	atomic_t link_status;
+	struct cpu_buff_t link_status_mem;
+
+	/*
+	 * Lock to guard users getting un/registered and link status change
+	 * invocation at the same time. Also, to protect table from concurrent
+	 * access.
+	 */
+	struct mutex event_tbl_lock;
+
+	/* Table of users registered for change in PCI link status. */
+	struct event_t {
+		/* is taken.*/
+		atomic_t in_use;
+
+		/* callback to invoke when change in status is seen.*/
+		struct callback_ops cb_ops;
+	} event_tbl[MAX_LINK_EVENT_USERS];
+
+	/*
+	 * Skip reserved iova for use. This area in BAR0 aperture is reserved for
+	 * GIC SPI interrupt mechanism. As the allocation, fragmentration
+	 * of iova must be identical on both @DRV_MODE_EPF and @DRV_MODE_EPC
+	 * skip this area for use in @DRV_MODE_EPC also. We skip by reserving
+	 * the iova region and thereby marking it as unusable.
+	 */
+	void *skip_iova;
+
+	/*
+	 * iova-mngr instance for managing the reserved iova region.
+	 * application allocated objs and endpoints allocated physical memory
+	 * are pinned to this address.
+	 */
+	void *mem_mngr_h;
+	/*
+	 * the context of DRV_MODE_EPC
+	 */
+	struct driver_ctx_t *drv_ctx;
+
+};
+
+static void
+free_link_status_mem(struct pci_client_t *ctx)
+{
+	if (!ctx || !ctx->link_status_mem.pva)
+		return;
+
+	kfree(ctx->link_status_mem.pva);
+	ctx->link_status_mem.pva = NULL;
+}
+
+static int
+allocate_link_status_mem(struct pci_client_t *ctx)
+{
+	int ret = 0;
+	struct cpu_buff_t *mem = &ctx->link_status_mem;
+
+	mem->size = PAGE_ALIGN(sizeof(enum nvscic2c_pcie_link));
+	mem->pva  = kzalloc(mem->size, GFP_KERNEL);
+	if (WARN_ON(!mem->pva))
+		return -ENOMEM;
+
+	atomic_set(&ctx->link_status, NVSCIC2C_PCIE_LINK_DOWN);
+	*((enum nvscic2c_pcie_link *)mem->pva) = NVSCIC2C_PCIE_LINK_DOWN;
+
+	/* physical address to be mmap() in user-space.*/
+	mem->phys_addr = virt_to_phys(mem->pva);
+
+	return ret;
+}
+
+int
+pci_client_init(struct pci_client_params *params, void **pci_client_h)
+{
+	u32 i = 0;
+	int ret = 0;
+	struct pci_client_t *ctx = NULL;
+
+	/* should not be an already instantiated pci client context. */
+	if (WARN_ON(!pci_client_h || !params))
+		return -EINVAL;
+	if (WARN_ON(!params->self_mem || !params->peer_mem || !params->dev))
+		return -EINVAL;
+
+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+	if (WARN_ON(!ctx))
+		return -ENOMEM;
+	ctx->dev = params->dev;
+	ctx->self_mem = params->self_mem;
+	ctx->peer_mem = params->peer_mem;
+	mutex_init(&ctx->event_tbl_lock);
+
+	/* for link event notifications. */
+	for (i = 0; i < MAX_LINK_EVENT_USERS; i++)
+		atomic_set(&ctx->event_tbl[i].in_use, 0);
+
+	ret = allocate_link_status_mem(ctx);
+	if (ret)
+		goto err;
+
+	/*
+	 * for mapping application objs and endpoint physical memory to remote
+	 * visible area.
+	 */
+	ctx->domain = iommu_get_domain_for_dev(ctx->dev);
+	if (WARN_ON(!ctx->domain)) {
+		pr_err("iommu not available for the pci device\n");
+		goto err;
+	}
+
+	/*
+	 * configure iova manager for inbound/self_mem. Application
+	 * supplied objs shall be pinned to this area.
+	 */
+	ret = iova_mngr_init("self_mem",
+			     ctx->self_mem->dma_handle, ctx->self_mem->size,
+			     &ctx->mem_mngr_h);
+	if (ret) {
+		pr_err("Failed to initialize iova memory manager\n");
+		goto err;
+	}
+
+	/*
+	 * Skip reserved iova for any use. This area in BAR0 is reserved for
+	 * GIC SPI interrupt mechanism. As the allocation, fragmentation
+	 * of iova must be identical on both @DRV_MODE_EPF and @DRV_MODE_EPC
+	 * skip this area for use in @DRV_MODE_EPC also. We skip by reserving
+	 * the iova region and thereby marking it as unusable for others.
+	 */
+	ret = iova_mngr_block_reserve(ctx->mem_mngr_h, SKIP_BAR_AREA,
+				      NULL, NULL, &ctx->skip_iova);
+	if (ret) {
+		pr_err("Failed to skip the reserved iova region\n");
+		goto err;
+	}
+	*pci_client_h = ctx;
+	return ret;
+
+err:
+	pci_client_deinit((void **)&ctx);
+	return ret;
+}
+
+void
+pci_client_deinit(void **pci_client_h)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)(*pci_client_h);
+
+	if (WARN_ON(!ctx))
+		return;
+
+	if (ctx->skip_iova) {
+		iova_mngr_block_release(ctx->mem_mngr_h, &ctx->skip_iova);
+		ctx->skip_iova = NULL;
+	}
+
+	if (ctx->mem_mngr_h) {
+		iova_mngr_deinit(&ctx->mem_mngr_h);
+		ctx->mem_mngr_h = NULL;
+	}
+
+	free_link_status_mem(ctx);
+	mutex_destroy(&ctx->event_tbl_lock);
+	kfree(ctx);
+
+	*pci_client_h = NULL;
+}
+
+int
+pci_client_alloc_iova(void *pci_client_h, size_t size, u64 *iova,
+		      size_t *offset, void **block_h)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx))
+		return -EINVAL;
+
+	return iova_mngr_block_reserve(ctx->mem_mngr_h, size,
+				       iova, offset, block_h);
+}
+
+int
+pci_client_free_iova(void *pci_client_h, void **block_h)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (!ctx || !block_h)
+		return -EINVAL;
+
+	return iova_mngr_block_release(ctx->mem_mngr_h, block_h);
+}
+
+int
+pci_client_map_addr(void *pci_client_h, u64 to_iova, phys_addr_t paddr,
+		    size_t size, int prot)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx || !to_iova || !paddr || !size))
+		return -EINVAL;
+
+	return iommu_map(ctx->domain, to_iova, paddr, size, prot);
+}
+
+size_t
+pci_client_unmap_addr(void *pci_client_h, u64 from_iova, size_t size)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx))
+		return 0;
+	if (WARN_ON(!ctx->domain))
+		return 0;
+
+	return iommu_unmap(ctx->domain, from_iova, size);
+}
+
+int
+pci_client_get_peer_aper(void *pci_client_h, size_t offsetof, size_t size,
+			 phys_addr_t *phys_addr)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx || !size || !phys_addr))
+		return -EINVAL;
+
+	if (ctx->peer_mem->size < (offsetof + size))
+		return -ENOMEM;
+
+	*phys_addr = ctx->peer_mem->aper + offsetof;
+
+	return 0;
+}
+
+struct dma_buf_attachment *
+pci_client_dmabuf_attach(void *pci_client_h, struct dma_buf *dmabuff)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx || !dmabuff))
+		return ERR_PTR(-EINVAL);
+
+	return dma_buf_attach(dmabuff, ctx->dev);
+}
+
+void
+pci_client_dmabuf_detach(void *pci_client_h, struct dma_buf *dmabuff,
+			 struct dma_buf_attachment *attach)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (!ctx || !dmabuff || !attach)
+		return;
+
+	return dma_buf_detach(dmabuff, attach);
+}
+
+#ifdef NVSCIC2C_GPU
+int pci_client_dma_map_vidmem(void *pci_client_h, struct nvidia_p2p_page_table *page_table,
+			      struct nvidia_p2p_dma_mapping **dma_mapping)
+{
+	int ret = 0;
+	struct pci_dev *pdev = pci_client_get_pci_dev(pci_client_h);
+
+	if (WARN_ON(!pdev || !page_table || !dma_mapping))
+		return -EINVAL;
+
+	ret = nvidia_p2p_dma_map_pages(pdev, page_table, dma_mapping);
+	if (ret) {
+		pr_err("nvidia_p2p_dma_map_pages failed with err = %d\n", ret);
+		return ret;
+	}
+	if (!NVIDIA_P2P_DMA_MAPPING_VERSION_COMPATIBLE(*dma_mapping)) {
+		nvidia_p2p_dma_unmap_pages(pdev, page_table, *dma_mapping);
+		return -EINVAL;
+	}
+	return ret;
+}
+
+void pci_client_dma_unmap_vidmem(void *pci_client_h, struct nvidia_p2p_page_table *page_table,
+				 struct nvidia_p2p_dma_mapping *dma_mapping)
+{
+	int ret = 0;
+	struct pci_dev *pdev = pci_client_get_pci_dev(pci_client_h);
+
+	if (WARN_ON(!pdev || !page_table || !dma_mapping))
+		return;
+
+	ret = nvidia_p2p_dma_unmap_pages(pdev, page_table, dma_mapping);
+	if (ret)
+		pr_err("nvidia_p2p_dma_unmap_pages failed with err = %d\n", ret);
+}
+#endif
+
+int pci_client_dma_map_sysmem(void *pci_client_h, struct sg_table *sgt,
+			      enum dma_data_direction dir)
+{
+	int count = 0;
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx || !sgt))
+		return -EINVAL;
+	if (WARN_ON(!ctx->dev || !sgt->sgl || !sgt->nents))
+		return -EINVAL;
+
+	count = dma_map_sg(ctx->dev, sgt->sgl, sgt->nents, dir);
+	if (count <= 0) {
+		pr_err("Failed to map %u sg_table entries to DMA\n", sgt->nents);
+		return -EFAULT;
+	}
+	return 0;
+}
+
+void pci_client_dma_unmap_sysmem(void *pci_client_h, struct sg_table *sgt,
+				 enum dma_data_direction dir)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx || !sgt))
+		return;
+	if (WARN_ON(!ctx->dev || !sgt->sgl || !sgt->nents))
+		return;
+
+	dma_unmap_sg(ctx->dev, sgt->sgl, sgt->nents, dir);
+}
+
+/* Helper function to mmap the PCI link status memory to user-space.*/
+int
+pci_client_mmap_link_mem(void *pci_client_h, struct vm_area_struct *vma)
+{
+	int ret = 0;
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!vma || !ctx))
+		return -EINVAL;
+	if (WARN_ON(!ctx->link_status_mem.pva))
+		return -EINVAL;
+	if ((vma->vm_end - vma->vm_start) != ctx->link_status_mem.size)
+		return -EINVAL;
+
+	vma->vm_page_prot = pgprot_noncached(vma->vm_page_prot);
+	ret = remap_pfn_range(vma,
+			      vma->vm_start,
+			      PFN_DOWN(ctx->link_status_mem.phys_addr),
+			      ctx->link_status_mem.size,
+			      vma->vm_page_prot);
+	if (ret)
+		pr_err("remap_pfn_range returns error: (%d) for Link mem\n", ret);
+
+	return ret;
+}
+
+/* Query PCI link status. */
+enum nvscic2c_pcie_link
+pci_client_query_link_status(void *pci_client_h)
+{
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx))
+		return NVSCIC2C_PCIE_LINK_DOWN;
+
+	return atomic_read(&ctx->link_status);
+}
+
+/*
+ * Users/Units can register for PCI link events as received by
+ * @DRV_MODE_EPC module abstraction.
+ */
+int
+pci_client_register_for_link_event(void *pci_client_h,
+				   struct callback_ops *ops, u32 *id)
+{
+	u32 i = 0;
+	int ret = 0;
+	struct event_t *event = NULL;
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx || !id || !ops))
+		return -EINVAL;
+	if (WARN_ON(!ops->callback))
+		return -EINVAL;
+
+	mutex_lock(&ctx->event_tbl_lock);
+	for (i = 0; i < MAX_LINK_EVENT_USERS; i++) {
+		event = &ctx->event_tbl[i];
+		if (!atomic_read(&event->in_use)) {
+			event->cb_ops.callback = ops->callback;
+			event->cb_ops.ctx = ops->ctx;
+			atomic_set(&event->in_use, 1);
+			*id = i;
+			break;
+		}
+	}
+	if (i == MAX_LINK_EVENT_USERS) {
+		ret = -ENOMEM;
+		pr_err("PCI link event registration full\n");
+	}
+	mutex_unlock(&ctx->event_tbl_lock);
+
+	return ret;
+}
+
+/* Unregister for PCI link events. - teardown only. */
+int
+pci_client_unregister_for_link_event(void *pci_client_h, u32 id)
+{
+	int ret = 0;
+	struct event_t *event = NULL;
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx))
+		return -EINVAL;
+	if (WARN_ON(id >= MAX_LINK_EVENT_USERS))
+		return -EINVAL;
+
+	mutex_lock(&ctx->event_tbl_lock);
+	event = &ctx->event_tbl[id];
+	if (atomic_read(&event->in_use)) {
+		atomic_set(&event->in_use, 0);
+		event->cb_ops.callback = NULL;
+		event->cb_ops.ctx = NULL;
+	}
+	mutex_unlock(&ctx->event_tbl_lock);
+
+	return ret;
+}
+
+/*
+ * Update the PCI link status as received in @DRV_MODE_EPC
+ * module abstraction. Propagate the link status to registered users.
+ */
+int
+pci_client_change_link_status(void *pci_client_h,
+			      enum nvscic2c_pcie_link status)
+{
+	u32 i = 0;
+	int ret = 0;
+	struct event_t *event = NULL;
+	struct callback_ops *ops = NULL;
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx))
+		return -EINVAL;
+
+	if (WARN_ON(status < NVSCIC2C_PCIE_LINK_DOWN ||
+		    status > NVSCIC2C_PCIE_LINK_UP))
+		return -EINVAL;
+
+	/*
+	 * Reflect the status for user-space to see.
+	 * For consistent view of same phys_addr by user-space, flush the update
+	 * Call is arm64 specific.
+	 */
+	atomic_set(&ctx->link_status, status);
+	*((enum nvscic2c_pcie_link *)ctx->link_status_mem.pva) = status;
+	/* interrupt registered users. */
+	mutex_lock(&ctx->event_tbl_lock);
+	for (i = 0; i < MAX_LINK_EVENT_USERS; i++) {
+		event = &ctx->event_tbl[i];
+		if (atomic_read(&event->in_use)) {
+			ops = &event->cb_ops;
+			ops->callback(NULL, ops->ctx);
+		}
+	}
+	mutex_unlock(&ctx->event_tbl_lock);
+
+	return ret;
+}
+
+/* Return pci_dev for comm-channel and endpoint */
+struct pci_dev *
+pci_client_get_pci_dev(void *pci_client_h)
+{
+	struct pci_dev *pdev = NULL;
+	struct pci_client_t *ctx = (struct pci_client_t *)pci_client_h;
+
+	if (WARN_ON(!ctx))
+		return NULL;
+
+	pdev = to_pci_dev(ctx->dev);
+
+	return  pdev;
+
+}
+
diff --git a/drivers/misc/nvscic2c-pcie/pci-client.h b/drivers/misc/nvscic2c-pcie/pci-client.h
new file mode 100644
index 000000000000..2590f3efd3b1
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/pci-client.h
@@ -0,0 +1,135 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __PCI_CLIENT_H__
+#define __PCI_CLIENT_H__
+
+#include "common.h"
+#include "module.h"
+
+#ifdef NVSCIC2C_GPU
+#include "nv-p2p.h"
+#endif
+
+#include "nvscic2c-pcie-ioctl.h"
+
+/* forward declaration.*/
+struct vm_area_struct;
+struct dma_buf;
+struct dma_buf_attachment;
+
+/*
+ * PCI client initialization parameters. The fields must remain persistent
+ * till deinitialization (exit).
+ */
+struct pci_client_params {
+	struct dma_buff_t *self_mem;
+	struct pci_aper_t *peer_mem;
+	/*
+	 * @DRV_MODE_EPC: &pci_dev->dev
+	 */
+	struct device *dev;
+};
+
+/* Initialize PCI client either for  @DRV_MODE_EPC. */
+int
+pci_client_init(struct pci_client_params *params, void **pci_client_h);
+
+/* Teardown of PCI client. */
+void
+pci_client_deinit(void **pci_client_h);
+
+/* reserve iova using the iova-manager.*/
+int
+pci_client_alloc_iova(void *pci_client_h, size_t size, u64 *address,
+		      size_t *offset, void **block_h);
+
+/* free the reserved iova.*/
+int
+pci_client_free_iova(void *pci_client_h, void **block_h);
+
+/* map the pages to IOVA.*/
+int
+pci_client_map_addr(void *pci_client_h, u64 to_iova, phys_addr_t paddr,
+		    size_t size, int prot);
+
+/* unmap the IOVA mapping.*/
+size_t
+pci_client_unmap_addr(void *pci_client_h, u64 from_iova, size_t size);
+
+/* get the pci aperture for a given offset.*/
+int
+pci_client_get_peer_aper(void *pci_client_h, size_t offsetof, size_t size,
+			 phys_addr_t *phys_addr);
+
+/* attach dma-buf to pci device.*/
+struct dma_buf_attachment *
+pci_client_dmabuf_attach(void *pci_client_h, struct dma_buf *dmabuff);
+
+/* detach dma-buf to pci device.*/
+void
+pci_client_dmabuf_detach(void *pci_client_h, struct dma_buf *dmabuff,
+			 struct dma_buf_attachment *attach);
+
+#ifdef NVSCIC2C_GPU
+/* Create DMA mapping for pages retrieved using nvidia_p2p_get_pages.*/
+int pci_client_dma_map_vidmem(void *pci_client_h, struct nvidia_p2p_page_table *page_table,
+			      struct nvidia_p2p_dma_mapping **dma_mapping);
+
+/* Unmap the DMA mappings*/
+void pci_client_dma_unmap_vidmem(void *pci_client_h, struct nvidia_p2p_page_table *page_table,
+				 struct nvidia_p2p_dma_mapping *dma_mapping);
+#endif
+
+/* Create DMA mapping for pages retrieved using get_user_pages.*/
+int pci_client_dma_map_sysmem(void *pci_client_h, struct sg_table *sgt,
+			      enum dma_data_direction dir);
+
+/* Unmap the DMA mappings*/
+void pci_client_dma_unmap_sysmem(void *pci_client_h, struct sg_table *sgt,
+				 enum dma_data_direction dir);
+
+/*
+ * Users/Units can register for PCI link events as received by
+ * @DRV_MODE_EPC module sbstraction.
+ */
+int
+pci_client_register_for_link_event(void *pci_client_h,
+				   struct callback_ops *ops, u32 *id);
+
+/* Unregister for PCI link events. - teardown only. */
+int
+pci_client_unregister_for_link_event(void *pci_client_h, u32 id);
+
+/*
+ * Update the PCI link status as received in @DRV_MODE_EPC
+ * module abstraction. Propagate the link status to registered users.
+ */
+int
+pci_client_change_link_status(void *pci_client_h,
+			      enum nvscic2c_pcie_link status);
+
+/* Helper function to mmap the PCI link status memory to user-space.*/
+int
+pci_client_mmap_link_mem(void *pci_client_h, struct vm_area_struct *vma);
+
+/* Query PCI link status. */
+enum nvscic2c_pcie_link
+pci_client_query_link_status(void *pci_client_h);
+
+/* Return pci_dev to comm-channel or endpoint. */
+struct pci_dev *
+pci_client_get_pci_dev(void *pci_client_h);
+
+#endif // __PCI_CLIENT_H__
diff --git a/drivers/misc/nvscic2c-pcie/stream-extensions.c b/drivers/misc/nvscic2c-pcie/stream-extensions.c
new file mode 100644
index 000000000000..8ca7d01b9c75
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/stream-extensions.c
@@ -0,0 +1,1384 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvscic2c-pcie: stream-ext: " fmt
+
+#include <linux/anon_inodes.h>
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/of_platform.h>
+#include <linux/file.h>
+#include <linux/fs.h>
+#include <linux/of.h>
+#include <linux/platform_device.h>
+#include <linux/slab.h>
+#include <linux/syscalls.h>
+#include <linux/version.h>
+#include <linux/fdtable.h>
+
+#include "comm-channel.h"
+#include "common.h"
+#include "descriptor.h"
+#include "module.h"
+#include "pci-client.h"
+#include "stream-extensions.h"
+#include "tegra-pcie-edma.h"
+#include "vmap.h"
+
+/* forward declaration.*/
+struct stream_ext_ctx_t;
+struct stream_ext_obj;
+
+/* limits as set for copy requests.*/
+struct copy_req_limits {
+	u64 max_copy_requests;
+	u64 max_flush_ranges;
+	u64 max_post_fences;
+};
+
+/*
+ * Copied submit-copy args from use-space. These are then parsed and validated.
+ * This copy is required as args have pointer to user-space area which be copied
+ * into kernel-space before using it. On subsequent copy, basic checks are done
+ * and shall be used to create a copy request payload for eDMA.
+ */
+struct copy_req_params {
+	u64 num_local_post_fences;
+	s32 *local_post_fences;
+	u64 num_remote_post_fences;
+	s32 *remote_post_fences;
+	u64 num_flush_ranges;
+	u64 *local_post_fence_values;
+	struct nvscic2c_pcie_flush_range *flush_ranges;
+};
+
+/* one copy request.*/
+struct copy_request {
+	/* book-keeping for copy completion.*/
+	struct list_head node;
+
+	/*
+	 * back-reference to stream_ext_context, used in eDMA callback.
+	 * to add this copy_request back in free_list for reuse
+	 */
+	struct stream_ext_ctx_t *ctx;
+
+	/*
+	 * actual number of handles per the submit-copy request.
+	 * Shall include ((2 * num_flush_range) + num_local_post_fences
+	 * + num_remote_post_fences).
+	 * used for refcounting: out of order free and copy.
+	 */
+	u64 num_handles;
+	/*
+	 * space for num_handles considering worst-case allocation:
+	 * ((2 * max_flush_ranges) + (max_post_fences)).
+	 */
+	struct stream_ext_obj **handles;
+
+	/*
+	 * actual number of edma-desc per the submit-copy request.
+	 * Shall include (num_flush_range + num_remote_post_fences (eDMAed))
+	 */
+	u64 num_edma_desc;
+	/*
+	 * space for num_edma_desc considering worst-case allocation:
+	 * (max_flush_ranges + max_post_fences), assuming submit-copy could have
+	 * all the post-fences for remote signalling by eDMA.
+	 */
+	struct tegra_pcie_edma_desc *edma_desc;
+
+	/*
+	 * actual number of local_post-fences per the submit-copy request.
+	 * Shall include (num_local_post_fences).
+	 */
+	u64 num_local_post_fences;
+
+	/*
+	 * values to be written for signling of local_post_fences based GPU
+	 * semaphores
+	 */
+	u64 *local_post_fence_values;
+	/*
+	 * space for num_local_post_fences considering worst-case allocation:
+	 * max_post_fences, assuming submit-copy could have all post-fences for
+	 * local signalling.
+	 */
+	struct stream_ext_obj **local_post_fences;
+};
+
+struct stream_ext_obj {
+	/* back-reference to vmap handle, required during free/unmap.*/
+	void *vmap_h;
+
+	/* for correctness check. */
+	enum nvscic2c_pcie_obj_type type;
+	u32 soc_id;
+	u32 cntrlr_id;
+	u32 ep_id;
+
+	/* for ordering out of order copy and free ops.*/
+	bool marked_for_del;
+	struct kref refcount;
+
+	/* virtual mapping information.*/
+	struct vmap_obj_attributes vmap;
+
+	/*
+	 * ImportObj only.
+	 * Add offsetof from peer window to local aper base for access
+	 * by local eDMA or CPU(mmap) towards peer obj.
+	 *  - For PCIe RP.
+	 * Add offsetof from peer window to local aper base for access by
+	 * CPU(mmap) towards peer obj, eDMA will use the iova direactly.
+	 *  - For PCIe EP.
+	 */
+	u32 import_type;
+	phys_addr_t aper;
+};
+
+/* stream extensions context per NvSciC2cPcie endpoint.*/
+struct stream_ext_ctx_t {
+	/*
+	 * mode - EPC(on PCIe RP) or EPF(on PCIe EP).
+	 * Destination address of eDMA descriptor is different for these
+	 * two modes.
+	 */
+	enum drv_mode_t drv_mode;
+
+	u32 ep_id;
+	char ep_name[NAME_MAX];
+
+	struct node_info_t local_node;
+	struct node_info_t peer_node;
+
+	/* vmap abstraction.*/
+	void *vmap_h;
+
+	/* tegra-pcie-edma cookie.*/
+	void *edma_h;
+
+	/* comm-channel abstraction. */
+	void *comm_channel_h;
+
+	/* pci client abstraction. */
+	void *pci_client_h;
+
+	/* max copy request limits as set by user.*/
+	struct copy_req_limits cr_limits;
+
+	/* Intermediate validated and copied user-args for submit-copy ioctl.*/
+	struct copy_req_params cr_params;
+
+	/* Async copy: book-keeping copy-requests: free and in-progress.*/
+	struct list_head free_list;
+	/* guard free_list.*/
+	struct mutex free_lock;
+	atomic_t transfer_count;
+	wait_queue_head_t transfer_waitq;
+};
+
+static int
+cache_copy_request_handles(struct copy_req_params *params,
+			   struct copy_request *cr);
+static int
+release_copy_request_handles(struct copy_request *cr);
+
+static void
+signal_local_post_fences(struct copy_request *cr);
+
+static int
+prepare_edma_desc(enum drv_mode_t drv_mode, struct copy_req_params *params,
+		  struct tegra_pcie_edma_desc *desc, u64 *num_desc);
+
+static edma_xfer_status_t
+schedule_edma_xfer(void *edma_h, void *priv, u64 num_desc,
+		   struct tegra_pcie_edma_desc *desc);
+static void
+callback_edma_xfer(void *priv, edma_xfer_status_t status,
+		   struct tegra_pcie_edma_desc *desc);
+static int
+validate_handle(struct stream_ext_ctx_t *ctx, s32 handle,
+		enum nvscic2c_pcie_obj_type type);
+static int
+allocate_handle(struct stream_ext_ctx_t *ctx,
+		enum nvscic2c_pcie_obj_type type,
+		void *ioctl_args);
+static int
+copy_args_from_user(struct stream_ext_ctx_t *ctx,
+		    struct nvscic2c_pcie_submit_copy_args *args,
+		    struct copy_req_params *params);
+static int
+allocate_copy_request(struct stream_ext_ctx_t *ctx,
+		      struct copy_request **copy_request);
+static void
+free_copy_request(struct copy_request **copy_request);
+
+static int
+allocate_copy_req_params(struct stream_ext_ctx_t *ctx,
+			 struct copy_req_params *params);
+static void
+free_copy_req_params(struct copy_req_params *params);
+
+static int
+validate_copy_req_params(struct stream_ext_ctx_t *ctx,
+			 struct copy_req_params *params);
+
+static int
+fops_mmap(struct file *filep, struct vm_area_struct *vma)
+{
+	int ret = 0;
+	u64 memaddr = 0x0;
+	u64 memsize = 0x0;
+	struct stream_ext_obj *stream_obj = NULL;
+
+	if (WARN_ON(!filep))
+		return -EFAULT;
+
+	if (WARN_ON(!(vma)))
+		return -EFAULT;
+
+	/* read access of import sync object would mean poll over pcie.*/
+	if (WARN_ON(vma->vm_flags & VM_READ))
+		return -EINVAL;
+
+	stream_obj = (struct stream_ext_obj *)(filep->private_data);
+	if (WARN_ON(stream_obj->type != NVSCIC2C_PCIE_OBJ_TYPE_IMPORT))
+		return -EOPNOTSUPP;
+	if (WARN_ON(stream_obj->import_type != STREAM_OBJ_TYPE_SYNC))
+		return -EOPNOTSUPP;
+	if (WARN_ON(stream_obj->marked_for_del))
+		return -EINVAL;
+
+	memsize = stream_obj->vmap.size;
+	memaddr = stream_obj->aper;
+
+	vma->vm_pgoff  = 0;
+	vma->vm_flags |= (VM_DONTCOPY);
+	vma->vm_page_prot = pgprot_device(vma->vm_page_prot);
+	ret = remap_pfn_range(vma, vma->vm_start,
+			      PFN_DOWN(memaddr),
+			      memsize, vma->vm_page_prot);
+	if (ret)
+		pr_err("mmap() failed for Imported sync object\n");
+
+	return ret;
+}
+
+static void
+streamobj_free(struct kref *kref)
+{
+	struct stream_ext_obj *stream_obj = NULL;
+
+	if (!kref)
+		return;
+
+	stream_obj = container_of(kref, struct stream_ext_obj, refcount);
+	if (stream_obj) {
+		vmap_obj_unmap(stream_obj->vmap_h, stream_obj->vmap.type,
+			       stream_obj->vmap.id);
+		kfree(stream_obj);
+	}
+}
+
+static int
+fops_release(struct inode *inode, struct file *filep)
+{
+	struct stream_ext_obj *stream_obj =
+				(struct stream_ext_obj *)(filep->private_data);
+
+	if (WARN_ON(!stream_obj))
+		return -EFAULT;
+
+	/*
+	 * actual free happens when the refcount reaches zero. This is done to
+	 * accommodate: out of order free while copy isin progress use-case.
+	 */
+	stream_obj->marked_for_del = true;
+	kref_put(&stream_obj->refcount, streamobj_free);
+
+	return 0;
+}
+
+/* for all stream objs - Local, remote + Mem, Sync, Import*/
+static const struct file_operations fops_default = {
+	.owner = THIS_MODULE,
+	.release = fops_release,
+	.mmap = fops_mmap,
+};
+
+/* implement NVSCIC2C_PCIE_IOCTL_FREE ioctl call. */
+static int
+ioctl_free_obj(struct stream_ext_ctx_t *ctx,
+	       struct nvscic2c_pcie_free_obj_args *args)
+{
+	int ret = 0;
+
+	/* validate the input handle for correctness.*/
+	ret = validate_handle(ctx, args->handle, args->obj_type);
+	if (ret)
+		return ret;
+
+	ksys_close(args->handle);
+	return 0;
+}
+
+/* implement NVSCIC2C_PCIE_IOCTL_GET_AUTH_TOKEN call. */
+static int
+ioctl_export_obj(struct stream_ext_ctx_t *ctx,
+		 struct nvscic2c_pcie_export_obj_args *args)
+{
+	int ret = 0;
+	u64 exp_desc = 0;
+	struct comm_msg msg = {0};
+	struct file *filep = NULL;
+	struct stream_ext_obj *stream_obj = NULL;
+	struct node_info_t *peer = &ctx->peer_node;
+	enum vmap_obj_type export_type = STREAM_OBJ_TYPE_MEM;
+
+	/* validate the input handle for correctness.*/
+	ret = validate_handle(ctx, args->in.handle, args->obj_type);
+	if (ret)
+		return ret;
+
+	/* only target/remote can be exported.*/
+	if (args->obj_type == NVSCIC2C_PCIE_OBJ_TYPE_TARGET_MEM)
+		export_type = STREAM_OBJ_TYPE_MEM;
+	else if (args->obj_type == NVSCIC2C_PCIE_OBJ_TYPE_REMOTE_SYNC)
+		export_type = STREAM_OBJ_TYPE_SYNC;
+	else
+		return -EINVAL;
+
+	filep = fget(args->in.handle);
+	stream_obj = filep->private_data;
+	/*
+	 * take a reference to the virtual mapping. The reference shall be
+	 * released by peer when the peer unregisters it's corresponding
+	 * imported obj. This happens via comm-channel.
+	 *
+	 * reference count of stream_obj is not taken, valid scenario to
+	 * free the exported obj from this SoC but the virtual mapping
+	 * to continue exist and is released when peer SoC releases it's
+	 * corresponding import stream obj.
+	 */
+	ret = vmap_obj_getref(stream_obj->vmap_h, stream_obj->vmap.type,
+			      stream_obj->vmap.id);
+	if (ret) {
+		pr_err("(%s): Failed ref counting an object\n", ctx->ep_name);
+		fput(filep);
+		return ret;
+	}
+
+	/* generate export desc.*/
+	peer = &ctx->peer_node;
+	exp_desc = gen_desc(peer->board_id, peer->soc_id, peer->cntrlr_id,
+			    ctx->ep_id, export_type, stream_obj->vmap.id);
+
+	/*share it with peer for peer for corresponding import.*/
+	pr_debug("Exporting descriptor = (%llu)\n", exp_desc);
+	msg.type = COMM_MSG_TYPE_REGISTER;
+	msg.u.reg.export_desc = exp_desc;
+	msg.u.reg.iova = stream_obj->vmap.iova;
+	msg.u.reg.size = stream_obj->vmap.size;
+	msg.u.reg.offsetof = stream_obj->vmap.offsetof;
+	ret = comm_channel_msg_send(ctx->comm_channel_h, &msg);
+	if (ret)
+		vmap_obj_putref(stream_obj->vmap_h, stream_obj->vmap.type,
+				stream_obj->vmap.id);
+	else
+		args->out.desc = exp_desc;
+
+	fput(filep);
+	return ret;
+}
+
+/* implement NVSCIC2C_PCIE_IOCTL_GET_HANDLE call. */
+static int
+ioctl_import_obj(struct stream_ext_ctx_t *ctx,
+		 struct nvscic2c_pcie_import_obj_args *args)
+{
+	int ret = 0;
+	s32 handle = -1;
+	struct file *filep = NULL;
+	struct stream_ext_obj *stream_obj = NULL;
+	struct node_info_t *local = &ctx->local_node;
+
+	if (args->obj_type != NVSCIC2C_PCIE_OBJ_TYPE_IMPORT)
+		return -EINVAL;
+
+	/* validate the incoming descriptor.*/
+	ret = validate_desc(args->in.desc, local->board_id, local->soc_id,
+			    local->cntrlr_id, ctx->ep_id);
+	if (ret) {
+		pr_err("(%s): Invalid descriptor: (%llu) received\n",
+		       ctx->ep_name, args->in.desc);
+		return ret;
+	}
+
+	/*
+	 * Import the desc :- create virt. mapping, bind it to a stream_obj
+	 * and create a UMD handle for this stream_obj.
+	 */
+	handle = allocate_handle(ctx, args->obj_type, (void *)args);
+	if (handle < 0)
+		return handle;
+
+	filep = fget(handle);
+	if (!filep)
+		return -ENOMEM;
+	stream_obj = filep->private_data;
+	stream_obj->import_type = get_handle_type_from_desc(args->in.desc);
+
+	ret = pci_client_get_peer_aper(ctx->pci_client_h, stream_obj->vmap.offsetof,
+				       stream_obj->vmap.size, &stream_obj->aper);
+	if (ret) {
+		pr_err("(%s): PCI Client Get Peer Aper Failed\n", ctx->ep_name);
+		fput(filep);
+		return ret;
+	}
+	fput(filep);
+
+	args->out.handle = handle;
+
+	return ret;
+}
+
+/* implement NVSCIC2C_PCIE_IOCTL_MAP ioctl call. */
+static int
+ioctl_map_obj(struct stream_ext_ctx_t *ctx,
+	      struct nvscic2c_pcie_map_obj_args *args)
+{
+	int ret = 0;
+	s32 handle = -1;
+
+	/*
+	 * Create virt. mapping for the user primitive objs - Mem or Sync.
+	 * Bind it to a stream_obj. Create a UMD handle for this stream_obj.
+	 */
+	handle = allocate_handle(ctx, args->obj_type, (void *)args);
+	if (handle < 0)
+		return handle;
+
+	args->out.handle = handle;
+
+	return ret;
+}
+
+/* implement NVSCIC2C_PCIE_IOCTL_SUBMIT_COPY_REQUEST ioctl call. */
+static int
+ioctl_submit_copy_request(struct stream_ext_ctx_t *ctx,
+			  struct nvscic2c_pcie_submit_copy_args *args)
+{
+	int ret = 0;
+	struct copy_request *cr = NULL;
+	edma_xfer_status_t edma_status = EDMA_XFER_FAIL_INVAL_INPUTS;
+	enum nvscic2c_pcie_link link = NVSCIC2C_PCIE_LINK_DOWN;
+
+	link = pci_client_query_link_status(ctx->pci_client_h);
+	if (link != NVSCIC2C_PCIE_LINK_UP)
+		return -ENOLINK;
+
+	/* copy user-supplied submit-copy args.*/
+	ret = copy_args_from_user(ctx, args, &ctx->cr_params);
+	if (ret)
+		return ret;
+
+	/* validate the user-supplied handles in flush_range and post-fence.*/
+	ret = validate_copy_req_params(ctx, &ctx->cr_params);
+	if (ret)
+		return ret;
+
+	/* get one copy-request from the free list.*/
+	mutex_lock(&ctx->free_lock);
+	if (list_empty(&ctx->free_list)) {
+		/*
+		 * user supplied more than mentioned in max_copy_requests OR
+		 * eDMA async didn't invoke callback when eDMA was done.
+		 */
+		mutex_unlock(&ctx->free_lock);
+		return -EAGAIN;
+	}
+	cr = list_first_entry(&ctx->free_list, struct copy_request, node);
+	list_del(&cr->node);
+	mutex_unlock(&ctx->free_lock);
+
+	/*
+	 * To support out-of-order free and copy-requets when eDMA is in async
+	 * mode, cache all the handles from the copy-submit params and increment
+	 * their reference count before eDMA ops. Post eDMA, decrement the
+	 * reference, thereby, when during in-progress eDMA, free() is received
+	 * for the same set of handles, the handles would be marked for deletion
+	 * but doesn't actually get deleted.
+	 */
+	ret = cache_copy_request_handles(&ctx->cr_params, cr);
+	if (ret)
+		goto reclaim_cr;
+
+	/* generate eDMA descriptors from flush_ranges, remote_post_fences.*/
+	ret = prepare_edma_desc(ctx->drv_mode, &ctx->cr_params, cr->edma_desc,
+				&cr->num_edma_desc);
+	if (ret) {
+		release_copy_request_handles(cr);
+		goto reclaim_cr;
+	}
+	/* schedule asynchronous eDMA.*/
+	atomic_inc(&ctx->transfer_count);
+	edma_status = schedule_edma_xfer(ctx->edma_h, (void *)cr,
+					 cr->num_edma_desc, cr->edma_desc);
+	if (edma_status != EDMA_XFER_SUCCESS) {
+		pr_err("epc: schedule_edma_xfer failed\n");
+		ret = -EIO;
+		atomic_dec(&ctx->transfer_count);
+		release_copy_request_handles(cr);
+		goto reclaim_cr;
+	}
+
+	return ret;
+
+reclaim_cr:
+	mutex_lock(&ctx->free_lock);
+	list_add_tail(&cr->node, &ctx->free_list);
+	mutex_unlock(&ctx->free_lock);
+	return ret;
+}
+
+/* implement NVSCIC2C_PCIE_IOCTL_MAX_COPY_REQUESTS ioctl call. */
+static int
+ioctl_set_max_copy_requests(struct stream_ext_ctx_t *ctx,
+			    struct nvscic2c_pcie_max_copy_args *args)
+{
+	int ret = 0;
+	u32 i = 0;
+	struct copy_request *cr = NULL;
+	struct list_head *curr = NULL, *next = NULL;
+
+	if (WARN_ON(!args->max_copy_requests ||
+		    !args->max_flush_ranges ||
+		    !args->max_post_fences))
+		return -EINVAL;
+
+	/* limits already set.*/
+	if (WARN_ON(ctx->cr_limits.max_copy_requests ||
+		    ctx->cr_limits.max_flush_ranges ||
+		    ctx->cr_limits.max_post_fences))
+		return -EINVAL;
+
+	ctx->cr_limits.max_copy_requests = args->max_copy_requests;
+	ctx->cr_limits.max_flush_ranges = args->max_flush_ranges;
+	ctx->cr_limits.max_post_fences = args->max_post_fences;
+
+	/* allocate one submit-copy params.*/
+	ret = allocate_copy_req_params(ctx, &ctx->cr_params);
+	if (ret) {
+		pr_err("Failed to allocate submit-copy params\n");
+		goto clean_up;
+	}
+
+	/* allocate the maximum outstanding copy requests we can have.*/
+	for (i = 0; i < ctx->cr_limits.max_copy_requests; i++) {
+		cr = NULL;
+		ret = allocate_copy_request(ctx, &cr);
+		if (ret) {
+			pr_err("Failed to allocate copy request\n");
+			goto clean_up;
+		}
+
+		mutex_lock(&ctx->free_lock);
+		list_add(&cr->node, &ctx->free_list);
+		mutex_unlock(&ctx->free_lock);
+	}
+
+	return ret;
+
+clean_up:
+	mutex_unlock(&ctx->free_lock);
+	list_for_each_safe(curr, next, &ctx->free_list) {
+		cr = list_entry(curr, struct copy_request, node);
+		list_del(curr);
+		free_copy_request(&cr);
+	}
+	mutex_unlock(&ctx->free_lock);
+
+	free_copy_req_params(&ctx->cr_params);
+
+	return ret;
+}
+
+int
+stream_extension_ioctl(void *stream_ext_h, unsigned int cmd, void *args)
+{
+	int ret = 0;
+	struct stream_ext_ctx_t *ctx = NULL;
+
+	if (WARN_ON(!stream_ext_h || !args))
+		return -EINVAL;
+
+	ctx = (struct stream_ext_ctx_t *)stream_ext_h;
+	switch (cmd) {
+	case NVSCIC2C_PCIE_IOCTL_MAP:
+		ret = ioctl_map_obj
+			((struct stream_ext_ctx_t *)ctx,
+			 (struct nvscic2c_pcie_map_obj_args *)args);
+		break;
+	case NVSCIC2C_PCIE_IOCTL_GET_AUTH_TOKEN:
+		ret = ioctl_export_obj
+			((struct stream_ext_ctx_t *)ctx,
+			 (struct nvscic2c_pcie_export_obj_args *)args);
+		break;
+	case NVSCIC2C_PCIE_IOCTL_GET_HANDLE:
+		ret = ioctl_import_obj
+			((struct stream_ext_ctx_t *)ctx,
+			 (struct nvscic2c_pcie_import_obj_args *)args);
+		break;
+	case NVSCIC2C_PCIE_IOCTL_FREE:
+		ret = ioctl_free_obj
+			((struct stream_ext_ctx_t *)ctx,
+			 (struct nvscic2c_pcie_free_obj_args *)args);
+		break;
+	case NVSCIC2C_PCIE_IOCTL_SUBMIT_COPY_REQUEST:
+		ret = ioctl_submit_copy_request
+			((struct stream_ext_ctx_t *)ctx,
+			 (struct nvscic2c_pcie_submit_copy_args *)args);
+		break;
+	case NVSCIC2C_PCIE_IOCTL_MAX_COPY_REQUESTS:
+		ret = ioctl_set_max_copy_requests
+			((struct stream_ext_ctx_t *)ctx,
+			 (struct nvscic2c_pcie_max_copy_args *)args);
+		break;
+	default:
+		pr_err("(%s): unrecognised nvscic2c-pcie ioclt cmd: 0x%x\n",
+		       ctx->ep_name, cmd);
+		ret = -ENOTTY;
+		break;
+	}
+	return ret;
+}
+
+int
+stream_extension_init(struct stream_ext_params *params, void **stream_ext_h)
+{
+	struct stream_ext_ctx_t *ctx = NULL;
+
+	if (WARN_ON(!params || !stream_ext_h || *stream_ext_h))
+		return -EINVAL;
+
+	ctx = kzalloc(sizeof(*ctx), GFP_KERNEL);
+	if (WARN_ON(!ctx))
+		return -ENOMEM;
+
+	ctx->drv_mode = params->drv_mode;
+	ctx->ep_id = params->ep_id;
+	ctx->edma_h = params->edma_h;
+	ctx->vmap_h = params->vmap_h;
+	ctx->pci_client_h = params->pci_client_h;
+	ctx->comm_channel_h = params->comm_channel_h;
+	strlcpy(ctx->ep_name, params->ep_name, NAME_MAX);
+	memcpy(&ctx->local_node, params->local_node, sizeof(ctx->local_node));
+	memcpy(&ctx->peer_node, params->peer_node, sizeof(ctx->peer_node));
+
+	/* copy operations.*/
+	mutex_init(&ctx->free_lock);
+	INIT_LIST_HEAD(&ctx->free_list);
+	atomic_set(&ctx->transfer_count, 0);
+	init_waitqueue_head(&ctx->transfer_waitq);
+
+	*stream_ext_h = (void *)ctx;
+
+	return 0;
+}
+
+#define MAX_TRANSFER_TIMEOUT_US	(5000000)
+void
+stream_extension_deinit(void **stream_ext_h)
+{
+	int ret = 0;
+	struct stream_ext_ctx_t *ctx = (struct stream_ext_ctx_t *)*stream_ext_h;
+	struct list_head *curr = NULL, *next = NULL;
+	struct copy_request *cr = NULL;
+
+	if (!ctx)
+		return;
+
+	/* wait for any on-going eDMA/copy(ies). */
+	ret = wait_event_interruptible_timeout
+			(ctx->transfer_waitq,
+			 !(atomic_read(&ctx->transfer_count)),
+			 msecs_to_jiffies(MAX_TRANSFER_TIMEOUT_US));
+	if (ret <= 0)
+		pr_err("eDMA transfers are still in progress\n");
+
+	mutex_lock(&ctx->free_lock);
+	list_for_each_safe(curr, next, &ctx->free_list) {
+		cr = list_entry(curr, struct copy_request, node);
+		list_del(curr);
+		free_copy_request(&cr);
+	}
+	mutex_unlock(&ctx->free_lock);
+
+	free_copy_req_params(&ctx->cr_params);
+
+	mutex_destroy(&ctx->free_lock);
+
+	kfree(ctx);
+	*stream_ext_h = NULL;
+}
+
+/*
+ * Clear edma handle associated with stream extension.
+ */
+void
+stream_extension_edma_deinit(void *stream_ext_h)
+{
+	struct stream_ext_ctx_t *ctx = (struct stream_ext_ctx_t *)stream_ext_h;
+
+	if (!ctx)
+		return;
+
+	ctx->edma_h = NULL;
+}
+
+static int
+allocate_handle(struct stream_ext_ctx_t *ctx, enum nvscic2c_pcie_obj_type type,
+		void *ioctl_args)
+{
+	int ret = 0;
+	s32 handle = -1;
+	struct stream_ext_obj *stream_obj = NULL;
+	struct vmap_obj_map_params vmap_params = {0};
+	struct vmap_obj_attributes vmap_attrib = {0};
+
+	/* one of the two below would apply.*/
+	struct nvscic2c_pcie_map_obj_args *map_args =
+		(struct nvscic2c_pcie_map_obj_args *)ioctl_args;
+	struct nvscic2c_pcie_import_obj_args *import_args =
+		(struct nvscic2c_pcie_import_obj_args *)ioctl_args;
+
+	/* create pcie virtual mapping of the obj.*/
+	switch (type) {
+	case NVSCIC2C_PCIE_OBJ_TYPE_SOURCE_MEM:
+		vmap_params.type = VMAP_OBJ_TYPE_MEM;
+		vmap_params.u.memobj.mngd = VMAP_MNGD_DEV;
+		vmap_params.u.memobj.prot = VMAP_OBJ_PROT_READ;
+		vmap_params.u.memobj.vaddr = map_args->in.vaddr;
+		vmap_params.u.memobj.size = map_args->in.size;
+		vmap_params.u.memobj.mem_type = map_args->in.mem_type;
+		break;
+	case NVSCIC2C_PCIE_OBJ_TYPE_TARGET_MEM:
+		vmap_params.type = VMAP_OBJ_TYPE_MEM;
+		vmap_params.u.memobj.mngd = VMAP_MNGD_CLIENT;
+		vmap_params.u.memobj.prot = VMAP_OBJ_PROT_WRITE;
+		vmap_params.u.memobj.vaddr = map_args->in.vaddr;
+		vmap_params.u.memobj.size = map_args->in.size;
+		vmap_params.u.memobj.mem_type = map_args->in.mem_type;
+		break;
+	case NVSCIC2C_PCIE_OBJ_TYPE_LOCAL_SYNC:
+		vmap_params.type = VMAP_OBJ_TYPE_SYNC;
+		vmap_params.u.syncobj.pin_reqd = false;
+		vmap_params.u.syncobj.vaddr = map_args->in.vaddr;
+		vmap_params.u.syncobj.size = map_args->in.size;
+		break;
+	case NVSCIC2C_PCIE_OBJ_TYPE_REMOTE_SYNC:
+		vmap_params.type = VMAP_OBJ_TYPE_SYNC;
+		vmap_params.u.syncobj.pin_reqd = true;
+		vmap_params.u.syncobj.mngd = VMAP_MNGD_CLIENT;
+		vmap_params.u.syncobj.prot = VMAP_OBJ_PROT_WRITE;
+		vmap_params.u.syncobj.vaddr = map_args->in.vaddr;
+		vmap_params.u.syncobj.size = map_args->in.size;
+		break;
+	case NVSCIC2C_PCIE_OBJ_TYPE_IMPORT:
+		vmap_params.type = VMAP_OBJ_TYPE_IMPORT;
+		vmap_params.u.importobj.export_desc = import_args->in.desc;
+		break;
+	default:
+		pr_err("Incorrect NVSCIC2C_IOCTL_MAP params\n");
+		return -EINVAL;
+	}
+
+	ret = vmap_obj_map(ctx->vmap_h, &vmap_params, &vmap_attrib);
+	if (ret) {
+		if (ret == -EAGAIN)
+			pr_info("Failed to map obj of type: (%d)\n", type);
+		else
+			pr_err("Failed to map obj of type: (%d)\n", type);
+		return ret;
+	}
+
+	/* bind the pcie virt. mapping to a streaming obj.*/
+	stream_obj = kzalloc(sizeof(*stream_obj), GFP_KERNEL);
+	if (WARN_ON(!stream_obj)) {
+		vmap_obj_unmap(ctx->vmap_h, vmap_attrib.type, vmap_attrib.id);
+		return -ENOMEM;
+	}
+
+	/*
+	 * allocate a UMD handle for this streaming_obj.
+	 * O_RDWR is required only for ImportedSyncObjs mmap() from user-space.
+	 */
+	handle = anon_inode_getfd("nvscic2c-pcie-stream-ext", &fops_default,
+				  stream_obj, (O_RDWR | O_CLOEXEC));
+	if (handle < 0) {
+		pr_err("(%s): Failed to get stream obj handle\n", ctx->ep_name);
+		vmap_obj_unmap(ctx->vmap_h, vmap_attrib.type, vmap_attrib.id);
+		kfree(stream_obj);
+		return -EFAULT;
+	}
+
+	stream_obj->vmap_h = ctx->vmap_h;
+	stream_obj->type = type;
+	stream_obj->soc_id = ctx->local_node.soc_id;
+	stream_obj->cntrlr_id = ctx->local_node.cntrlr_id;
+	stream_obj->ep_id = ctx->ep_id;
+	memcpy(&stream_obj->vmap, &vmap_attrib, sizeof(vmap_attrib));
+	kref_init(&stream_obj->refcount);
+
+	return handle;
+}
+
+static edma_xfer_status_t
+schedule_edma_xfer(void *edma_h, void *priv, u64 num_desc,
+		   struct tegra_pcie_edma_desc *desc)
+{
+	struct tegra_pcie_edma_xfer_info info = {0};
+
+	if (WARN_ON(!num_desc || !desc))
+		return -EINVAL;
+	/* x86 edma only support rx channel at EDMA_XFER_READ */
+	info.type = EDMA_XFER_READ;
+	info.channel_num = 0; // no use-case to use all WR channels yet.
+	info.desc = desc;
+	info.nents = num_desc;
+	info.complete = callback_edma_xfer;
+	info.priv = priv;
+
+	return tegra_pcie_edma_submit_xfer(edma_h, &info);
+}
+
+/* Callback with each async eDMA submit xfer.*/
+static void
+callback_edma_xfer(void *priv, edma_xfer_status_t status,
+		   struct tegra_pcie_edma_desc *desc)
+{
+	struct copy_request *cr = (struct copy_request *)priv;
+
+	mutex_lock(&cr->ctx->free_lock);
+	/* increment num_local_fences.*/
+	if (status == EDMA_XFER_SUCCESS)
+		signal_local_post_fences(cr);
+
+	/* releases the references of the cubmit-copy handles.*/
+	release_copy_request_handles(cr);
+
+	/* reclaim the copy_request for reuse.*/
+	list_add_tail(&cr->node, &cr->ctx->free_list);
+	mutex_unlock(&cr->ctx->free_lock);
+
+	atomic_dec(&cr->ctx->transfer_count);
+	wake_up_interruptible_all(&cr->ctx->transfer_waitq);
+}
+
+static int
+prepare_edma_desc(enum drv_mode_t drv_mode, struct copy_req_params *params,
+		  struct tegra_pcie_edma_desc *desc, u64 *num_desc)
+{
+	u32 i = 0;
+	int ret = 0;
+	u32 iter = 0;
+	s32 handle = -1;
+	struct file *src_filep = NULL;
+	struct file *dst_filep = NULL;
+	struct stream_ext_obj *src_stream_obj = NULL;
+	struct stream_ext_obj *dst_stream_obj = NULL;
+	struct nvscic2c_pcie_flush_range *flush_range = NULL;
+	phys_addr_t dummy_addr = 0x0;
+	dma_addr_t dst_addr = 0x0;
+	u32 sg_index = 0;
+	struct scatterlist *sg = NULL;
+
+	*num_desc = 0;
+	for (i = 0; i < params->num_flush_ranges; i++) {
+		flush_range = &params->flush_ranges[i];
+		src_filep = fget(flush_range->src_handle);
+		src_stream_obj = src_filep->private_data;
+		dst_filep = fget(flush_range->dst_handle);
+		dst_stream_obj = dst_filep->private_data;
+		dummy_addr = src_stream_obj->vmap.iova;
+
+		/* For using DMA on Tegra remotely, the IOVA of the object should be used
+		 * directly for the programming of the dst for DMA operation
+		 */
+		dst_addr = dst_stream_obj->vmap.iova + flush_range->offset;
+		/* The DMA mapping for Vidmem buffer is not contiguous and the entire sgt
+		 * needs to be used for programming of DMA. However, with sysmem only the
+		 * starting address and size is programmed as the DMA addresses are contiguous
+		 */
+		if (src_stream_obj->vmap.sgt) {
+			for_each_sg(src_stream_obj->vmap.sgt->sgl, sg,
+				    src_stream_obj->vmap.sgt->nents, sg_index) {
+				desc[iter + sg_index].src = sg_dma_address(sg) +
+							    flush_range->offset;
+				desc[iter + sg_index].sz = sg_dma_len(sg);
+				desc[iter + sg_index].dst = dst_addr;
+				dst_addr += sg_dma_len(sg);
+			}
+			iter += sg_index;
+		} else {
+			desc[iter].src = src_stream_obj->vmap.iova + flush_range->offset;
+			desc[iter].dst = dst_stream_obj->vmap.iova + flush_range->offset;
+			desc[iter].sz = flush_range->size;
+			iter++;
+		}
+		fput(src_filep);
+		fput(dst_filep);
+	}
+	for (i = 0; i < params->num_remote_post_fences; i++) {
+		handle = params->remote_post_fences[i];
+
+		desc[iter].src = dummy_addr;
+
+		dst_filep = fget(handle);
+		dst_stream_obj = dst_filep->private_data;
+		/* For using DMA on Tegra remotely, the IOVA of the object can be used
+		 * directly for the programming of the dst for DMA operation
+		 */
+		desc[iter].dst = dst_stream_obj->vmap.iova;
+		fput(dst_filep);
+
+		desc[iter].sz = 4;
+		iter++;
+	}
+	*num_desc += iter;
+	return ret;
+}
+
+/* this is post eDMA path, must be done with references still taken.*/
+static void
+signal_local_post_fences(struct copy_request *cr)
+{
+	u32 i = 0;
+	struct stream_ext_obj *stream_obj = NULL;
+
+	for (i = 0; i < cr->num_local_post_fences; i++) {
+		stream_obj = cr->local_post_fences[i];
+		pr_debug("signaling local post fence with value %llu\n",
+			 cr->local_post_fence_values[i]);
+		*stream_obj->vmap.local_fence_addr = cr->local_post_fence_values[i];
+	}
+}
+
+static int
+release_copy_request_handles(struct copy_request *cr)
+{
+	u32 i = 0;
+	struct stream_ext_obj *stream_obj = NULL;
+
+	for (i = 0; i < cr->num_handles; i++) {
+		stream_obj = cr->handles[i];
+		kref_put(&stream_obj->refcount, streamobj_free);
+	}
+
+	return 0;
+}
+
+static int
+cache_copy_request_handles(struct copy_req_params *params,
+			   struct copy_request *cr)
+{
+	u32 i = 0;
+	s32 handle = -1;
+	struct file *filep = NULL;
+	struct stream_ext_obj *stream_obj = NULL;
+
+	cr->num_handles = 0;
+	cr->num_local_post_fences = 0;
+	for (i = 0; i < params->num_local_post_fences; i++) {
+		handle = params->local_post_fences[i];
+		filep = fget(handle);
+		stream_obj = filep->private_data;
+		kref_get(&stream_obj->refcount);
+		cr->handles[cr->num_handles] = stream_obj;
+		cr->num_handles++;
+		/* collect all local post fences separately for nvhost incr.*/
+		cr->local_post_fences[cr->num_local_post_fences] = stream_obj;
+		cr->local_post_fence_values[i] = params->local_post_fence_values[i];
+		cr->num_local_post_fences++;
+		fput(filep);
+	}
+	for (i = 0; i < params->num_remote_post_fences; i++) {
+		handle = params->remote_post_fences[i];
+		filep = fget(handle);
+		stream_obj = filep->private_data;
+		kref_get(&stream_obj->refcount);
+		cr->handles[cr->num_handles] = stream_obj;
+		cr->num_handles++;
+		fput(filep);
+	}
+	for (i = 0; i < params->num_flush_ranges; i++) {
+		handle = params->flush_ranges[i].src_handle;
+		filep = fget(handle);
+		stream_obj = filep->private_data;
+		kref_get(&stream_obj->refcount);
+		cr->handles[cr->num_handles] = stream_obj;
+		cr->num_handles++;
+		fput(filep);
+
+		handle = params->flush_ranges[i].dst_handle;
+		filep = fget(handle);
+		stream_obj = filep->private_data;
+		kref_get(&stream_obj->refcount);
+		cr->handles[cr->num_handles] = stream_obj;
+		cr->num_handles++;
+		fput(filep);
+	}
+
+	return 0;
+}
+
+static int
+validate_handle(struct stream_ext_ctx_t *ctx, s32 handle,
+		enum nvscic2c_pcie_obj_type type)
+{
+	int ret = -EINVAL;
+	struct stream_ext_obj *stream_obj = NULL;
+	struct file *filep = fget(handle);
+
+	if (!filep)
+		goto exit;
+
+	if (filep->f_op != &fops_default)
+		goto err;
+
+	stream_obj = filep->private_data;
+	if (!stream_obj)
+		goto err;
+
+	if (stream_obj->marked_for_del)
+		goto err;
+
+	if (stream_obj->soc_id != ctx->local_node.soc_id ||
+	    stream_obj->cntrlr_id != ctx->local_node.cntrlr_id ||
+	    stream_obj->ep_id != ctx->ep_id)
+		goto err;
+
+	if (stream_obj->type != type)
+		goto err;
+
+	/* okay.*/
+	ret = 0;
+err:
+	fput(filep);
+exit:
+	return ret;
+}
+
+static int
+validate_import_handle(struct stream_ext_ctx_t *ctx, s32 handle,
+		       u32 import_type)
+{
+	int ret = 0;
+	struct stream_ext_obj *stream_obj = NULL;
+	struct file *filep = NULL;
+
+	ret = validate_handle(ctx, handle, NVSCIC2C_PCIE_OBJ_TYPE_IMPORT);
+	if (ret)
+		return ret;
+
+	filep = fget(handle);
+	stream_obj = filep->private_data;
+	if (stream_obj->import_type != import_type) {
+		fput(filep);
+		return -EINVAL;
+	}
+	fput(filep);
+
+	return ret;
+}
+
+static int
+validate_flush_range(struct stream_ext_ctx_t *ctx,
+		     struct nvscic2c_pcie_flush_range *flush_range)
+{
+	int ret = 0;
+	struct file *filep = NULL;
+	struct stream_ext_obj *stream_obj = NULL;
+
+	if (flush_range->size <= 0)
+		return -EINVAL;
+
+	if (flush_range->size & 0x3)
+		return -EINVAL;
+
+	if (flush_range->offset & 0x3)
+		return -EINVAL;
+
+	ret = validate_handle(ctx, flush_range->src_handle,
+			      NVSCIC2C_PCIE_OBJ_TYPE_SOURCE_MEM);
+	if (ret)
+		return ret;
+
+	ret = validate_import_handle(ctx, flush_range->dst_handle,
+				     STREAM_OBJ_TYPE_MEM);
+	if (ret)
+		return ret;
+
+	filep = fget(flush_range->src_handle);
+	stream_obj = filep->private_data;
+	if ((flush_range->offset + flush_range->size) > stream_obj->vmap.size) {
+		fput(filep);
+		return -EINVAL;
+	}
+	fput(filep);
+
+	filep = fget(flush_range->dst_handle);
+	stream_obj = filep->private_data;
+	if ((flush_range->offset + flush_range->size) > stream_obj->vmap.size) {
+		fput(filep);
+		return -EINVAL;
+	}
+	fput(filep);
+
+	return 0;
+}
+
+static int
+validate_copy_req_params(struct stream_ext_ctx_t *ctx,
+			 struct copy_req_params *params)
+{
+	u32 i = 0;
+	int ret = 0;
+
+	/* for each local post-fence.*/
+	for (i = 0; i < params->num_local_post_fences; i++) {
+		s32 handle = 0;
+
+		handle = params->local_post_fences[i];
+		ret = validate_handle(ctx, handle,
+				      NVSCIC2C_PCIE_OBJ_TYPE_LOCAL_SYNC);
+		if (ret)
+			return ret;
+	}
+
+	/* for each remote post-fence.*/
+	for (i = 0; i < params->num_remote_post_fences; i++) {
+		s32 handle = 0;
+
+		handle = params->remote_post_fences[i];
+		ret = validate_import_handle(ctx, handle, STREAM_OBJ_TYPE_SYNC);
+		if (ret)
+			return ret;
+	}
+
+	/* for each flush-range.*/
+	for (i = 0; i < params->num_flush_ranges; i++) {
+		struct nvscic2c_pcie_flush_range *flush_range = NULL;
+
+		flush_range = &params->flush_ranges[i];
+		ret = validate_flush_range(ctx, flush_range);
+		if (ret)
+			return ret;
+	}
+
+	return ret;
+}
+
+static int
+copy_args_from_user(struct stream_ext_ctx_t *ctx,
+		    struct nvscic2c_pcie_submit_copy_args *args,
+		    struct copy_req_params *params)
+{
+	int ret = 0;
+
+	if (WARN_ON(!args->num_local_post_fences ||
+		    !args->num_flush_ranges ||
+		    !args->num_remote_post_fences))
+		return -EINVAL;
+
+	if ((args->num_local_post_fences + args->num_remote_post_fences) >
+	    ctx->cr_limits.max_post_fences)
+		return -EINVAL;
+	if (args->num_flush_ranges > ctx->cr_limits.max_flush_ranges)
+		return -EINVAL;
+
+	params->num_local_post_fences = args->num_local_post_fences;
+	params->num_remote_post_fences = args->num_remote_post_fences;
+	params->num_flush_ranges = args->num_flush_ranges;
+
+	ret = copy_from_user(params->local_post_fences,
+			     (void __user *)args->local_post_fences,
+			     (params->num_local_post_fences * sizeof(s32)));
+	if (ret < 0)
+		return -EFAULT;
+
+	ret = copy_from_user(params->remote_post_fences,
+			     (void __user *)args->remote_post_fences,
+			     (params->num_remote_post_fences * sizeof(s32)));
+	if (ret < 0)
+		return -EFAULT;
+
+	ret = copy_from_user(params->local_post_fence_values,
+			     (void __user *)args->local_post_fence_values,
+			     (params->num_local_post_fences * sizeof(u64)));
+	if (ret < 0)
+		return -EFAULT;
+
+	ret = copy_from_user(params->flush_ranges,
+			     (void __user *)args->flush_ranges,
+			     (params->num_flush_ranges *
+			      sizeof(struct nvscic2c_pcie_flush_range)));
+	if (ret < 0)
+		return -EFAULT;
+
+	return 0;
+}
+
+static void
+free_copy_request(struct copy_request **copy_request)
+{
+	struct copy_request *cr = *copy_request;
+
+	if (!cr)
+		return;
+
+	kfree(cr->local_post_fences);
+	kfree(cr->local_post_fence_values);
+	kfree(cr->edma_desc);
+	kfree(cr->handles);
+	kfree(cr);
+
+	*copy_request = NULL;
+}
+
+static int
+allocate_copy_request(struct stream_ext_ctx_t *ctx,
+		      struct copy_request **copy_request)
+{
+	int ret = 0;
+	struct copy_request *cr = NULL;
+
+	/*worst-case allocation for each copy request.*/
+
+	cr = kzalloc(sizeof(*cr), GFP_KERNEL);
+	if (WARN_ON(!cr)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+	cr->ctx = ctx;
+
+	/* flush range has two handles: src, dst + all possible post_fences.*/
+	cr->handles = kzalloc((sizeof(*cr->handles) *
+			       ((2 * ctx->cr_limits.max_flush_ranges) +
+				(ctx->cr_limits.max_post_fences))),
+			      GFP_KERNEL);
+	if (WARN_ON(!cr->handles)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	/*
+	 * edma_desc shall include flush_range + worst-case all post-fences
+	 * (all max_post_fences could be remote_post_fence which need be eDMAd).
+	 */
+	cr->edma_desc = kzalloc((sizeof(*cr->edma_desc) * MAX_EDMA_DESC),
+				GFP_KERNEL);
+	if (WARN_ON(!cr->edma_desc)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	/* OR all max_post_fences could be local_post_fence. */
+	cr->local_post_fences = kzalloc((sizeof(*cr->local_post_fences) *
+					 ctx->cr_limits.max_post_fences),
+					GFP_KERNEL);
+	if (WARN_ON(!cr->local_post_fences)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	cr->local_post_fence_values =
+		kzalloc((sizeof(*cr->local_post_fence_values) *
+		ctx->cr_limits.max_post_fences),
+		GFP_KERNEL);
+	if (WARN_ON(!cr->local_post_fence_values)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	*copy_request = cr;
+	return ret;
+err:
+	free_copy_request(&cr);
+	return ret;
+}
+
+static void
+free_copy_req_params(struct copy_req_params *params)
+{
+	if (!params)
+		return;
+
+	kfree(params->flush_ranges);
+	params->flush_ranges = NULL;
+	kfree(params->local_post_fences);
+	params->local_post_fences = NULL;
+	kfree(params->remote_post_fences);
+	params->remote_post_fences = NULL;
+	kfree(params->local_post_fence_values);
+	params->local_post_fence_values = NULL;
+}
+
+static int
+allocate_copy_req_params(struct stream_ext_ctx_t *ctx,
+			 struct copy_req_params *params)
+{
+	int ret = 0;
+
+	/*worst-case allocation for each.*/
+
+	params->flush_ranges = kzalloc((sizeof(*params->flush_ranges) *
+					ctx->cr_limits.max_flush_ranges),
+				       GFP_KERNEL);
+	if (WARN_ON(!params->flush_ranges)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+	params->local_post_fences =
+				kzalloc((sizeof(*params->local_post_fences) *
+					 ctx->cr_limits.max_post_fences),
+					GFP_KERNEL);
+	if (WARN_ON(!params->local_post_fences)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+	params->local_post_fence_values =
+				kzalloc((sizeof(*params->local_post_fence_values) *
+					 ctx->cr_limits.max_post_fences),
+					GFP_KERNEL);
+	if (WARN_ON(!params->local_post_fence_values)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	params->remote_post_fences =
+				kzalloc((sizeof(*params->remote_post_fences) *
+					 ctx->cr_limits.max_post_fences),
+					GFP_KERNEL);
+	if (WARN_ON(!params->remote_post_fences)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	return ret;
+err:
+	free_copy_req_params(params);
+	return ret;
+}
diff --git a/drivers/misc/nvscic2c-pcie/stream-extensions.h b/drivers/misc/nvscic2c-pcie/stream-extensions.h
new file mode 100644
index 000000000000..4ad68737cfe6
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/stream-extensions.h
@@ -0,0 +1,53 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+/*
+ * Internal to gos-nvscic2c module. This file is not supposed to be included
+ * by any other external modules.
+ */
+#ifndef __STREAM_EXTENSIONS_H__
+#define __STREAM_EXTENSIONS_H__
+
+#include <linux/types.h>
+
+#include "common.h"
+
+/* forward declaration. */
+struct driver_ctx_t;
+
+/* params to instantiate a stream-extension instance.*/
+struct stream_ext_params {
+	struct node_info_t *local_node;
+	struct node_info_t *peer_node;
+	u32 ep_id;
+	char *ep_name;
+	enum drv_mode_t drv_mode;
+	void *pci_client_h;
+	void *comm_channel_h;
+	void *vmap_h;
+	void *edma_h;
+};
+
+int
+stream_extension_ioctl(void *stream_ext_h, unsigned int cmd, void *arg);
+
+int
+stream_extension_init(struct stream_ext_params *params, void **handle);
+
+void
+stream_extension_deinit(void **handle);
+
+void
+stream_extension_edma_deinit(void *stream_ext_h);
+#endif //__STREAM_EXTENSIONS_H__
diff --git a/drivers/misc/nvscic2c-pcie/tegra-pcie-dma-osi.h b/drivers/misc/nvscic2c-pcie/tegra-pcie-dma-osi.h
new file mode 100644
index 000000000000..5110a46f9c3a
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/tegra-pcie-dma-osi.h
@@ -0,0 +1,176 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2021-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef TEGRA_PCIE_DMA_OSI_H
+#define TEGRA_PCIE_DMA_OSI_H
+
+#define OSI_BIT(b)		(1 << (b))
+/** generates bit mask for 32 bit value */
+#define OSI_GENMASK(h, l)	(((~0U) << (l)) & (~0U >> (31U - (h))))
+
+/* Channel specific registers */
+#define DMA_CH_CONTROL1_OFF_WRCH		0x0
+#define DMA_CH_CONTROL1_OFF_WRCH_LLE		OSI_BIT(9)
+#define DMA_CH_CONTROL1_OFF_WRCH_CCS		OSI_BIT(8)
+#define DMA_CH_CONTROL1_OFF_WRCH_CS_MASK	OSI_GENMASK(6U, 5U)
+#define DMA_CH_CONTROL1_OFF_WRCH_CS_SHIFT	5
+#define DMA_CH_CONTROL1_OFF_WRCH_RIE		OSI_BIT(4)
+#define DMA_CH_CONTROL1_OFF_WRCH_LIE		OSI_BIT(3)
+#define DMA_CH_CONTROL1_OFF_WRCH_LLP		OSI_BIT(2)
+#define DMA_CH_CONTROL1_OFF_WRCH_CB		OSI_BIT(0)
+
+#define DMA_WRITE_ENGINE_EN_OFF		0xC
+#define WRITE_ENABLE			OSI_BIT(0)
+#define WRITE_DISABLE			0x0
+
+#define DMA_WRITE_DOORBELL_OFF		0x10
+#define DMA_WRITE_DOORBELL_OFF_WR_STOP	OSI_BIT(31)
+
+#define DMA_READ_ENGINE_EN_OFF		0x2C
+#define READ_ENABLE			OSI_BIT(0)
+#define READ_DISABLE			0x0
+
+#define DMA_READ_DOORBELL_OFF		0x30
+#define DMA_READ_DOORBELL_OFF_RD_STOP	OSI_BIT(31)
+
+#define DMA_TRANSFER_SIZE_OFF_WRCH		0x8
+#define DMA_SAR_LOW_OFF_WRCH			0xC
+#define DMA_SAR_HIGH_OFF_WRCH			0x10
+#define DMA_DAR_LOW_OFF_WRCH			0x14
+#define DMA_DAR_HIGH_OFF_WRCH			0x18
+#define DMA_LLP_LOW_OFF_WRCH			0x1C
+#define DMA_LLP_HIGH_OFF_WRCH			0x20
+
+#define DMA_WRITE_DONE_IMWR_LOW_OFF		0x60
+#define DMA_WRITE_DONE_IMWR_HIGH_OFF		0x64
+#define DMA_WRITE_ABORT_IMWR_LOW_OFF		0x68
+#define DMA_WRITE_ABORT_IMWR_HIGH_OFF		0x6c
+#define DMA_WRITE_CH01_IMWR_DATA_OFF		0x70
+#define DMA_WRITE_CH23_IMWR_DATA_OFF		0x74
+
+#define DMA_WRITE_LINKED_LIST_ERR_EN_OFF	0x90
+#define DMA_READ_LINKED_LIST_ERR_EN_OFF		0xC4
+
+#define DMA_READ_DONE_IMWR_LOW_OFF		0xcc
+#define DMA_READ_DONE_IMWR_HIGH_OFF		0xd0
+#define DMA_READ_ABORT_IMWR_LOW_OFF		0xd4
+#define DMA_READ_ABORT_IMWR_HIGH_OFF		0xd8
+#define DMA_READ_CH01_IMWR_DATA_OFF		0xdc
+
+#define DMA_CH_CONTROL1_OFF_RDCH		0x100
+#define DMA_CH_CONTROL1_OFF_RDCH_LLE		OSI_BIT(9)
+#define DMA_CH_CONTROL1_OFF_RDCH_CCS		OSI_BIT(8)
+#define DMA_CH_CONTROL1_OFF_RDCH_CS_MASK	OSI_GENMASK(6U, 5U)
+#define DMA_CH_CONTROL1_OFF_RDCH_CS_SHIFT	5
+#define DMA_CH_CONTROL1_OFF_RDCH_RIE		OSI_BIT(4)
+#define DMA_CH_CONTROL1_OFF_RDCH_LIE		OSI_BIT(3)
+#define DMA_CH_CONTROL1_OFF_RDCH_LLP		OSI_BIT(2)
+#define DMA_CH_CONTROL1_OFF_RDCH_CB		OSI_BIT(0)
+
+#define DMA_TRANSFER_SIZE_OFF_RDCH		0x108
+#define DMA_SAR_LOW_OFF_RDCH			0x10c
+#define DMA_SAR_HIGH_OFF_RDCH			0x110
+#define DMA_DAR_LOW_OFF_RDCH			0x114
+#define DMA_DAR_HIGH_OFF_RDCH			0x118
+#define DMA_LLP_LOW_OFF_RDCH			0x11c
+#define DMA_LLP_HIGH_OFF_RDCH			0x120
+
+#define DMA_WRITE_INT_STATUS_OFF	0x4C
+#define DMA_WRITE_INT_MASK_OFF		0x54
+#define DMA_WRITE_INT_CLEAR_OFF		0x58
+
+#define DMA_READ_INT_STATUS_OFF		0xA0
+#define DMA_READ_INT_MASK_OFF		0xA8
+#define DMA_READ_INT_CLEAR_OFF		0xAC
+
+struct edma_ctrl {
+	uint32_t cb:1;
+	uint32_t tcb:1;
+	uint32_t llp:1;
+	uint32_t lie:1;
+	uint32_t rie:1;
+};
+
+struct edma_hw_desc {
+	volatile union {
+		struct edma_ctrl ctrl_e;
+		uint32_t ctrl_d;
+	} ctrl_reg;
+	uint32_t size;
+	uint32_t sar_low;
+	uint32_t sar_high;
+	uint32_t dar_low;
+	uint32_t dar_high;
+};
+
+struct edma_hw_desc_llp {
+	volatile union {
+		struct edma_ctrl ctrl_e;
+		uint32_t ctrl_d;
+	} ctrl_reg;
+	uint32_t size;
+	uint32_t sar_low;
+	uint32_t sar_high;
+};
+
+struct edma_dblock {
+	struct edma_hw_desc desc[2];
+	struct edma_hw_desc_llp llp;
+};
+
+static inline unsigned int osi_readl(void *addr)
+{
+	return *(volatile unsigned int *)addr;
+}
+
+
+static inline unsigned int dma_common_rd(void *p, unsigned int offset)
+{
+	unsigned char *addr;
+
+	addr = (unsigned char *)p + offset;
+	return osi_readl((void *)addr);
+}
+
+static inline void osi_writel(unsigned int val, void *addr)
+{
+	*(volatile unsigned int *)addr = val;
+}
+
+static inline void dma_common_wr(void *p, unsigned int val, unsigned int offset)
+{
+	unsigned char *addr;
+
+	addr = (unsigned char *)p + offset;
+	osi_writel(val, (void *)addr);
+}
+
+static inline void dma_channel_wr(void *p, unsigned char c, unsigned int val,
+				  u32 offset)
+{
+	unsigned char *addr;
+
+	addr = (unsigned char *)p + offset + (0x200 * (c + 1));
+	osi_writel(val, (void *)addr);
+}
+
+static inline unsigned int dma_channel_rd(void *p, unsigned char c, u32 offset)
+{
+	unsigned char *addr;
+
+	addr = (unsigned char *)p + offset + (0x200 * (c + 1));
+	return osi_readl((void *)addr);
+}
+
+#endif // TEGRA_PCIE_DMA_OSI_H
diff --git a/drivers/misc/nvscic2c-pcie/tegra-pcie-edma.c b/drivers/misc/nvscic2c-pcie/tegra-pcie-edma.c
new file mode 100644
index 000000000000..ce4d84b2fe77
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/tegra-pcie-edma.c
@@ -0,0 +1,742 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2021-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/delay.h>
+#include <linux/dma-iommu.h>
+#include <linux/dma-mapping.h>
+#include <linux/io.h>
+#include <linux/iopoll.h>
+#include <linux/of_platform.h>
+#include <linux/slab.h>
+#include <linux/interrupt.h>
+#include "tegra-pcie-edma.h"
+#include "tegra-pcie-dma-osi.h"
+
+/** Default number of descriptors used */
+#define NUM_EDMA_DESC	4096
+
+/* DMA base offset starts at 0x20000 from ATU_DMA base */
+#define DMA_OFFSET	0x20000
+
+/** Calculates timeout based on size.
+ *  Time in nano sec = size in bytes / (1000000 * 2).
+ *    2Gbps is max speed for Gen 1 with 2.5GT/s at 8/10 encoding.
+ *  Convert to milli seconds and add 1sec timeout
+ */
+#define GET_SYNC_TIMEOUT(s)	((((s) * 8UL) / 2000000) + 1000)
+
+#define INCR_DESC(idx, i) ((idx) = ((idx) + (i)) % (ch->desc_sz))
+
+struct edma_chan {
+	void *desc;
+	struct tegra_pcie_edma_xfer_info *ring;
+	dma_addr_t dma_iova;
+	uint32_t desc_sz;
+	/** Index from where cleanup needs to be done */
+	volatile uint32_t r_idx;
+	/** Index from where descriptor update is needed */
+	volatile uint32_t w_idx;
+	struct mutex lock;
+	wait_queue_head_t wq;
+	edma_chan_type_t type;
+	u64 wcount;
+	u64 rcount;
+	unsigned long busy;
+	bool pcs;
+	bool db_pos;
+	/** This filed is updated to abort of de-init to stop further xfer submits */
+	edma_xfer_status_t st;
+};
+
+struct edma_prv {
+	void __iomem *edma_desc;
+	u32 edma_desc_size;
+	int irq;
+	char *irq_name;
+	bool is_remote_dma;
+	uint16_t msi_data;
+	uint64_t msi_addr;
+	/** EDMA base address */
+	void *edma_base;
+	/** EDMA base address size */
+	uint32_t edma_base_size;
+	struct device *dev;
+	struct edma_chan tx[DMA_WR_CHNL_NUM];
+	struct edma_chan rx[DMA_RD_CHNL_NUM];
+	uint32_t ch_init;
+};
+
+/** TODO: Define osi_ll_init strcuture and make this as OSI */
+static inline void edma_ll_ch_init(void *edma_base, uint32_t ch,
+					 dma_addr_t ll_phy_addr, bool rw_type,
+					 bool is_remote_dma)
+{
+	uint32_t int_mask_val = OSI_BIT(ch);
+	uint32_t val;
+	/** configure write by default and overwrite if read */
+	uint32_t int_mask = DMA_WRITE_INT_MASK_OFF;
+	uint32_t ctrl1_offset = DMA_CH_CONTROL1_OFF_WRCH;
+	uint32_t low_offset = DMA_LLP_LOW_OFF_WRCH;
+	uint32_t high_offset = DMA_LLP_HIGH_OFF_WRCH;
+	uint32_t lle_ccs = DMA_CH_CONTROL1_OFF_WRCH_LIE | DMA_CH_CONTROL1_OFF_WRCH_LLE |
+			   DMA_CH_CONTROL1_OFF_WRCH_CCS;
+	uint32_t rie = DMA_CH_CONTROL1_OFF_WRCH_RIE;
+	uint32_t err_off = DMA_WRITE_LINKED_LIST_ERR_EN_OFF;
+	uint32_t err_val = 0;
+
+	if (!rw_type) {
+		int_mask = DMA_READ_INT_MASK_OFF;
+		low_offset = DMA_LLP_LOW_OFF_RDCH;
+		high_offset = DMA_LLP_HIGH_OFF_RDCH;
+		ctrl1_offset = DMA_CH_CONTROL1_OFF_RDCH;
+		lle_ccs = DMA_CH_CONTROL1_OFF_RDCH_LIE | DMA_CH_CONTROL1_OFF_RDCH_LLE |
+			   DMA_CH_CONTROL1_OFF_RDCH_CCS;
+		rie = DMA_CH_CONTROL1_OFF_RDCH_RIE;
+		err_off = DMA_READ_LINKED_LIST_ERR_EN_OFF;
+	}
+	/* Enable LIE or RIE for all write channels */
+	val = dma_common_rd(edma_base, int_mask);
+	err_val = dma_common_rd(edma_base, err_off);
+	if (!is_remote_dma) {
+		val &= ~int_mask_val;
+		val &= ~(int_mask_val << 16);
+		err_val |= OSI_BIT((16 + ch));
+	} else {
+		val |= int_mask_val;
+		val |= (int_mask_val << 16);
+		err_val |= OSI_BIT(ch);
+	}
+	dma_common_wr(edma_base, val, int_mask);
+	dma_common_wr(edma_base, err_val, err_off);
+
+	val = lle_ccs;
+	/* Enable RIE for remote DMA */
+	val |= (is_remote_dma ? rie : 0);
+	dma_channel_wr(edma_base, ch, val, ctrl1_offset);
+	dma_channel_wr(edma_base, ch, lower_32_bits(ll_phy_addr), low_offset);
+	dma_channel_wr(edma_base, ch, upper_32_bits(ll_phy_addr), high_offset);
+}
+
+static inline void edma_hw_init(void *cookie, bool rw_type)
+{
+	struct edma_prv *prv = (struct edma_prv *)cookie;
+	uint32_t msi_data;
+	u32 eng_off[2] = {DMA_WRITE_ENGINE_EN_OFF, DMA_READ_ENGINE_EN_OFF};
+
+	if (prv->ch_init & OSI_BIT(rw_type))
+		dma_common_wr(prv->edma_base, WRITE_ENABLE, eng_off[rw_type]);
+
+	/* Program MSI addr & data for remote DMA use case */
+	if (prv->is_remote_dma) {
+		msi_data = prv->msi_data;
+		msi_data |= (msi_data << 16);
+
+		dma_common_wr(prv->edma_base, lower_32_bits(prv->msi_addr),
+			      DMA_WRITE_DONE_IMWR_LOW_OFF);
+		dma_common_wr(prv->edma_base, upper_32_bits(prv->msi_addr),
+			      DMA_WRITE_DONE_IMWR_HIGH_OFF);
+		dma_common_wr(prv->edma_base, lower_32_bits(prv->msi_addr),
+			      DMA_WRITE_ABORT_IMWR_LOW_OFF);
+		dma_common_wr(prv->edma_base, upper_32_bits(prv->msi_addr),
+			      DMA_WRITE_ABORT_IMWR_HIGH_OFF);
+		dma_common_wr(prv->edma_base, msi_data,
+			      DMA_WRITE_CH01_IMWR_DATA_OFF);
+		dma_common_wr(prv->edma_base, msi_data,
+			      DMA_WRITE_CH23_IMWR_DATA_OFF);
+
+		dma_common_wr(prv->edma_base, lower_32_bits(prv->msi_addr),
+			      DMA_READ_DONE_IMWR_LOW_OFF);
+		dma_common_wr(prv->edma_base, upper_32_bits(prv->msi_addr),
+			      DMA_READ_DONE_IMWR_HIGH_OFF);
+		dma_common_wr(prv->edma_base, lower_32_bits(prv->msi_addr),
+			      DMA_READ_ABORT_IMWR_LOW_OFF);
+		dma_common_wr(prv->edma_base, upper_32_bits(prv->msi_addr),
+			      DMA_READ_ABORT_IMWR_HIGH_OFF);
+		dma_common_wr(prv->edma_base, msi_data,
+			      DMA_READ_CH01_IMWR_DATA_OFF);
+	}
+}
+
+static inline int edma_ch_init(struct edma_chan *ch)
+{
+	struct edma_dblock *db;
+	dma_addr_t addr;
+	uint32_t j;
+
+	memset(ch->desc, 0, (sizeof(struct edma_dblock)) *
+			    ((ch->desc_sz / 2) + 1));
+
+	db = (struct edma_dblock *)ch->desc + ((ch->desc_sz / 2) - 1);
+	db->llp.sar_low = lower_32_bits(ch->dma_iova);
+	db->llp.sar_high = upper_32_bits(ch->dma_iova);
+	db->llp.ctrl_reg.ctrl_e.llp = 1;
+	db->llp.ctrl_reg.ctrl_e.tcb = 1;
+	for (j = 0; j < (ch->desc_sz / 2 - 1); j++) {
+		db = (struct edma_dblock *)ch->desc + j;
+		addr = ch->dma_iova + (sizeof(struct edma_dblock) * (j + 1));
+		db->llp.sar_low = lower_32_bits(addr);
+		db->llp.sar_high = upper_32_bits(addr);
+		db->llp.ctrl_reg.ctrl_e.llp = 1;
+	}
+	ch->wcount = 0;
+	ch->rcount = 0;
+	ch->w_idx  = 0;
+	ch->r_idx  = 0;
+	ch->pcs = 1;
+	ch->st = EDMA_XFER_SUCCESS;
+
+	if (!ch->ring) {
+		ch->ring = kcalloc(ch->desc_sz, sizeof(*ch->ring), GFP_KERNEL);
+		if (!ch->ring)
+			return -ENOMEM;
+	}
+
+	return 0;
+}
+
+static inline void edma_hw_deinit(void *cookie, bool rw_type)
+{
+	struct edma_prv *prv = (struct edma_prv *)cookie;
+	int i;
+	u32 eng_off[2] = {DMA_WRITE_ENGINE_EN_OFF, DMA_READ_ENGINE_EN_OFF};
+	u32 ctrl_off[2] = {DMA_CH_CONTROL1_OFF_WRCH, DMA_CH_CONTROL1_OFF_RDCH};
+	u32 mode_cnt[2] = {DMA_WR_CHNL_NUM, DMA_RD_CHNL_NUM};
+
+	if (prv->ch_init & OSI_BIT(rw_type)) {
+		dma_common_wr(prv->edma_base, 0, eng_off[rw_type]);
+		for (i = 0; i < mode_cnt[rw_type]; i++)
+			dma_channel_wr(prv->edma_base, i, 0, ctrl_off[rw_type]);
+	}
+}
+
+/** From OSI */
+static inline u32 get_dma_idx_from_llp(struct edma_prv *prv, u32 chan, struct edma_chan *ch,
+				       u32 type)
+{
+	u64 cur_iova;
+	u64 high_iova, tmp_iova;
+	u32 cur_idx;
+	u32 llp_low_off[2] = {DMA_LLP_LOW_OFF_WRCH, DMA_LLP_LOW_OFF_RDCH};
+	u32 llp_high_off[2] = {DMA_LLP_HIGH_OFF_WRCH, DMA_LLP_HIGH_OFF_RDCH};
+	u64 block_idx, idx_in_block;
+
+	/*
+	 * Read current element address in DMA_LLP register which pending
+	 * DMA request and validate for spill over.
+	 */
+	high_iova = dma_channel_rd(prv->edma_base, chan, llp_high_off[type]);
+	cur_iova = (high_iova << 32);
+	cur_iova |= dma_channel_rd(prv->edma_base, chan, llp_low_off[type]);
+	tmp_iova = dma_channel_rd(prv->edma_base, chan, llp_high_off[type]);
+	if (tmp_iova > high_iova) {
+		/* Take latest reading of low offset and use it with new high offset */
+		cur_iova = dma_channel_rd(prv->edma_base, chan, llp_low_off[type]);
+		cur_iova |= (tmp_iova << 32);
+	}
+	/* Compute DMA desc index */
+	block_idx = ((cur_iova - ch->dma_iova) / sizeof(struct edma_dblock));
+	idx_in_block = (cur_iova & (sizeof(struct edma_dblock) - 1)) /
+		       sizeof(struct edma_hw_desc);
+
+	cur_idx = (block_idx * 2) + idx_in_block;
+
+	return cur_idx % (ch->desc_sz);
+}
+
+static inline void process_r_idx(struct edma_chan *ch, edma_xfer_status_t st, u32 idx)
+{
+	u32 count = 0;
+	struct edma_hw_desc *dma_ll_virt;
+	struct edma_dblock *db;
+	struct tegra_pcie_edma_xfer_info *ring;
+
+	while ((ch->r_idx != idx) && (count < ch->desc_sz)) {
+		count++;
+		ring = &ch->ring[ch->r_idx];
+		db = (struct edma_dblock *)ch->desc + ch->r_idx / 2;
+		dma_ll_virt = &db->desc[ch->r_idx % 2];
+		INCR_DESC(ch->r_idx, 1);
+		ch->rcount++;
+		if (ch->type == EDMA_CHAN_XFER_ASYNC && ring->complete) {
+			ring->complete(ring->priv, st, NULL);
+			/* Clear ring callback and lie,rie */
+			ring->complete = NULL;
+			dma_ll_virt->ctrl_reg.ctrl_e.lie = 0;
+			dma_ll_virt->ctrl_reg.ctrl_e.rie = 0;
+		}
+	}
+}
+
+static inline void process_ch_irq(struct edma_prv *prv, u32 chan, struct edma_chan *ch,
+				  u32 type)
+{
+	u32 idx;
+
+	idx = get_dma_idx_from_llp(prv, chan, ch, type);
+
+	if (ch->type == EDMA_CHAN_XFER_SYNC) {
+		if (ch->busy & OSI_BIT(chan)) {
+			ch->busy &= ~(OSI_BIT(chan));
+			wake_up(&ch->wq);
+		}
+	}
+
+	if (ch->st == EDMA_XFER_ABORT) {
+		dev_info(prv->dev, "Abort: ch %d at r_idx %d->idx %d, w_idx is %d\n", chan,
+			 ch->r_idx, idx, ch->w_idx);
+		if (ch->r_idx == idx)
+			goto process_abort;
+	}
+
+	process_r_idx(ch, EDMA_XFER_SUCCESS, idx);
+
+process_abort:
+	if (ch->st == EDMA_XFER_ABORT)
+		process_r_idx(ch, EDMA_XFER_ABORT, ch->w_idx);
+}
+
+static irqreturn_t edma_irq_handler(int irq, void *cookie)
+{
+	struct edma_prv *prv = (struct edma_prv *)cookie;
+	int bit = 0;
+	u32 val, i = 0;
+	struct edma_chan *chan[2] = {&prv->tx[0], &prv->rx[0]};
+	struct edma_chan *ch;
+	u32 int_status_off[2] = {DMA_WRITE_INT_STATUS_OFF, DMA_READ_INT_STATUS_OFF};
+	u32 int_clear_off[2] = {DMA_WRITE_INT_CLEAR_OFF, DMA_READ_INT_CLEAR_OFF};
+	u32 mode_cnt[2] = {DMA_WR_CHNL_NUM, DMA_RD_CHNL_NUM};
+
+	for (i = 0; i < 2; i++) {
+		if (!(prv->ch_init & BIT(i)))
+			continue;
+
+		val = dma_common_rd(prv->edma_base, int_status_off[i]);
+		if ((val & OSI_GENMASK(31, 16)) != 0U) {
+			/**
+			 * If ABORT, immediately update state for all channels as aborted.
+			 * This setting stop further SW queuing
+			 */
+			dev_info(prv->dev, "Abort int status 0x%x", val);
+			for (bit = 0; bit < mode_cnt[i]; bit++) {
+				ch = chan[i] + bit;
+				ch->st = EDMA_XFER_ABORT;
+			}
+
+			edma_hw_deinit(prv, !!i);
+
+			/** Perform abort handling */
+			for (bit = 0; bit < mode_cnt[i]; bit++) {
+				ch = chan[i] + bit;
+				if (!ch->ring)
+					continue;
+
+				/* Clear ABORT and DONE interrupt, as abort handles both */
+				dma_common_wr(prv->edma_base, OSI_BIT(16 + bit) | OSI_BIT(bit),
+					      int_clear_off[i]);
+				/** wait until exisitng xfer submit completed */
+				mutex_lock(&ch->lock);
+				mutex_unlock(&ch->lock);
+
+				process_ch_irq(prv, bit, ch, i);
+
+				edma_ch_init(ch);
+				edma_ll_ch_init(prv->edma_base, bit, ch->dma_iova, (i == 0),
+						prv->is_remote_dma);
+			}
+
+			edma_hw_init(prv, !!i);
+		} else {
+			for (bit = 0; bit < mode_cnt[i]; bit++) {
+				ch = chan[i] + bit;
+				if (OSI_BIT(bit) & val) {
+					dma_common_wr(prv->edma_base, OSI_BIT(bit),
+						      int_clear_off[i]);
+					process_ch_irq(prv, bit, ch, i);
+				}
+			}
+		}
+	}
+
+	return IRQ_HANDLED;
+}
+
+void *tegra_pcie_edma_initialize(struct tegra_pcie_edma_init_info *info)
+{
+	struct edma_prv *prv;
+	struct resource *dma_res;
+	int32_t ret, i, j;
+	struct edma_chan *ch = NULL;
+	struct edma_chan *chan[2];
+	u32 mode_cnt[2] = {DMA_WR_CHNL_NUM, DMA_RD_CHNL_NUM};
+	struct tegra_pcie_edma_chans_info *chan_info[2];
+	struct tegra_pcie_edma_chans_info *ch_info;
+	struct platform_device *pdev;
+
+	prv = kzalloc(sizeof(*prv), GFP_KERNEL);
+	if (!prv) {
+		pr_err("Failed to allocate memory for edma_prv\n");
+		return NULL;
+	}
+
+	chan[0] = &prv->tx[0];
+	chan[1] = &prv->rx[0];
+	chan_info[0] = &info->tx[0];
+	chan_info[1] = &info->rx[0];
+
+	if (info->edma_remote != NULL) {
+		if (!info->edma_remote->dev) {
+			pr_err("%s: dev pointer is NULL\n", __func__);
+			goto free_priv;
+		}
+
+		prv->dev = info->edma_remote->dev;
+		prv->irq = info->edma_remote->msi_irq;
+		prv->msi_data = info->edma_remote->msi_data;
+		prv->msi_addr = info->edma_remote->msi_addr;
+		prv->is_remote_dma = true;
+
+		prv->edma_base = devm_ioremap(prv->dev, info->edma_remote->dma_phy_base,
+					      info->edma_remote->dma_size);
+		if (IS_ERR(prv->edma_base)) {
+			dev_err(prv->dev, "dma region map failed.\n");
+			goto free_priv;
+		}
+
+		for (j = 0; j < 2; j++) {
+			for (i = 0; i < mode_cnt[j]; i++) {
+				ch_info = chan_info[j] + i;
+				ch = chan[j] + i;
+
+				if (ch_info->num_descriptors == 0)
+					continue;
+
+				ch->type = ch_info->ch_type;
+				ch->desc_sz = ch_info->num_descriptors;
+				ch->dma_iova = ch_info->desc_iova;
+
+				ch->desc = devm_ioremap_wc(prv->dev, ch_info->desc_phy_base,
+							   (sizeof(struct edma_dblock)) *
+							   ((ch->desc_sz / 2) + 1));
+
+				if (!ch->desc) {
+					dev_err(prv->dev, "desc region map failed, phy: 0x%llx\n",
+						ch_info->desc_phy_base);
+					goto dma_iounmap;
+				}
+
+				prv->ch_init |= OSI_BIT(j);
+
+				if (edma_ch_init(ch) < 0)
+					goto desc_iounmap;
+
+				edma_ll_ch_init(prv->edma_base, i, ch->dma_iova, (j == 0),
+						prv->is_remote_dma);
+			}
+		}
+	} else if (info->np != NULL) {
+		prv->is_remote_dma = false;
+
+		pdev = of_find_device_by_node(info->np);
+		if (!pdev) {
+			pr_err("Unable to retrieve pdev node\n");
+			goto free_priv;
+		}
+		prv->dev = &pdev->dev;
+
+		dma_res = platform_get_resource_byname(pdev, IORESOURCE_MEM,
+						       "atu_dma");
+		if (!dma_res) {
+			dev_err(prv->dev, "missing atu_dma resource in DT\n");
+			goto put_dev;
+		}
+
+		prv->edma_base = devm_ioremap(prv->dev, dma_res->start + DMA_OFFSET,
+					      resource_size(dma_res) - DMA_OFFSET);
+		if (IS_ERR(prv->edma_base)) {
+			dev_err(prv->dev, "dma region map failed.\n");
+			goto put_dev;
+		}
+
+		for (j = 0; j < 2; j++) {
+			for (i = 0; i < mode_cnt[j]; i++) {
+				ch = chan[j] + i;
+				ch_info = chan_info[j] + i;
+
+				if (ch_info->num_descriptors == 0)
+					continue;
+
+				ch->type = ch_info->ch_type;
+				ch->desc_sz = ch_info->num_descriptors;
+				ch->desc = dma_alloc_coherent(prv->dev,
+							      (sizeof(struct edma_dblock)) *
+							      ((ch->desc_sz / 2) + 1),
+							      &ch->dma_iova, GFP_KERNEL);
+				if (!ch->desc) {
+					dev_err(prv->dev, "Cannot allocate required descriptos(%d) of size (%lu) for channel:%d type: %d\n",
+						ch->desc_sz,
+						(sizeof(struct edma_hw_desc) *
+						ch->desc_sz), i, j);
+					goto dma_iounmap;
+				}
+
+				prv->ch_init |= OSI_BIT(j);
+
+				if (edma_ch_init(ch) < 0)
+					goto free_dma_desc;
+
+				edma_ll_ch_init(prv->edma_base, i, ch->dma_iova, (j == 0),
+						prv->is_remote_dma);
+			}
+		}
+
+		prv->irq = platform_get_irq_byname(pdev, "intr");
+		if (!prv->irq) {
+			dev_err(prv->dev, "failed to get intr interrupt\n");
+			goto free_ring;
+		};
+	} else {
+		pr_err("Neither device node nor edma remote available");
+		goto free_priv;
+	}
+
+	prv->irq_name = kasprintf(GFP_KERNEL, "%s_edma_lib", dev_name(prv->dev));
+	if (!prv->irq_name)
+		goto free_ring;
+
+	ret = request_threaded_irq(prv->irq, NULL, edma_irq_handler,
+				   IRQF_SHARED | IRQF_ONESHOT,
+				   prv->irq_name, prv);
+	if (ret < 0) {
+		dev_err(prv->dev, "failed to request \"intr\" irq\n");
+		goto free_irq_name;
+	}
+
+	for (i = 0; i < DMA_WR_CHNL_NUM; i++) {
+		mutex_init(&prv->tx[i].lock);
+		init_waitqueue_head(&prv->tx[i].wq);
+	}
+
+	for (i = 0; i < DMA_RD_CHNL_NUM; i++) {
+		mutex_init(&prv->rx[i].lock);
+		init_waitqueue_head(&prv->rx[i].wq);
+	}
+
+	edma_hw_init(prv, false);
+	edma_hw_init(prv, true);
+	dev_info(prv->dev, "%s: success", __func__);
+
+	return prv;
+
+free_irq_name:
+	kfree(prv->irq_name);
+free_ring:
+	for (j = 0; j < 2; j++) {
+		for (i = 0; i < mode_cnt[j]; i++) {
+			ch = chan[j] + i;
+			kfree(ch->ring);
+		}
+	}
+free_dma_desc:
+	for (j = 0; j < 2; j++) {
+		for (i = 0; i < mode_cnt[j]; i++) {
+			ch = chan[j] + i;
+			if (ch->desc)
+				dma_free_coherent(prv->dev,
+						  (sizeof(struct edma_hw_desc) * ch->desc_sz),
+						  ch->desc, ch->dma_iova);
+		}
+	}
+desc_iounmap:
+	if (prv->is_remote_dma)
+		devm_iounmap(prv->dev, ch->desc);
+dma_iounmap:
+	devm_iounmap(prv->dev, prv->edma_base);
+put_dev:
+	if (!prv->is_remote_dma)
+		put_device(prv->dev);
+free_priv:
+	kfree(prv);
+	return NULL;
+}
+
+edma_xfer_status_t tegra_pcie_edma_submit_xfer(void *cookie,
+						struct tegra_pcie_edma_xfer_info *tx_info)
+{
+	struct edma_prv *prv = (struct edma_prv *)cookie;
+	struct edma_chan *ch;
+	struct edma_hw_desc *dma_ll_virt;
+	struct edma_dblock *db;
+	int i, ret;
+	u64 total_sz = 0;
+	edma_xfer_status_t st = EDMA_XFER_SUCCESS;
+	u32 avail;
+	struct tegra_pcie_edma_xfer_info *ring;
+	u32 int_status_off[2] = {DMA_WRITE_INT_STATUS_OFF, DMA_READ_INT_STATUS_OFF};
+	u32 doorbell_off[2] = {DMA_WRITE_DOORBELL_OFF, DMA_READ_DOORBELL_OFF};
+	u32 mode_cnt[2] = {DMA_WR_CHNL_NUM, DMA_RD_CHNL_NUM};
+
+	if (!prv || !tx_info || tx_info->nents == 0 || !tx_info->desc ||
+	    tx_info->channel_num >= mode_cnt[tx_info->type])
+		return EDMA_XFER_FAIL_INVAL_INPUTS;
+
+	ch = (tx_info->type == EDMA_XFER_WRITE) ? &prv->tx[tx_info->channel_num] :
+						  &prv->rx[tx_info->channel_num];
+
+	if ((tx_info->complete == NULL) && (ch->type == EDMA_CHAN_XFER_ASYNC))
+		return EDMA_XFER_FAIL_INVAL_INPUTS;
+
+	/* Get hold of the hardware - locking */
+	mutex_lock(&ch->lock);
+
+	if (ch->st != EDMA_XFER_SUCCESS) {
+		st = ch->st;
+		goto unlock;
+	}
+
+	avail = (ch->r_idx - ch->w_idx - 1U) & (ch->desc_sz - 1U);
+	if (tx_info->nents > avail) {
+		dev_err(prv->dev, "Descriptors full. w_idx %d. r_idx %d, avail %d, req %d\n",
+			ch->w_idx, ch->r_idx, avail, tx_info->nents);
+		st = EDMA_XFER_FAIL_NOMEM;
+		goto unlock;
+	}
+
+	ch->busy |= 1 << tx_info->channel_num;
+
+	dev_dbg(prv->dev, "xmit for %d nents at %d widx and %d ridx\n",
+		tx_info->nents, ch->w_idx, ch->r_idx);
+	db = (struct edma_dblock *)ch->desc + (ch->w_idx/2);
+	for (i = 0; i < tx_info->nents; i++) {
+		dma_ll_virt = &db->desc[ch->db_pos];
+		dma_ll_virt->size = tx_info->desc[i].sz;
+		/* calculate number of packets and add those many headers */
+		total_sz += ((tx_info->desc[i].sz / ch->desc_sz) + 1) * 30;
+		total_sz += tx_info->desc[i].sz;
+		dma_ll_virt->sar_low = lower_32_bits(tx_info->desc[i].src);
+		dma_ll_virt->sar_high = upper_32_bits(tx_info->desc[i].src);
+		dma_ll_virt->dar_low = lower_32_bits(tx_info->desc[i].dst);
+		dma_ll_virt->dar_high = upper_32_bits(tx_info->desc[i].dst);
+		dma_ll_virt->ctrl_reg.ctrl_e.cb = ch->pcs;
+		ch->db_pos = !ch->db_pos;
+		avail = ch->w_idx;
+		ch->w_idx++;
+		if (!ch->db_pos) {
+			ch->wcount = 0;
+			db->llp.ctrl_reg.ctrl_e.cb = ch->pcs;
+			if (ch->w_idx == ch->desc_sz) {
+				ch->pcs = !ch->pcs;
+				db->llp.ctrl_reg.ctrl_e.cb = ch->pcs;
+				dev_dbg(prv->dev, "Toggled pcs at w_idx %d\n", ch->w_idx);
+				ch->w_idx = 0;
+			}
+			db = (struct edma_dblock *)ch->desc + (ch->w_idx/2);
+		}
+	}
+
+	ring = &ch->ring[avail];
+	ring->complete = tx_info->complete;
+	ring->priv = tx_info->priv;
+	ring->nents = tx_info->nents;
+	ring->desc = tx_info->desc;
+
+	/* Set LIE or RIE in last element */
+	dma_ll_virt->ctrl_reg.ctrl_e.lie = 1;
+	if (prv->is_remote_dma)
+		dma_ll_virt->ctrl_reg.ctrl_e.rie = 1;
+
+	dma_common_wr(prv->edma_base, tx_info->channel_num, doorbell_off[tx_info->type]);
+
+	if (ch->type == EDMA_CHAN_XFER_SYNC) {
+		ret = wait_event_timeout(ch->wq,
+				!(ch->busy & (1 << tx_info->channel_num)),
+				msecs_to_jiffies((uint32_t)(GET_SYNC_TIMEOUT(total_sz))));
+		if (ret == 0) {
+			dev_err(prv->dev, "%s: timeout at %d ch, w_idx(%d), r_idx(%d)\n",
+				__func__, tx_info->channel_num, ch->w_idx,
+				ch->r_idx);
+			dev_err(prv->dev, "%s: int status 0x%x", __func__,
+				dma_common_rd(prv->edma_base, int_status_off[tx_info->type]));
+			st = EDMA_XFER_FAIL_TIMEOUT;
+			goto unlock;
+		} else {
+			st = ch->st;
+		}
+		dev_dbg(prv->dev, "xmit done for %d nents at %d widx and %d ridx\n",
+			tx_info->nents, ch->w_idx, ch->r_idx);
+	}
+unlock:
+	/* release hardware - unlocking */
+	mutex_unlock(&ch->lock);
+
+	return st;
+}
+
+void tegra_pcie_edma_deinit(void *cookie)
+{
+	struct edma_prv *prv = (struct edma_prv *)cookie;
+	int i, ret;
+	u32 val;
+
+	if (cookie == NULL)
+		return;
+
+	for (i = 0; i < DMA_WR_CHNL_NUM; i++)
+		prv->tx[i].st = EDMA_XFER_DEINIT;
+	for (i = 0; i < DMA_RD_CHNL_NUM; i++)
+		prv->rx[i].st = EDMA_XFER_DEINIT;
+
+	/** Poll for 10 seconds to clear RD & WR interrupts */
+	ret = readl_poll_timeout(prv->edma_base + DMA_WRITE_INT_STATUS_OFF, val,
+				 (val == 0), 20000, 10000000);
+	if (ret) {
+		dev_info(prv->dev, "DMA write interrupt pending: 0x%x\n", val);
+		dma_common_wr(prv->edma_base, val, DMA_WRITE_INT_STATUS_OFF);
+	}
+
+	ret = readl_poll_timeout(prv->edma_base + DMA_READ_INT_STATUS_OFF, val,
+				 (val == 0), 20000, 10000000);
+	if (ret) {
+		dev_info(prv->dev, "DMA read interrupt pending: 0x%x\n", val);
+		dma_common_wr(prv->edma_base, val, DMA_READ_INT_STATUS_OFF);
+	}
+
+	edma_hw_deinit(cookie, false);
+	edma_hw_deinit(cookie, true);
+
+	free_irq(prv->irq, prv);
+	kfree(prv->irq_name);
+
+	for (i = 0; i < DMA_WR_CHNL_NUM; i++) {
+		if (!prv->is_remote_dma && prv->tx[i].desc)
+			dma_free_coherent(prv->dev,
+					  (sizeof(struct edma_hw_desc) * prv->tx[i].desc_sz),
+					  prv->tx[i].desc, prv->tx[i].dma_iova);
+
+		kfree(prv->tx[i].ring);
+	}
+
+	for (i = 0; i < DMA_RD_CHNL_NUM; i++) {
+		if (!prv->is_remote_dma && prv->rx[i].desc)
+			dma_free_coherent(prv->dev,
+					  (sizeof(struct edma_hw_desc) * prv->rx[i].desc_sz),
+					  prv->rx[i].desc, prv->rx[i].dma_iova);
+
+		kfree(prv->rx[i].ring);
+	}
+
+	devm_iounmap(prv->dev, prv->edma_base);
+	if (!prv->is_remote_dma)
+		put_device(prv->dev);
+	kfree(prv);
+}
diff --git a/drivers/misc/nvscic2c-pcie/tegra-pcie-edma.h b/drivers/misc/nvscic2c-pcie/tegra-pcie-edma.h
new file mode 100644
index 000000000000..335ec3055ba6
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/tegra-pcie-edma.h
@@ -0,0 +1,167 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2021-2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef TEGRA_PCIE_EDMA_H
+#define TEGRA_PCIE_EDMA_H
+
+#define DMA_RD_CHNL_NUM			2
+#define DMA_WR_CHNL_NUM			4
+
+#define EDMA_DESC_SZ			32
+/**
+ * Application can use this macro as default number of descriptors.
+ * Number of descriptors should always be power of 2. */
+#define NUM_EDMA_DESC			4096
+
+/**
+ * @brief typedef to define various values for xfer status passed for edma_complete_t or
+ * tegra_pcie_edma_submit_xfer()
+ */
+typedef enum {
+	EDMA_XFER_SUCCESS = 0,
+	EDMA_XFER_FAIL_INVAL_INPUTS,
+	EDMA_XFER_FAIL_NOMEM,
+	EDMA_XFER_FAIL_TIMEOUT,
+	EDMA_XFER_ABORT,
+	EDMA_XFER_DEINIT,
+} edma_xfer_status_t;
+
+/** @brief typedef to define various values for xfer type passed tegra_pcie_edma_submit_xfer() */
+typedef enum {
+	EDMA_XFER_WRITE = 0,
+	EDMA_XFER_READ,
+} edma_xfer_type_t;
+
+/**
+ * @brief typedef to define various values for channel xfer configuration done
+ * during tegra_pcie_edma_submit_init()
+ */
+typedef enum {
+	EDMA_CHAN_XFER_SYNC = 0,
+	EDMA_CHAN_XFER_ASYNC,
+} edma_chan_type_t;
+
+/** Forward declaration */
+struct tegra_pcie_edma_desc;
+
+/** @brief Tx Async callback function pointer */
+typedef void (edma_complete_t)(void *priv, edma_xfer_status_t status,
+			       struct tegra_pcie_edma_desc *desc);
+
+
+/** @brief Remote EDMA controller details.
+ *  @note: this is initial revision and expected to be modified.
+ */
+struct pcie_tegra_edma_remote_info {
+	/** MSI IRQ number */
+	uint32_t msi_irq;
+	/** MSI data that needs to be configured on EP DMA registers */
+	uint16_t msi_data;
+	/** MSI address that needs to be configured on EP DMA registers */
+	uint64_t msi_addr;
+	/** EP's DMA PHY base address [can be read from BAR4] */
+	phys_addr_t dma_phy_base;
+	uint32_t dma_size;
+	/** &pci_dev.dev poniter used for devm_* and logging */
+	struct device *dev;
+};
+
+/** @brief details of EDMA Tx channel configuration */
+struct tegra_pcie_edma_chans_info {
+	/** Variable to specify if corresponding channel should run in Sync/Async mode. */
+	edma_chan_type_t ch_type;
+	/** Number of descriptors that needs to be configured for this channel.
+	 *  @note
+	 *   - If 0 is passed,
+	 *     - if non remote edma mode. default 4096 will be used.
+	 *     - else, this channel will be treated un-used.
+	 *   - else it must be power of 2.
+	 */
+	uint32_t num_descriptors;
+	/* Below parameter are used, only if edma_remote is present in #tegra_pcie_edma_init_info */
+	/** Descriptor PHY base allocated by client which is part of BAR0. Memory allocated for this
+	 *  should be 1 more than number of descriptors.
+	 */
+	phys_addr_t desc_phy_base;
+	/** Abosolute IOVA address of desc in BAR0 backing memory allocated in EP. */
+	dma_addr_t desc_iova;
+};
+
+/** @brief init data structure to be used for tegra_pcie_edma_init() API */
+struct tegra_pcie_edma_init_info {
+	/** configuration details for edma Tx channels */
+	struct tegra_pcie_edma_chans_info tx[DMA_WR_CHNL_NUM];
+	/** configuration details for edma Tx channels */
+	struct tegra_pcie_edma_chans_info rx[DMA_RD_CHNL_NUM];
+	/** device node for corresponding edma to read information from DT */
+	struct device_node *np;
+	/**
+	 * Remote edma pointer: if not NULL, library uses remote EDMA engine for transfers
+	 * else uses local controller EDMA engine.
+	 */
+	struct pcie_tegra_edma_remote_info *edma_remote;
+};
+
+/** @brief edma descriptor for data transfer operations */
+struct tegra_pcie_edma_desc {
+	/** source address of data buffer */
+	dma_addr_t src;
+	/** destination address where data buffer needs to be transferred */
+	dma_addr_t dst;
+	/** Size of data buffer */
+	uint32_t sz;
+};
+
+/** @brief data strcuture needs to be passed for Tx operations */
+struct tegra_pcie_edma_xfer_info {
+	/** Read or write operation. 0 -> write, 1->read */
+	edma_xfer_type_t type;
+	/** Channel on which operation needs to be performed.
+	 *  Range 0 to (DMA_RD_CHNL_NUM-1)/(DMA_WR_CHNL_NUM-1)
+	 */
+	uint32_t channel_num;
+	/** EDMA descriptor structure with source, destination DMA addr along with its size. */
+	struct tegra_pcie_edma_desc *desc;
+	/** Number of desc entries. */
+	uint32_t nents;
+	/** complete callback to be called */
+	edma_complete_t *complete;
+	/** Caller'z private data pointer which will be passed as part of edma_complete_t */
+	void *priv;
+};
+
+/**
+ * @brief: API to perform EDMA library initialization.
+ * @param[in] info: EDMA init data structure. Refer struct tegra_pcie_edma_init_info for details.
+ * @retVal: NULL on failure and valid pointer if success and this should
+ *          be passed for all subsequent calls of this library.
+ */
+void *tegra_pcie_edma_initialize(struct tegra_pcie_edma_init_info *info);
+
+/**
+ * @brief: API to perform transfer operation.
+ * @param[in] tx_info: EDMA Tx data structure. Refer struct tegra_pcie_edma_xfer_info for details.
+ * @param[in] coockie : cookie data returned in tegra_pcie_edma_initialize() call.
+ * @retVal: Refer edma_xfer_status_t.
+ */
+edma_xfer_status_t tegra_pcie_edma_submit_xfer(void *cookie,
+						struct tegra_pcie_edma_xfer_info *tx_info);
+
+/**
+ * @brief: API to perform de-init of EDMA library.
+ * @param[in] cookie : cookie data returned in tegra_pcie_edma_initialize() call.
+ */
+void tegra_pcie_edma_deinit(void *cookie);
+
+#endif //TEGRA_PCIE_EDMA_H
diff --git a/drivers/misc/nvscic2c-pcie/vmap-internal.h b/drivers/misc/nvscic2c-pcie/vmap-internal.h
new file mode 100644
index 000000000000..8e4d4428fc8c
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/vmap-internal.h
@@ -0,0 +1,183 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __VMAP_INTERNAL_H__
+#define __VMAP_INTERNAL_H__
+
+#include <linux/dma-buf.h>
+#include <linux/dma-iommu.h>
+#include <linux/pci.h>
+
+#include "common.h"
+#ifdef NVSCIC2C_GPU
+#include "nv-p2p.h"
+#include "rmp2pdefines.h"
+#endif
+#include "vmap.h"
+
+/* forward declaration. */
+struct vmap_ctx_t;
+
+struct memobj_pin_t {
+	enum vmap_mngd mngd;
+	enum vmap_obj_prot prot;
+	enum vmap_obj_type type;
+	enum mem_loc mem_type;
+	struct sg_table *sgt;
+
+#ifdef NVSCIC2C_GPU
+	/* P2P page tables and DMA mapping for Vidmem buffers*/
+	struct nvidia_p2p_page_table *page_table;
+	struct nvidia_p2p_dma_mapping *dma_mapping;
+#endif
+	/* list of sysmem pages pinned to memory for DMA operations*/
+	struct page **pages;
+	__u64 vaddr;
+	__u64 nr_pages;
+	enum dma_data_direction dir;
+
+	/*
+	 * [OUT]contiguous iova region obtained from client (iova-mngr).
+	 * which input buffer is mapped to.
+	 */
+	void *iova_block_h;
+	struct vmap_obj_attributes attrib;
+
+	/*
+	 * [OUT]
+	 * Per scatter-list nent mapping -  used during free.
+	 * Used for client-managed map only.
+	 */
+	u32 nr_nents;
+	struct iova_nent {
+		u64 iova;
+		size_t len;
+		bool mapped_iova;
+	} *nents;
+};
+
+struct syncobj_pin_t {
+	phys_addr_t phy_addr;
+
+	enum vmap_mngd mngd;
+	enum vmap_obj_prot prot;
+	enum vmap_obj_type type;
+
+	/* list of pages pinned to memory for DMA operations*/
+	struct page **pages;
+	__u64 vaddr;
+	__u64 nr_pages;
+
+	/* local sync objs do not require pinning to pcie address space.*/
+	bool pin_reqd;
+
+	/*
+	 * [OUT]contiguous iova region obtained from client (iova-mngr)
+	 * which sync primitive is mapped to.
+	 */
+	void *iova_block_h;
+	struct vmap_obj_attributes attrib;
+	bool mapped_iova;
+};
+
+struct importobj_reg_t {
+	/*
+	 * export descriptor and whereabouts of exported obj
+	 * as received from remote end.
+	 */
+	u64 export_desc;
+
+	/* times exported by remote, imported by local.*/
+	u32 nr_export;
+	u32 nr_import;
+
+	struct vmap_obj_attributes attrib;
+};
+
+/* virtual mapping information for Mem obj.*/
+struct memobj_map_ref {
+	s32 obj_id;
+	struct kref refcount;
+	struct memobj_pin_t pin;
+	struct vmap_ctx_t *vmap_ctx;
+};
+
+/* virtual mapping information for Sync obj. */
+struct syncobj_map_ref {
+	s32 obj_id;
+	struct kref refcount;
+	struct syncobj_pin_t pin;
+	struct vmap_ctx_t *vmap_ctx;
+};
+
+/* virtual mapping information for Imported obj. */
+struct importobj_map_ref {
+	s32 obj_id;
+	struct kref refcount;
+	struct importobj_reg_t reg;
+	struct vmap_ctx_t *vmap_ctx;
+};
+
+/* vmap subunit/abstraction context. */
+struct vmap_ctx_t {
+	/* pci-client abstraction handle.*/
+	void *pci_client_h;
+
+	/* comm-channel abstraction. */
+	void *comm_channel_h;
+
+	/*
+	 * dummy platform device. - This has smmu disabled to get the
+	 * physical addresses of exported Mem objects when using client
+	 * managed mapping.
+	 */
+	struct platform_device *dummy_pdev;
+	bool dummy_pdev_init;
+
+	/*
+	 * Management of Mem/Sync object Ids.
+	 *
+	 * All objects mapped are identified by - pin_id. IDR mechanism
+	 * generates these IDs. We maintain separate book-keeping for
+	 * Mem, Sync and Import objects. The ID shall overalap between
+	 * Mem, Sync and Import objects.
+	 *
+	 * ID is the pinned handle returned to other units.
+	 */
+	struct idr mem_idr;
+	struct idr sync_idr;
+	struct idr import_idr;
+
+	/* exclusive access to mem idr.*/
+	struct mutex mem_idr_lock;
+	/* exclusive access to sync idr.*/
+	struct mutex sync_idr_lock;
+	/* exclusive access to import idr.*/
+	struct mutex import_idr_lock;
+};
+
+void
+memobj_unpin(struct vmap_ctx_t *vmap_ctx,
+	     struct memobj_pin_t *pin);
+int
+memobj_pin(struct vmap_ctx_t *vmap_ctx,
+	   struct memobj_pin_t *pin);
+void
+syncobj_unpin(struct vmap_ctx_t *vmap_ctx,
+	      struct syncobj_pin_t *pin);
+int
+syncobj_pin(struct vmap_ctx_t *vmap_ctx,
+	    struct syncobj_pin_t *pin);
+
+#endif //__VMAP_INTERNAL_H__
diff --git a/drivers/misc/nvscic2c-pcie/vmap-pin.c b/drivers/misc/nvscic2c-pcie/vmap-pin.c
new file mode 100644
index 000000000000..093ed28eb781
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/vmap-pin.c
@@ -0,0 +1,507 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#include <linux/device.h>
+#include <linux/dma-buf.h>
+#include <linux/dma-iommu.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/pci.h>
+#include <linux/platform_device.h>
+#include <linux/printk.h>
+#include <linux/types.h>
+
+#include "common.h"
+#include "module.h"
+#include "pci-client.h"
+#include "vmap.h"
+#include "vmap-internal.h"
+
+#ifdef NVSCIC2C_GPU
+static u32 nvidia_p2p_page_size_mappings[NVIDIA_P2P_PAGE_SIZE_COUNT] = {
+	NVRM_P2P_PAGESIZE_SMALL_4K,
+	NVRM_P2P_PAGESIZE_BIG_64K,
+	NVRM_P2P_PAGESIZE_BIG_128K
+};
+
+void
+vidmemobj_devmngd_unpin(struct vmap_ctx_t *vmap_ctx,
+			struct memobj_pin_t *pin)
+{
+	if (!vmap_ctx)
+		return;
+	if (!pin)
+		return;
+	if (!pin->sgt)
+		return;
+
+	sg_free_table(pin->sgt);
+	kfree(pin->sgt);
+	pin->sgt = NULL;
+
+	pci_client_dma_unmap_vidmem(vmap_ctx->pci_client_h, pin->page_table,
+				    pin->dma_mapping);
+}
+
+static int
+vidmemobj_devmngd_pin(struct vmap_ctx_t *vmap_ctx,
+		      struct memobj_pin_t *pin)
+{
+	int ret = 0;
+	u32 page_size;
+	u32 sg_index = 0;
+	struct scatterlist *sg = NULL;
+	enum nvidia_p2p_page_size_type page_size_type;
+
+	if (pin->prot == VMAP_OBJ_PROT_WRITE)
+		pin->dir = DMA_FROM_DEVICE;
+	else
+		pin->dir = DMA_TO_DEVICE;
+
+	page_size_type = pin->page_table->page_size;
+	if (page_size_type <= NVIDIA_P2P_PAGE_SIZE_4KB ||
+	    page_size_type >= NVIDIA_P2P_PAGE_SIZE_COUNT) {
+		pr_err("Invalid page size for vidmem\n");
+		return -EINVAL;
+	}
+
+	page_size = nvidia_p2p_page_size_mappings[page_size_type];
+	pin->attrib.size = pin->page_table->entries * page_size;
+	ret = pci_client_dma_map_vidmem(vmap_ctx->pci_client_h, pin->page_table,
+					&pin->dma_mapping);
+	if (ret) {
+		pr_err("Failed to create DMA mapping for vidmem err=%d\n", ret);
+		return ret;
+	}
+
+	pin->sgt = kzalloc(sizeof(*pin->sgt), GFP_KERNEL);
+	if (WARN_ON(!pin->sgt)) {
+		pr_err("Failed to allocate sg table for vidmem\n");
+		return -ENOMEM;
+	}
+	ret = sg_alloc_table(pin->sgt, pin->dma_mapping->entries, GFP_KERNEL);
+	if (ret) {
+		pr_err("Failed to build sg_table for %d dma mapping\n", pin->dma_mapping->entries);
+		kfree(pin->sgt);
+		pin->sgt = NULL;
+		return -ENOMEM;
+	}
+	for_each_sg(pin->sgt->sgl, sg, pin->dma_mapping->entries, sg_index) {
+		sg->dma_address = pin->dma_mapping->dma_addresses[sg_index];
+		sg->dma_length = page_size;
+	}
+
+	pin->attrib.iova = pin->dma_mapping->dma_addresses[pin->dma_mapping->entries - 1];
+	pin->attrib.offsetof = 0;
+
+	return ret;
+}
+#endif
+
+void
+sysmemobj_devmngd_unpin(struct vmap_ctx_t *vmap_ctx,
+			struct memobj_pin_t *pin)
+{
+	if (!vmap_ctx)
+		return;
+	if (!pin)
+		return;
+	if (!pin->sgt)
+		return;
+
+	pci_client_dma_unmap_sysmem(vmap_ctx->pci_client_h, pin->sgt,
+				    pin->dir);
+
+	sg_free_table(pin->sgt);
+	kfree(pin->sgt);
+	pin->sgt = NULL;
+}
+
+static int
+sysmemobj_devmngd_pin(struct vmap_ctx_t *vmap_ctx,
+		      struct memobj_pin_t *pin)
+{
+	int ret = 0;
+
+	if (pin->prot == VMAP_OBJ_PROT_WRITE)
+		pin->dir = DMA_FROM_DEVICE;
+	else
+		pin->dir = DMA_TO_DEVICE;
+
+	pin->sgt = kzalloc(sizeof(*pin->sgt), GFP_KERNEL);
+	if (WARN_ON(!pin->sgt)) {
+		pr_err("Failed to allocate sg table for sysmem\n");
+		return -ENOMEM;
+	}
+
+	pin->attrib.size = pin->nr_pages * PAGE_SIZE;
+	ret = sg_alloc_table_from_pages(pin->sgt, pin->pages, pin->nr_pages, 0,
+					pin->attrib.size, GFP_KERNEL);
+	if (ret) {
+		pr_err("Failed to build sg_table for %lld pages\n", pin->nr_pages);
+		kfree(pin->sgt);
+		pin->sgt = NULL;
+		return -ENOMEM;
+	}
+
+	ret = pci_client_dma_map_sysmem(vmap_ctx->pci_client_h, pin->sgt, pin->dir);
+	if (ret) {
+		pr_err("Failed to map sysmem to devmngd iova\n");
+		kfree(pin->sgt);
+		pin->sgt = NULL;
+		return -ENOMEM;
+	}
+	pin->attrib.iova = sg_dma_address(pin->sgt->sgl);
+	/*
+	 * dev mngd used in local mem or remote mem (when exporting from
+	 * Tegra PCIe RP), in both cases, offsetof is not needed.
+	 */
+	pin->attrib.offsetof = 0;
+
+	return ret;
+}
+
+void
+memobj_clientmngd_unpin(struct vmap_ctx_t *vmap_ctx,
+			struct memobj_pin_t *pin)
+{
+	u32 i = 0;
+
+	if (!vmap_ctx)
+		return;
+	if (!pin)
+		return;
+
+	/*common flow for both sysmem and vidmem buffers*/
+	if (pin->nents) {
+		for (i = 0; i < pin->nr_nents; i++) {
+			if (pin->nents[i].mapped_iova) {
+				pci_client_unmap_addr(vmap_ctx->pci_client_h,
+						      pin->nents[i].iova,
+						      pin->nents[i].len);
+				pin->nents[i].mapped_iova = false;
+			}
+		}
+		kfree(pin->nents);
+		pin->nents = NULL;
+	}
+	if (pin->iova_block_h) {
+		pci_client_free_iova(vmap_ctx->pci_client_h,
+				     &pin->iova_block_h);
+		pin->iova_block_h = NULL;
+	}
+}
+
+#ifdef NVSCIC2C_GPU
+static int
+vidmemobj_clientmngd_pin(struct vmap_ctx_t *vmap_ctx,
+			 struct memobj_pin_t *pin)
+{
+	int ret = 0;
+	u64 iova = 0;
+	u32 index = 0;
+	u32 page_size;
+	enum nvidia_p2p_page_size_type page_size_type;
+	int prot = IOMMU_WRITE;
+
+	/* The remote end will perform a dummy read operation after
+	 * DMA write operation to ensure data coherency.
+	 * This is required only for Vidmem memory based on limitation
+	 * of Ampere Architecture but shall be done even for sysmem buffers
+	 * as the remote end is not aware of the buffers it's writing into.
+	 */
+	if (pin->prot == VMAP_OBJ_PROT_WRITE) {
+		prot = IOMMU_READ | IOMMU_WRITE | IOMMU_CACHE;
+		pin->dir = DMA_FROM_DEVICE;
+	} else {
+		prot = IOMMU_READ | IOMMU_CACHE;
+		pin->dir = DMA_TO_DEVICE;
+	}
+
+	page_size_type = pin->page_table->page_size;
+	if (page_size_type <= NVIDIA_P2P_PAGE_SIZE_4KB ||
+	    page_size_type >= NVIDIA_P2P_PAGE_SIZE_COUNT) {
+		pr_err("Invalid page size for vidmem\n");
+		return -EINVAL;
+	}
+	page_size = nvidia_p2p_page_size_mappings[page_size_type];
+	pin->attrib.size = pin->page_table->entries * page_size;
+
+	/* get one contiguous iova.*/
+	ret = pci_client_alloc_iova(vmap_ctx->pci_client_h, pin->attrib.size,
+				    &pin->attrib.iova, &pin->attrib.offsetof,
+				    &pin->iova_block_h);
+	if (ret) {
+		pr_err("Failed to reserve iova block of size: (%lu)\n",
+			pin->attrib.size);
+		return -ENOMEM;
+	}
+
+	/* pin the scatter list to contiguous iova.*/
+	pin->nr_nents = pin->page_table->entries;
+	pin->nents = kzalloc((sizeof(*pin->nents) * pin->nr_nents), GFP_KERNEL);
+	if (WARN_ON(!pin->nents)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	iova = pin->attrib.iova;
+	for (index = 0; index < pin->page_table->entries; index++) {
+		phys_addr_t paddr = (phys_addr_t)(pin->page_table->pages[index]->physical_address);
+
+		pin->nents[index].iova = iova;
+		pin->nents[index].len = page_size;
+		ret = pci_client_map_addr(vmap_ctx->pci_client_h,
+					  pin->nents[index].iova, paddr,
+					  pin->nents[index].len, prot);
+		if (ret < 0) {
+			pr_err("Failed: to iommu_map sg_nent: (%u), size: (%u)\n",
+				index, page_size);
+			goto err;
+		}
+		pin->nents[index].mapped_iova = true;
+
+		/* store information for unmap.*/
+		iova += page_size;
+	}
+	return ret;
+err:
+	memobj_clientmngd_unpin(vmap_ctx, pin);
+	return ret;
+}
+#endif
+
+static int
+sysmemobj_clientmngd_pin(struct vmap_ctx_t *vmap_ctx,
+			 struct memobj_pin_t *pin)
+{
+	int ret = 0;
+	u64 iova = 0;
+	u32 index = 0;
+	int prot  = IOMMU_WRITE;
+	phys_addr_t paddr = 0;
+
+	/* The remote end will perform a dummy read operation after
+	 * DMA write operation to ensure data coherency.
+	 * This is required only for Vidmem memory based on limitation
+	 * of Ampere Architecture but shall be done even for sysmem buffers
+	 * as the remote end is not aware of the buffers it's writing into.
+	 */
+	if (pin->prot == VMAP_OBJ_PROT_WRITE) {
+		prot = IOMMU_READ | IOMMU_WRITE | IOMMU_CACHE;
+		pin->dir = DMA_FROM_DEVICE;
+	} else {
+		prot = IOMMU_READ | IOMMU_CACHE;
+		pin->dir = DMA_TO_DEVICE;
+	}
+
+	pin->attrib.size = pin->nr_pages * PAGE_SIZE;
+	/* get one contiguous iova.*/
+	ret = pci_client_alloc_iova(vmap_ctx->pci_client_h, pin->attrib.size,
+				    &pin->attrib.iova, &pin->attrib.offsetof,
+				    &pin->iova_block_h);
+	if (ret) {
+		pr_err("Failed to reserve iova block of size: (%lu)\n",
+			pin->attrib.size);
+		goto err;
+	}
+
+	/* pin the scatter list to contiguous iova.*/
+	pin->nr_nents = pin->nr_pages;
+	pin->nents = kzalloc((sizeof(*pin->nents) * pin->nr_nents), GFP_KERNEL);
+	if (WARN_ON(!pin->nents)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	iova = pin->attrib.iova;
+	for (index = 0; index < pin->nr_nents; index++) {
+		paddr = page_to_phys(pin->pages[index]);
+		pin->nents[index].iova = iova;
+		pin->nents[index].len = PAGE_SIZE;
+		ret = pci_client_map_addr(vmap_ctx->pci_client_h,
+					  pin->nents[index].iova, paddr,
+					  pin->nents[index].len, prot);
+		if (ret < 0) {
+			pr_err("Failed: to iommu_map sg_nent: (%u), size: (%lu)\n",
+				index, PAGE_SIZE);
+			goto err;
+		}
+		pin->nents[index].mapped_iova = true;
+
+		/* store information for unmap.*/
+		iova += PAGE_SIZE;
+	}
+
+	return ret;
+err:
+	memobj_clientmngd_unpin(vmap_ctx, pin);
+	return ret;
+}
+
+void
+memobj_unpin(struct vmap_ctx_t *vmap_ctx,
+	     struct memobj_pin_t *pin)
+{
+	if (!vmap_ctx)
+		return;
+	if (!pin)
+		return;
+
+	/* Common for both Sysmem and Vidmem*/
+	if (pin->mngd == VMAP_MNGD_CLIENT)
+		memobj_clientmngd_unpin(vmap_ctx, pin);
+	/* Devmngd iova unpinning*/
+	else {
+		if (pin->mem_type == VMAP_SYSMEM)
+			sysmemobj_devmngd_unpin(vmap_ctx, pin);
+#ifdef NVSCIC2C_GPU
+		else if (pin->mem_type == VMAP_VIDMEM)
+			vidmemobj_devmngd_unpin(vmap_ctx, pin);
+#endif
+		else
+			pr_err("invalid unpin request\n");
+	}
+}
+
+int
+memobj_pin(struct vmap_ctx_t *vmap_ctx,
+	   struct memobj_pin_t *pin)
+{
+	int ret = 0;
+
+	if (pin->mngd == VMAP_MNGD_CLIENT) {
+		if (pin->mem_type == VMAP_SYSMEM)
+			ret = sysmemobj_clientmngd_pin(vmap_ctx, pin);
+#ifdef NVSCIC2C_GPU
+		else if (pin->mem_type == VMAP_VIDMEM)
+			ret = vidmemobj_clientmngd_pin(vmap_ctx, pin);
+#endif
+		else {
+			pr_err("Invalid memory type for pinning\n");
+			return -EINVAL;
+		}
+	}
+	/* Devmngd iova pinning*/
+	else {
+		if (pin->mem_type == VMAP_SYSMEM)
+			ret = sysmemobj_devmngd_pin(vmap_ctx, pin);
+#ifdef NVSCIC2C_GPU
+		else if (pin->mem_type == VMAP_VIDMEM)
+			ret = vidmemobj_devmngd_pin(vmap_ctx, pin);
+#endif
+		else {
+			pr_err("Invalid memory type for pinning\n");
+			return -EINVAL;
+		}
+	}
+	return ret;
+}
+
+static void
+syncobj_clientmngd_unpin(struct vmap_ctx_t *vmap_ctx,
+			 struct syncobj_pin_t *pin)
+{
+	if (!vmap_ctx)
+		return;
+
+	if (!pin)
+		return;
+
+	if (pin->mapped_iova) {
+		pci_client_unmap_addr(vmap_ctx->pci_client_h,
+				      pin->attrib.iova, pin->attrib.size);
+		pin->mapped_iova = false;
+	}
+
+	if (pin->iova_block_h) {
+		pci_client_free_iova(vmap_ctx->pci_client_h,
+				     &pin->iova_block_h);
+		pin->iova_block_h = NULL;
+	}
+}
+
+static int
+syncobj_clientmngd_pin(struct vmap_ctx_t *vmap_ctx,
+		       struct syncobj_pin_t *pin)
+{
+	int ret = 0;
+	phys_addr_t paddr;
+
+	if (pin->prot != VMAP_OBJ_PROT_WRITE) {
+		pr_err("Pinning syncobj with read access not supported\n");
+		return -EOPNOTSUPP;
+	}
+
+	pin->attrib.size = PAGE_SIZE;
+	ret = pci_client_alloc_iova(vmap_ctx->pci_client_h, pin->attrib.size,
+				    &pin->attrib.iova, &pin->attrib.offsetof,
+				    &pin->iova_block_h);
+	if (ret) {
+		pr_err("Failed to reserve iova block of size: (%lu)\n",
+			pin->attrib.size);
+		goto err;
+	}
+
+	/* Only 1 page can be pinned for syncobj*/
+	paddr = page_to_phys(pin->pages[0]);
+	ret = pci_client_map_addr(vmap_ctx->pci_client_h, pin->attrib.iova,
+				  paddr, pin->attrib.size,
+				  (IOMMU_CACHE | IOMMU_WRITE));
+	if (ret) {
+		pr_err("Failed to pin sync physical addr to client iova\n");
+		goto err;
+	}
+	pin->mapped_iova = true;
+
+	return ret;
+err:
+	syncobj_clientmngd_unpin(vmap_ctx, pin);
+	return ret;
+}
+
+void
+syncobj_unpin(struct vmap_ctx_t *vmap_ctx,
+	      struct syncobj_pin_t *pin)
+{
+	if (!vmap_ctx)
+		return;
+
+	if (!pin)
+		return;
+
+	if (pin->pin_reqd) {
+		if (pin->mngd == VMAP_MNGD_DEV)
+			return;
+
+		syncobj_clientmngd_unpin(vmap_ctx, pin);
+	}
+}
+
+int
+syncobj_pin(struct vmap_ctx_t *vmap_ctx,
+	    struct syncobj_pin_t *pin)
+{
+	if (pin->pin_reqd) {
+		/*
+		 * remote/export sync obj are mapped to an iova of client
+		 * choice always and we should not come here for local sync objs
+		 */
+		if (pin->mngd == VMAP_MNGD_DEV)
+			return -EOPNOTSUPP;
+
+		return syncobj_clientmngd_pin(vmap_ctx, pin);
+	}
+	return 0;
+}
diff --git a/drivers/misc/nvscic2c-pcie/vmap.c b/drivers/misc/nvscic2c-pcie/vmap.c
new file mode 100644
index 000000000000..0a443351ecf4
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/vmap.c
@@ -0,0 +1,901 @@
+// SPDX-License-Identifier: GPL-2.0+
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#define pr_fmt(fmt)	"nvscic2c-pcie: vmap: " fmt
+
+#include <linux/device.h>
+#include <linux/errno.h>
+#include <linux/kernel.h>
+#include <linux/module.h>
+#include <linux/of.h>
+#include <linux/of_platform.h>
+#include <linux/platform_device.h>
+#include <linux/printk.h>
+#include <linux/highmem.h>
+#include <linux/types.h>
+#include <linux/version.h>
+
+#include "comm-channel.h"
+#include "common.h"
+#include "descriptor.h"
+#include "module.h"
+#include "pci-client.h"
+#include "vmap.h"
+#include "vmap-internal.h"
+
+/*
+ * *_START must be > 0 to avoid interference with idr_for_each().
+ */
+#define MEMOBJ_START	(1)
+#define SYNCOBJ_START	(1)
+#define IMPORTOBJ_START	(1)
+#define MEMOBJ_END	(MAX_STREAM_MEMOBJS)
+#define SYNCOBJ_END	(MAX_STREAM_SYNCOBJS)
+#define IMPORTOBJ_END	(MAX_STREAM_MEMOBJS + MAX_STREAM_SYNCOBJS)
+
+#ifdef NVSCIC2C_GPU
+#define GPU_PAGE_SHIFT	16
+#define GPU_PAGE_SIZE	((u64)1 << GPU_PAGE_SHIFT)
+#define GPU_PAGE_OFFSET	(GPU_PAGE_SIZE - 1)
+#define GPU_PAGE_MASK	(~GPU_PAGE_OFFSET)
+
+static int
+vidmemobj_map(struct vmap_ctx_t *vmap_ctx,
+	      struct vmap_memobj_map_params *params,
+	      struct vmap_obj_attributes *attrib)
+{
+	int ret = 0;
+	struct memobj_map_ref *map = NULL;
+	nvidia_p2p_page_table_t *page_table = NULL;
+	u64 page_virt_start;
+	u64 page_virt_end;
+	size_t mapped_size;
+
+	page_virt_start = params->vaddr & GPU_PAGE_MASK;
+	page_virt_end   = (params->vaddr + params->size + GPU_PAGE_SIZE - 1) & GPU_PAGE_MASK;
+	mapped_size  = page_virt_end - page_virt_start;
+
+	ret = nvidia_p2p_get_pages(0, 0, page_virt_start,
+				   mapped_size, &page_table,
+				   NULL, NULL);
+	if (!page_table) {
+		pr_err("Failed to create page table for virtual address %llu\n", params->vaddr);
+		return -EFAULT;
+	}
+
+	mutex_lock(&vmap_ctx->mem_idr_lock);
+	map = kzalloc(sizeof(*map), GFP_KERNEL);
+	if (WARN_ON(!map)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+	map->vmap_ctx = vmap_ctx;
+	kref_init(&map->refcount);
+	map->pin.vaddr = params->vaddr;
+	map->pin.page_table = page_table;
+	map->pin.prot = params->prot;
+	map->pin.mngd = params->mngd;
+	map->pin.mem_type = params->mem_type;
+	map->obj_id = idr_alloc(&vmap_ctx->mem_idr, map,
+				MEMOBJ_START, MEMOBJ_END,
+				GFP_KERNEL);
+	if (map->obj_id <= 0) {
+		ret = map->obj_id;
+		pr_err("Failed to idr alloc for mem obj\n");
+		kfree(map);
+		goto err;
+	}
+
+	/* populates map->pin.attrib within.*/
+	ret = memobj_pin(vmap_ctx, &map->pin);
+	if (ret) {
+		pr_err("Failed to pin mem obj with vaddr: (%lld)\n", params->vaddr);
+		idr_remove(&vmap_ctx->mem_idr, map->obj_id);
+		kfree(map);
+		goto err;
+	}
+
+	attrib->type = VMAP_OBJ_TYPE_MEM;
+	attrib->id = map->obj_id;
+	attrib->iova = map->pin.attrib.iova;
+	attrib->size = map->pin.attrib.size;
+	attrib->sgt = map->pin.sgt;
+	attrib->offsetof = map->pin.attrib.offsetof;
+	mutex_unlock(&vmap_ctx->mem_idr_lock);
+	return 0;
+err:
+	mutex_unlock(&vmap_ctx->mem_idr_lock);
+	(void)nvidia_p2p_put_pages(0, 0, params->vaddr, page_table);
+	return ret;
+}
+#endif
+
+static int
+sysmemobj_map(struct vmap_ctx_t *vmap_ctx,
+	      struct vmap_memobj_map_params *params,
+	      struct vmap_obj_attributes *attrib)
+{
+	int ret = 0;
+	u64 nr_pages = 0;
+	u64 first, last;
+	u32 foll_flags = 0;
+	struct memobj_map_ref *map = NULL;
+	struct page **pages;
+
+	first = (params->vaddr & PAGE_MASK) >> PAGE_SHIFT;
+	last  = ((params->vaddr + params->size - 1) & PAGE_MASK) >> PAGE_SHIFT;
+	nr_pages = last - first + 1;
+	pages = kcalloc(nr_pages, sizeof(void *), GFP_KERNEL);
+	if (!pages)
+		return -ENOMEM;
+
+	if (params->prot == VMAP_OBJ_PROT_WRITE)
+		foll_flags |= FOLL_WRITE;
+
+	ret = get_user_pages((params->vaddr) & PAGE_MASK, nr_pages,
+			     foll_flags | FOLL_LONGTERM, pages, NULL);
+	if (ret < nr_pages) {
+		pr_err("Pinning of pages failed for address %lld\n", params->vaddr);
+		kfree(pages);
+		return -ENOMEM;
+	}
+
+	mutex_lock(&vmap_ctx->mem_idr_lock);
+	map = kzalloc(sizeof(*map), GFP_KERNEL);
+	if (WARN_ON(!map)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	map->vmap_ctx = vmap_ctx;
+	kref_init(&map->refcount);
+	map->pin.vaddr = params->vaddr;
+	map->pin.pages = pages;
+	map->pin.nr_pages = nr_pages;
+	map->pin.prot = params->prot;
+	map->pin.mngd = params->mngd;
+	map->pin.mem_type = params->mem_type;
+	map->obj_id = idr_alloc(&vmap_ctx->mem_idr, map,
+				MEMOBJ_START, MEMOBJ_END,
+				GFP_KERNEL);
+	if (map->obj_id <= 0) {
+		ret = map->obj_id;
+		pr_err("Failed to idr alloc for mem obj\n");
+		kfree(map);
+		ret = -ENOMEM;
+		goto err;
+	}
+	/* populates map->pin.attrib within.*/
+	ret = memobj_pin(vmap_ctx, &map->pin);
+	if (ret) {
+		pr_err("Failed to pin mem obj with vaddr: (%llx)\n", params->vaddr);
+		idr_remove(&vmap_ctx->mem_idr, map->obj_id);
+		kfree(map);
+		goto err;
+	}
+
+	attrib->type = VMAP_OBJ_TYPE_MEM;
+	attrib->id = map->obj_id;
+	attrib->iova = map->pin.attrib.iova;
+	attrib->size = map->pin.attrib.size;
+	attrib->offsetof = map->pin.attrib.offsetof;
+	mutex_unlock(&vmap_ctx->mem_idr_lock);
+	return 0;
+err:
+	mutex_unlock(&vmap_ctx->mem_idr_lock);
+	if (pages) {
+		put_user_pages(pages, nr_pages);
+		kfree(pages);
+	}
+	return ret;
+}
+
+static int
+memobj_map(struct vmap_ctx_t *vmap_ctx,
+	   struct vmap_memobj_map_params *params,
+	   struct vmap_obj_attributes *attrib)
+{
+	if (params->mem_type == VMAP_SYSMEM)
+		return sysmemobj_map(vmap_ctx, params, attrib);
+#ifdef NVSCIC2C_GPU
+	else if (params->mem_type == VMAP_VIDMEM)
+		return vidmemobj_map(vmap_ctx, params, attrib);
+#endif
+	else {
+		pr_err("invalid memory location specified for buffer (%llx)\n", params->vaddr);
+		return -EINVAL;
+	}
+}
+
+/* must be called with idr lock held.*/
+static void
+memobj_free(struct kref *kref)
+{
+	struct memobj_map_ref *map = NULL;
+
+	if (!kref)
+		return;
+	map = container_of(kref, struct memobj_map_ref, refcount);
+	if (map) {
+		memobj_unpin(map->vmap_ctx, &map->pin);
+#ifdef NVSCIC2C_GPU
+		if (map->pin.page_table) {
+			(void)nvidia_p2p_put_pages(0, 0, map->pin.vaddr, map->pin.page_table);
+			map->pin.page_table = NULL;
+		}
+#endif
+		if (map->pin.pages) {
+			put_user_pages(map->pin.pages, map->pin.nr_pages);
+			kfree(map->pin.pages);
+			map->pin.pages = NULL;
+		}
+		idr_remove(&map->vmap_ctx->mem_idr, map->obj_id);
+		kfree(map);
+	}
+}
+
+static int
+memobj_unmap(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	struct memobj_map_ref *map = NULL;
+
+	mutex_lock(&vmap_ctx->mem_idr_lock);
+	map = idr_find(&vmap_ctx->mem_idr, obj_id);
+	if (!map) {
+		mutex_unlock(&vmap_ctx->mem_idr_lock);
+		return -EBADF;
+	}
+
+	kref_put(&map->refcount, memobj_free);
+	mutex_unlock(&vmap_ctx->mem_idr_lock);
+
+	return 0;
+}
+
+static int
+memobj_putref(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	return memobj_unmap(vmap_ctx, obj_id);
+}
+
+static int
+memobj_getref(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	struct memobj_map_ref *map = NULL;
+
+	mutex_lock(&vmap_ctx->mem_idr_lock);
+	map = idr_find(&vmap_ctx->mem_idr, obj_id);
+	if (WARN_ON(!map)) {
+		mutex_unlock(&vmap_ctx->mem_idr_lock);
+		return -EBADF;
+	}
+
+	kref_get(&map->refcount);
+	mutex_unlock(&vmap_ctx->mem_idr_lock);
+
+	return 0;
+}
+
+static int
+syncobj_map(struct vmap_ctx_t *vmap_ctx,
+	    struct vmap_syncobj_map_params *params,
+	    struct vmap_obj_attributes *attrib)
+{
+	int ret = 0;
+	u64 nr_pages = 0;
+	struct syncobj_map_ref *map = NULL;
+	struct page **pages;
+	u64 first, last;
+
+	first = (params->vaddr & PAGE_MASK) >> PAGE_SHIFT;
+	last  = ((params->vaddr + params->size - 1) & PAGE_MASK) >> PAGE_SHIFT;
+	nr_pages = last - first + 1;
+	if (nr_pages != 1) {
+		pr_err("For semaphores, only 1 page can be used for pinning\n");
+		return -EINVAL;
+	}
+
+	pages = kzalloc(nr_pages * sizeof(void *), GFP_KERNEL);
+	if (!pages)
+		return -ENOMEM;
+
+	/* Pinning is required only for REMOTE_SYNC which have prot=VMAP_OBJ_PROT_WRITE*/
+	ret = get_user_pages(params->vaddr, nr_pages, FOLL_WRITE | FOLL_LONGTERM, pages, NULL);
+	if (ret < nr_pages) {
+		pr_err("Pinning of pages failed for address (%llx)\n", params->vaddr);
+		kfree(pages);
+		return -ENOMEM;
+	}
+
+	mutex_lock(&vmap_ctx->sync_idr_lock);
+	map = kzalloc(sizeof(*map), GFP_KERNEL);
+	if (WARN_ON(!map)) {
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	map->vmap_ctx = vmap_ctx;
+	kref_init(&map->refcount);
+	/* local syncobjs do not need to be pinned to pcie iova.*/
+	map->pin.pin_reqd = params->pin_reqd;
+	map->pin.vaddr = params->vaddr;
+	map->pin.pages = pages;
+	map->pin.nr_pages = nr_pages;
+	map->pin.prot = params->prot;
+	map->pin.mngd = params->mngd;
+	map->obj_id = idr_alloc(&vmap_ctx->sync_idr, map,
+				MEMOBJ_START, MEMOBJ_END,
+				GFP_KERNEL);
+	if (map->obj_id <= 0) {
+		ret = map->obj_id;
+		pr_err("Failed to idr alloc for mem obj\n");
+		kfree(map);
+		ret = -ENOMEM;
+		goto err;
+	}
+
+	/* populates map->pin.attrib within.*/
+	ret = syncobj_pin(vmap_ctx, &map->pin);
+	if (ret) {
+		pr_err("Failed to pin sync obj with vaddr: (%lld)\n", params->vaddr);
+		idr_remove(&vmap_ctx->sync_idr, map->obj_id);
+		kfree(map);
+		goto err;
+	}
+
+	attrib->type = VMAP_OBJ_TYPE_SYNC;
+	attrib->id = map->obj_id;
+	attrib->iova = map->pin.attrib.iova;
+	attrib->size = map->pin.attrib.size;
+	attrib->offsetof = map->pin.attrib.offsetof;
+	attrib->local_fence_addr = kmap(pages[0]);
+	mutex_unlock(&vmap_ctx->sync_idr_lock);
+	return 0;
+err:
+	mutex_unlock(&vmap_ctx->sync_idr_lock);
+	if (pages) {
+		put_user_pages(pages, nr_pages);
+		kfree(pages);
+	}
+	return ret;
+}
+
+/* must be called with idr lock held.*/
+static void
+syncobj_free(struct kref *kref)
+{
+	struct syncobj_map_ref *map = NULL;
+
+	if (!kref)
+		return;
+	map = container_of(kref, struct syncobj_map_ref, refcount);
+	if (map) {
+		syncobj_unpin(map->vmap_ctx, &map->pin);
+		kunmap(map->pin.pages[0]);
+		idr_remove(&map->vmap_ctx->sync_idr, map->obj_id);
+		kfree(map);
+	}
+}
+
+static int
+syncobj_unmap(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	struct syncobj_map_ref *map = NULL;
+
+	mutex_lock(&vmap_ctx->sync_idr_lock);
+	map = idr_find(&vmap_ctx->sync_idr, obj_id);
+	if (!map) {
+		mutex_unlock(&vmap_ctx->sync_idr_lock);
+		return -EBADF;
+	}
+
+	kref_put(&map->refcount, syncobj_free);
+	mutex_unlock(&vmap_ctx->sync_idr_lock);
+
+	return 0;
+}
+
+static int
+syncobj_putref(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	return syncobj_unmap(vmap_ctx, obj_id);
+}
+
+static int
+syncobj_getref(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	struct memobj_map_ref *map = NULL;
+
+	if (!vmap_ctx)
+		return -EINVAL;
+
+	mutex_lock(&vmap_ctx->sync_idr_lock);
+	map = idr_find(&vmap_ctx->sync_idr, obj_id);
+	if (WARN_ON(!map)) {
+		mutex_unlock(&vmap_ctx->sync_idr_lock);
+		return -EBADF;
+	}
+	kref_get(&map->refcount);
+	mutex_unlock(&vmap_ctx->sync_idr_lock);
+
+	return 0;
+}
+
+static int
+match_export_desc(int id, void *entry, void *data)
+{
+	struct importobj_map_ref *map = (struct importobj_map_ref *)entry;
+
+	if (map->reg.export_desc == *((u64 *)data))
+		return id;
+
+	/* 0 shall pick-up next entry.*/
+	return 0;
+}
+
+static int
+importobj_map(struct vmap_ctx_t *vmap_ctx,
+	      struct vmap_importobj_map_params *params,
+	      struct vmap_obj_attributes *attrib)
+{
+	int ret = 0;
+	u32 id_exist = 0;
+	struct importobj_map_ref *map = NULL;
+
+	mutex_lock(&vmap_ctx->import_idr_lock);
+
+	/* check if we have export descriptor from remote already ? */
+	id_exist = idr_for_each(&vmap_ctx->import_idr, match_export_desc,
+				&params->export_desc);
+	if (id_exist > 0)
+		map = idr_find(&vmap_ctx->import_idr, id_exist);
+
+	if (!map) {
+		ret = -EAGAIN;
+		pr_debug("Failed to find descriptor: (%llu), try again\n",
+			 params->export_desc);
+		goto err;
+	} else {
+		/* importing beyond the export from remote is not permitted.*/
+		if (map->reg.nr_import == map->reg.nr_export) {
+			ret = -EPERM;
+			goto err;
+		}
+		map->reg.nr_import++;
+
+		attrib->type = VMAP_OBJ_TYPE_IMPORT;
+		attrib->id = map->obj_id;
+		attrib->iova = map->reg.attrib.iova;
+		attrib->size = map->reg.attrib.size;
+		attrib->offsetof = map->reg.attrib.offsetof;
+	}
+err:
+	mutex_unlock(&vmap_ctx->import_idr_lock);
+
+	return ret;
+}
+
+/* must be called with idr lock held.*/
+static void
+importobj_free(struct kref *kref)
+{
+	struct importobj_map_ref *map = NULL;
+
+	if (!kref)
+		return;
+
+	map = container_of(kref, struct importobj_map_ref, refcount);
+	if (map) {
+		idr_remove(&map->vmap_ctx->import_idr, map->obj_id);
+		kfree(map);
+	}
+}
+
+static int
+importobj_unmap(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	struct importobj_map_ref *map = NULL;
+	struct comm_msg msg = {0};
+
+	mutex_lock(&vmap_ctx->import_idr_lock);
+
+	map = idr_find(&vmap_ctx->import_idr, obj_id);
+	if (!map) {
+		mutex_unlock(&vmap_ctx->import_idr_lock);
+		return -EINVAL;
+	}
+	if (WARN_ON(!map->reg.nr_import)) {
+		pr_err("Import ObjId: (%d) wasn't imported yet\n", obj_id);
+		mutex_unlock(&vmap_ctx->import_idr_lock);
+		return -EINVAL;
+	}
+
+	/*
+	 * Each import corresponds to an export, if we unmap an imported
+	 * object, it's exported instance is also refcounted. Remote must
+	 * export again for it to be imported on local SoC again.
+	 */
+	msg.type = COMM_MSG_TYPE_UNREGISTER;
+	msg.u.unreg.export_desc = map->reg.export_desc;
+	msg.u.unreg.iova = map->reg.attrib.iova;
+	msg.u.unreg.size = map->reg.attrib.size;
+	msg.u.unreg.offsetof = map->reg.attrib.offsetof;
+	comm_channel_msg_send(vmap_ctx->comm_channel_h, &msg);
+
+	kref_put(&map->refcount, importobj_free);
+	mutex_unlock(&vmap_ctx->import_idr_lock);
+
+	return 0;
+}
+
+static int
+importobj_putref(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	return importobj_unmap(vmap_ctx, obj_id);
+}
+
+static int
+importobj_getref(struct vmap_ctx_t *vmap_ctx, u32 obj_id)
+{
+	struct memobj_map_ref *map = NULL;
+
+	mutex_lock(&vmap_ctx->import_idr_lock);
+	map = idr_find(&vmap_ctx->import_idr, obj_id);
+	if (WARN_ON(!map)) {
+		mutex_unlock(&vmap_ctx->import_idr_lock);
+		return -EBADF;
+	}
+
+	kref_get(&map->refcount);
+	mutex_unlock(&vmap_ctx->import_idr_lock);
+
+	return 0;
+}
+
+int
+vmap_obj_map(void *vmap_h, struct vmap_obj_map_params *params,
+	     struct vmap_obj_attributes *attrib)
+{
+	int ret = 0;
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)vmap_h;
+
+	if (WARN_ON(!vmap_ctx || !params || !attrib)) {
+		pr_err("DBG: invalid params\n");
+		return -EINVAL;
+	}
+
+	switch (params->type) {
+	case VMAP_OBJ_TYPE_MEM:
+		ret = memobj_map(vmap_ctx, &params->u.memobj, attrib);
+		break;
+	case VMAP_OBJ_TYPE_SYNC:
+		ret = syncobj_map(vmap_ctx, &params->u.syncobj, attrib);
+		break;
+	case VMAP_OBJ_TYPE_IMPORT:
+		ret = importobj_map(vmap_ctx, &params->u.importobj, attrib);
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+int
+vmap_obj_unmap(void *vmap_h, enum vmap_obj_type type, u32 obj_id)
+{
+	int ret = 0;
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)vmap_h;
+
+	if (WARN_ON(!vmap_ctx))
+		return -EINVAL;
+
+	switch (type) {
+	case VMAP_OBJ_TYPE_MEM:
+		ret = memobj_unmap(vmap_ctx, obj_id);
+		break;
+	case VMAP_OBJ_TYPE_SYNC:
+		ret = syncobj_unmap(vmap_ctx, obj_id);
+		break;
+	case VMAP_OBJ_TYPE_IMPORT:
+		ret = importobj_unmap(vmap_ctx, obj_id);
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+int
+vmap_obj_getref(void *vmap_h, enum vmap_obj_type type, u32 obj_id)
+{
+	int ret = 0;
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)vmap_h;
+
+	if (WARN_ON(!vmap_ctx))
+		return -EINVAL;
+
+	switch (type) {
+	case VMAP_OBJ_TYPE_MEM:
+		ret = memobj_getref(vmap_ctx, obj_id);
+		break;
+	case VMAP_OBJ_TYPE_SYNC:
+		ret = syncobj_getref(vmap_ctx, obj_id);
+		break;
+	case VMAP_OBJ_TYPE_IMPORT:
+		ret = importobj_getref(vmap_ctx, obj_id);
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+int
+vmap_obj_putref(void *vmap_h, enum vmap_obj_type type, u32 obj_id)
+{
+	int ret = 0;
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)vmap_h;
+
+	if (WARN_ON(!vmap_ctx))
+		return -EINVAL;
+
+	switch (type) {
+	case VMAP_OBJ_TYPE_MEM:
+		ret = memobj_putref(vmap_ctx, obj_id);
+		break;
+	case VMAP_OBJ_TYPE_SYNC:
+		ret = syncobj_putref(vmap_ctx, obj_id);
+		break;
+	case VMAP_OBJ_TYPE_IMPORT:
+		ret = importobj_putref(vmap_ctx, obj_id);
+		break;
+	default:
+		ret = -EINVAL;
+		break;
+	}
+	return ret;
+}
+
+static void
+vmap_importobj_unregister(void *data, void *ctx)
+{
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)ctx;
+	struct comm_msg *msg = (struct comm_msg *)data;
+	union descriptor_t desc;
+
+	WARN_ON(!vmap_ctx);
+	WARN_ON(!msg);
+	WARN_ON(msg->type != COMM_MSG_TYPE_UNREGISTER);
+
+	desc.value = msg->u.unreg.export_desc;
+	pr_debug("Unregister Desc: (%llu)\n", desc.value);
+	if (desc.bit.handle_type == STREAM_OBJ_TYPE_MEM)
+		vmap_obj_putref(vmap_ctx, VMAP_OBJ_TYPE_MEM,
+				desc.bit.handle_id);
+	else
+		vmap_obj_putref(vmap_ctx, VMAP_OBJ_TYPE_SYNC,
+				desc.bit.handle_id);
+}
+
+static void
+vmap_importobj_register(void *data, void *ctx)
+{
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)ctx;
+	struct comm_msg *msg = (struct comm_msg *)data;
+	struct importobj_map_ref *map = NULL;
+	u32 id_exist = 0;
+
+	WARN_ON(!vmap_ctx);
+	WARN_ON(!msg);
+	WARN_ON(msg->type != COMM_MSG_TYPE_REGISTER);
+
+	mutex_lock(&vmap_ctx->import_idr_lock);
+
+	/* check if we have export descriptor from remote already ? */
+	id_exist = idr_for_each(&vmap_ctx->import_idr, match_export_desc,
+				&msg->u.reg.export_desc);
+	if (id_exist > 0)
+		map = idr_find(&vmap_ctx->import_idr, id_exist);
+
+	if (map) {
+		if (msg->u.reg.iova != map->reg.attrib.iova) {
+			pr_err("attrib:iova doesn't match for export desc\n");
+			goto err;
+		}
+		if (msg->u.reg.size != map->reg.attrib.size) {
+			pr_err("attrib:size doesn't match for export desc\n");
+			goto err;
+		}
+		if (msg->u.reg.offsetof != map->reg.attrib.offsetof) {
+			pr_err("attrib:offsetof doesn't match for export desc\n");
+			goto err;
+		}
+		map->reg.nr_export++;
+		kref_get(&map->refcount);
+		pr_debug("Registered descriptor again: (%llu)\n",
+			 map->reg.export_desc);
+	} else {
+		/* map for the first time.*/
+		map = kzalloc(sizeof(*map), GFP_KERNEL);
+		if (WARN_ON(!map))
+			goto err;
+
+		map->vmap_ctx = vmap_ctx;
+		kref_init(&map->refcount);
+		map->reg.nr_export = 1;
+		map->reg.export_desc = msg->u.reg.export_desc;
+		map->reg.attrib.iova = msg->u.reg.iova;
+		map->reg.attrib.size = msg->u.reg.size;
+		map->reg.attrib.offsetof = msg->u.reg.offsetof;
+		map->obj_id = idr_alloc(&vmap_ctx->import_idr, map,
+					IMPORTOBJ_START, IMPORTOBJ_END,
+					GFP_KERNEL);
+		if (map->obj_id <= 0) {
+			pr_err("Failed to idr alloc for import obj\n");
+			kfree(map);
+			goto err;
+		}
+		pr_debug("Registered descriptor: (%llu)\n", map->reg.export_desc);
+	}
+err:
+	mutex_unlock(&vmap_ctx->import_idr_lock);
+}
+
+/* Entry point for the virtual mapping sub-module/abstraction. */
+int
+vmap_init(struct driver_ctx_t *drv_ctx, void **vmap_h)
+{
+	int ret = 0;
+	struct callback_ops cb_ops = {0};
+	struct vmap_ctx_t *vmap_ctx = NULL;
+
+	/* should not be an already instantiated vmap context. */
+	if (WARN_ON(!drv_ctx || !vmap_h || *vmap_h))
+		return -EINVAL;
+
+	vmap_ctx = kzalloc(sizeof(*vmap_ctx), GFP_KERNEL);
+	if (WARN_ON(!vmap_ctx))
+		return -ENOMEM;
+
+	vmap_ctx->comm_channel_h = drv_ctx->comm_channel_h;
+	vmap_ctx->pci_client_h = drv_ctx->pci_client_h;
+	idr_init(&vmap_ctx->mem_idr);
+	idr_init(&vmap_ctx->sync_idr);
+	idr_init(&vmap_ctx->import_idr);
+	mutex_init(&vmap_ctx->mem_idr_lock);
+	mutex_init(&vmap_ctx->sync_idr_lock);
+	mutex_init(&vmap_ctx->import_idr_lock);
+
+	/* comm-channel for registering and unregistering import objects.*/
+	cb_ops.callback = vmap_importobj_register;
+	cb_ops.ctx = (void *)vmap_ctx;
+	ret = comm_channel_register_msg_cb(vmap_ctx->comm_channel_h,
+					   COMM_MSG_TYPE_REGISTER, &cb_ops);
+	if (ret) {
+		pr_err("Failed to add callback with for Register msg\n");
+		goto err;
+	}
+
+	cb_ops.callback = vmap_importobj_unregister;
+	cb_ops.ctx = (void *)vmap_ctx;
+	ret = comm_channel_register_msg_cb(vmap_ctx->comm_channel_h,
+					   COMM_MSG_TYPE_UNREGISTER, &cb_ops);
+	if (ret) {
+		pr_err("Failed to add callback with for Unregister msg\n");
+		goto err;
+	}
+
+	*vmap_h = vmap_ctx;
+	return ret;
+err:
+	vmap_deinit((void **)&vmap_ctx);
+	return ret;
+}
+
+/* Exit point only. */
+static int
+memobj_release(s32 obj_id, void *ptr, void *data)
+{
+	struct memobj_map_ref *map = (struct memobj_map_ref *)(ptr);
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)(data);
+
+	/* release irrespective of reference counts. */
+	if (map) {
+		memobj_unpin(vmap_ctx, &map->pin);
+		kfree(map);
+	}
+
+	return 0;
+}
+
+/* Exit point only. */
+static int
+syncobj_release(s32 obj_id, void *ptr, void *data)
+{
+	struct syncobj_map_ref *map = (struct syncobj_map_ref *)(ptr);
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)(data);
+
+	/* release irrespective of reference counts. */
+	if (map) {
+		syncobj_unpin(vmap_ctx, &map->pin);
+		kfree(map);
+	}
+
+	return 0;
+}
+
+/* Exit point only. */
+static int
+importobj_release(s32 obj_id, void *ptr, void *data)
+{
+	struct importobj_map_ref *map = (struct importobj_map_ref *)(ptr);
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)(data);
+	struct comm_msg msg = {0};
+
+	if (map) {
+		msg.type = COMM_MSG_TYPE_UNREGISTER;
+		msg.u.unreg.export_desc = map->reg.export_desc;
+		msg.u.unreg.iova = map->reg.attrib.iova;
+		msg.u.unreg.size = map->reg.attrib.size;
+		msg.u.unreg.offsetof = map->reg.attrib.offsetof;
+		comm_channel_msg_send(vmap_ctx->comm_channel_h, &msg);
+
+		kfree(map);
+	}
+
+	return 0;
+}
+
+/* Exit point for nvscic2c-pcie vmap sub-module/abstraction. */
+void
+vmap_deinit(void **vmap_h)
+{
+	struct vmap_ctx_t *vmap_ctx = (struct vmap_ctx_t *)(*vmap_h);
+
+	if (!vmap_ctx)
+		return;
+
+	comm_channel_unregister_msg_cb(vmap_ctx->comm_channel_h,
+				       COMM_MSG_TYPE_REGISTER);
+	comm_channel_unregister_msg_cb(vmap_ctx->comm_channel_h,
+				       COMM_MSG_TYPE_UNREGISTER);
+	/*
+	 * free all the allocations still idr allocated IDR.
+	 *
+	 * ideally, this should not be the case, however in scenario:
+	 * application went away and the remote missed to free the imported
+	 * target handle, then during module unload (PCIe link shall be down)
+	 * we shall free all the pinned + yet to be unpinned handles.
+	 */
+	mutex_lock(&vmap_ctx->mem_idr_lock);
+	idr_for_each(&vmap_ctx->mem_idr, memobj_release, vmap_ctx);
+	idr_destroy(&vmap_ctx->mem_idr);
+	mutex_unlock(&vmap_ctx->mem_idr_lock);
+
+	mutex_lock(&vmap_ctx->sync_idr_lock);
+	idr_for_each(&vmap_ctx->sync_idr, syncobj_release, vmap_ctx);
+	idr_destroy(&vmap_ctx->sync_idr);
+	mutex_unlock(&vmap_ctx->sync_idr_lock);
+
+	mutex_lock(&vmap_ctx->import_idr_lock);
+	idr_for_each(&vmap_ctx->import_idr, importobj_release, vmap_ctx);
+	idr_destroy(&vmap_ctx->import_idr);
+	mutex_unlock(&vmap_ctx->import_idr_lock);
+
+	kfree(vmap_ctx);
+	*vmap_h = NULL;
+}
diff --git a/drivers/misc/nvscic2c-pcie/vmap.h b/drivers/misc/nvscic2c-pcie/vmap.h
new file mode 100644
index 000000000000..3c0a3efd98c8
--- /dev/null
+++ b/drivers/misc/nvscic2c-pcie/vmap.h
@@ -0,0 +1,264 @@
+/* SPDX-License-Identifier: GPL-2.0+ */
+/*
+ * Copyright (c) 2022, NVIDIA CORPORATION.  All rights reserved.
+ *
+ * This program is free software; you can redistribute it and/or modify it
+ * under the terms and conditions of the GNU General Public License,
+ * version 2, as published by the Free Software Foundation.
+ *
+ * This program is distributed in the hope it will be useful, but WITHOUT
+ * ANY WARRANTY; without even the implied warranty of MERCHANTABILITY or
+ * FITNESS FOR A PARTICULAR PURPOSE.  See the GNU General Public License for
+ * more details.
+ */
+
+#ifndef __VMAP_H__
+#define __VMAP_H__
+
+#include "common.h"
+
+/*
+ * Virtual mapping abstraction offers pinning functionalities for the very
+ * the specific use-case(s) to achieve NvStreams streaming over NvSciC2cPcie
+ * on Tegra PCIe EP or Tegra PCIe RP. It's scope is specific to/for and limited
+ * to NvSciC2cPcie.
+ *
+ * Virtual mapping interfaces primarily offer mapping(also referred as pinning)
+ * and unmapping(also referred as unpinning) of Memory and Sync objects. Both
+ * memory and sync objects are representd by NvRmShim handles provided by the
+ * application
+ *
+ * This SW can run only on X86 PCIe RP
+ *
+ * Each object is considered one of the following:
+ *
+ * 1. Local - The object visibility is limited to local SoC on which it is
+ *    allocated. Also sometimes referred as Unexported. These objects are mapped
+ *    to PCIe device address space and it's whereabouts are NOT shared with
+ *    remote SoC.
+ * 2. Export/Exported - The object visibility is across PCIe to remote SoC
+ *    and subsequently remote SoC can initiate writes to it. For tegra,
+ *    exported objects are never read over PCIe. These objects are mapped to
+ *    PCIe device address space and it's whereabouts shall be shared with remote
+ *    SoC.
+ * 3. Import/Imported - This is a virtual object which points to the
+ *    corresponding object exported by remote SoC. Being virtual, it shall be
+ *    similar for both Mem and Sync objects - therefore Imported object is
+ *    just an import/imported object and NOT imported Mem/Sync obj.
+ *
+ * Protection/Permission flags:
+ * a. Local Mem objects map to PCIe device with READ access.
+ * b. Export Mem objects map to PCIe device with WRITE access (We export for
+ *    remote to write to it via CPU or PCIe eDMA(zero-copy).
+ * c. Local Sync objects are not mapped to PCIe device as NvSciC2cPcie KMD
+ *    shall signal(write) them via CPU mapping
+ * d. Export Sync objects map to PCIe device with WRITE access (We export for
+ *    remote to signal(write) to it via CPU or PCIe eDMA).
+ *
+ * Mapping:
+ * Tegra PCIe EP exposes three BAR memory windows towards PCIe RP. Of these,
+ * Only one (1) (BAR0) is available for NvSciC2cPcie access. Therefore, all
+ * Export objects must be mapped to a PCIe address which this PCIe EP BAR0
+ * translation is programmed with. With the overall PCIe address space being
+ * much bigger than the PCIe BAR0 space, there is an explicit need to stitch
+ * all Exported objects to a single region. This requires Export objects be
+ * mapped using iommu apis to achieve BAR stitching and this mapping is
+ * referred as client managed with NvSciC2cPcie managing the iova region.
+ *
+ * Tegra PCIe EP has limited set of translation registers for it's CPU to raise
+ * PCIe transactions towards a PCIe RP. Therefore, when Sync objects are
+ * exported from PCIe RP towards PCIe EP to CPU signal them, they must be mapped
+ * to a single iova region which PCIe EP has setup for it's translaction
+ * registers. This is strictly not applicable for Exported Mem objects as they
+ * are written by eDMA always by the importing SoC. However, to keep parity and
+ * symmetry in the behavior of Exported Mem objects from Tegra PCIe EP->PCIe RP,
+ * Exported Mem objects from X86 PCIe RP->Tegra PCIe EP shall also be mapped
+ * to a client managed iova region.
+ *
+ * For Local Mem objects which are accessed by local SoC PCIe eDMA, they
+ * can be mapped to any pcie address outside the reserved iova region by
+ * NvSciC2cPcie for exports. This doesn't require any iova management by
+ * client and is prefectly fine to use pci device (iommu) managed iova.
+ * This mapping is referred as device(dev) managed mapping. Only on X86 PCIe
+ * RP, Exported Mem objects can be mapped using dev managed iova as Tegra PCIe
+ * EP shall write them using eDMA, but as stated before, to keep parity and
+ * symmetry in the behavior of Exported Mem objects from Tegra PCIe EP->PCIe RP,
+ * Exported Mem objects from X86 PCIe RP->Tegra PCIe EP shall also be mapped
+ * to a client managed iova region.
+ *
+ * All Sync objects (local or export) are mapped to NvSciC2cPcie for signalling
+ * (Write access), and there shall be only one signaler allowed. Therefore, Sync
+ * objects are pinned/mapped just once.
+ * Export Mem objects are mapped to NvSciC2cPcie for remote SoC to produce data
+ * (Write access), also in the lack of N producer -> 1 consumer user-case,
+ * remote Mem objects are pinned/mapped just once. However, Local Mem objects
+ * which have Read access, can be mapped/pinned again. Essentially, all objects
+ * requiring Write access by NvSciC2cPcie (PCIe device) are pinned/mapped once.
+ *
+ * Summary:
+ *  i.   Types:
+ *        a. Local Mem objects.
+ *        b. Export Mem objects.
+ *        c. Local Sync objects.
+ *        d. Export Sync objects.
+ *        e. Import objects.
+ *  ii.  Mapping:
+ *        a. Local Mem objects - dev managed (READ only).
+ *        b. Export Mem objects - client managed (WRITE only).
+ *            On Tegra PCIe EP:- compulsarily client managed.
+ *            On Tegra PCIe RP:- Can be either dev managed or client managed.
+ *            Choose client manged to be in symmetry with Tegra PCIe EP.
+ *        c. Local Sync objects - Not mapped but pinned (tracked).
+ *        d. Export Sync objects - client managed (WRITE ONLY).
+ *            On Tegra PCIe EP:- compulsarily client managed.
+ *            On Tegra PCIe RP:- Can be either dev managed (if PCIe RP uses
+ *            only eDMA for signaling, but we have use-cases for CPU signaling
+ *            also from Tegra PCIe RP. Therefore choose client managed as it can
+ *            satisfy CPU and eDMA signaling needs from remote  PCIe EP.
+ *        e. Import Objects - virtual objects pointing to exported objects by
+ *            remote. Every exported object from a SoC must have a corresponding
+ *            import object on remote SoC. Not mapped but pinned(tracked).
+ */
+
+/* forward declaration. */
+struct driver_ctx_t;
+
+/* Object type that shall be virtually mapped for PCIe access. */
+enum vmap_obj_type {
+	/* NvSciC2cPcieBufRmHandle, aka, memobj.*/
+	VMAP_OBJ_TYPE_MEM = STREAM_OBJ_TYPE_MEM,
+
+	/* NvSciC2cPcieSyncRmHandle, aka, syncobj.*/
+	VMAP_OBJ_TYPE_SYNC = STREAM_OBJ_TYPE_SYNC,
+
+	/*(virtual) objects imported from remote SoC.*/
+	VMAP_OBJ_TYPE_IMPORT,
+};
+
+/*
+ * Permissions for pin/mapping Buff/Sync objs to PCIe device
+ *
+ * at the moment, WRITE for all EXPORT*
+ * at the moment, READ for all LOCAL*
+ */
+enum vmap_obj_prot {
+	/* read-only access by PCIe device. */
+	VMAP_OBJ_PROT_READ = 1,
+
+	/* write only access to PCIe device. */
+	VMAP_OBJ_PROT_WRITE = 2,
+
+	/*
+	 * no use-case known today.
+	 * VMAP_OBJ_PERM_READWRITE = 4,
+	 */
+};
+
+/* Which IOVA to use for mapping Mem/Sync objs.*/
+enum vmap_mngd {
+	/*
+	 * Stitching of all exported objects is done by reserving an IOVA region
+	 * and mapping Mem and Sync objs to it. The reserved IOVA region is
+	 * managed by client/user (NvSciC2cPcie) and use iommu apis to map Mem
+	 * or Sync objects to the specific IOVA.
+	 */
+	VMAP_MNGD_CLIENT = 0,
+
+	/*
+	 * The IOVA is managed by pci dev, therefore dev managed. Used only for
+	 * Mem objects (Local and possible for exported too).
+	 */
+	VMAP_MNGD_DEV,
+};
+
+/* Memory location of input buffer.*/
+enum mem_loc {
+	VMAP_SYSMEM = 0,
+	VMAP_VIDMEM,
+};
+
+/* Returned object attributes after mapping.*/
+struct vmap_obj_attributes {
+	enum vmap_obj_type type;
+	s32 id;
+	/* For programming of sysmem buf*/
+	u64 iova;
+	size_t size;
+	size_t offsetof;
+	/* For programming of vidmem dma sgt*/
+	struct sg_table *sgt;
+	/* required for signalling of local sync obj.*/
+	__u64 *local_fence_addr;
+};
+
+/* Parameters to map Mem object.*/
+struct vmap_memobj_map_params {
+	__u64 vaddr;
+	__u64 size;
+	/* To allow mapping Export Mem objects as dev-managed - Tegra PCIe RP.*/
+	enum vmap_mngd mngd;
+	/* local/source mem as read-only, remote/export as write-only.*/
+	enum vmap_obj_prot prot;
+	enum mem_loc mem_type;
+};
+
+/* Parameters to map Sync object.*/
+struct vmap_syncobj_map_params {
+	__u64 vaddr;
+	__u64 size;
+	/* client mngd only.*/
+	enum vmap_mngd mngd;
+	/* write-only.*/
+	enum vmap_obj_prot prot;
+	/* local sync objs will not be pinned to pcie address space.*/
+	bool pin_reqd;
+};
+
+/* Parameters to map Import object.*/
+struct vmap_importobj_map_params {
+	u64 export_desc;
+};
+
+/* Parameters to map Mem/Sync/Import object.*/
+struct vmap_obj_map_params {
+	enum vmap_obj_type type;
+	union vmap_params {
+		struct vmap_memobj_map_params memobj;
+		struct vmap_syncobj_map_params syncobj;
+		struct vmap_importobj_map_params importobj;
+	} u;
+};
+
+/* Parameters to register Import object, as received from remote NvSciC2cPcie.*/
+struct vmap_importobj_reg_params {
+	u64 export_desc;
+	u64 iova;
+	size_t size;
+	size_t offsetof;
+};
+
+/* Entry point for the virtual mapping sub-module/abstraction. */
+int
+vmap_init(struct driver_ctx_t *drv_ctx, void **vmap_h);
+
+/* Exit point for nvscic2c-pcie vmap sub-module/abstraction. */
+void
+vmap_deinit(void **vmap_h);
+
+/* Map objects to pcie device.*/
+int
+vmap_obj_map(void *vmap_h, struct vmap_obj_map_params *params,
+	     struct vmap_obj_attributes *attrib);
+
+/* Unmap objects from pcie device.*/
+int
+vmap_obj_unmap(void *vmap_h, enum vmap_obj_type type, u32 obj_id);
+
+/* Increment reference count for objects. */
+int
+vmap_obj_getref(void *vmap_h, enum vmap_obj_type type, u32 obj_id);
+
+/* Decrement reference count for objects. */
+int
+vmap_obj_putref(void *vmap_h, enum vmap_obj_type type, u32 obj_id);
+#endif // __VMAP_H__
